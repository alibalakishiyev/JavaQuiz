<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT-2 Modelinin ÆtraflÄ± vÉ™ Dinamik Ä°zahÄ±</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.8;
            background-color: #fdfdfd;
            color: #1a1a1a;
            padding: 40px;
            max-width: 960px;
            margin: auto;
        }

        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 10px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        h1, h2, h3 {
            color: #003366;
        }
        code {
            background-color: #eef;
            padding: 3px 6px;
            border-radius: 4px;
        }
        .code {
            font-family: Consolas, monospace;
            background: #272822;
            color: #f8f8f2;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        pre {
            background: #eee;
            border-left: 5px solid #004d99;
            padding: 10px;
            overflow-x: auto;
        }
        ul {
            padding-left: 20px;
        }
        .section {
            margin-bottom: 50px;
        }
    </style>
</head>
<body>
<h1>GPT-2 Modeli - ÆtraflÄ± vÉ™ Dinamik Ä°zah</h1>

<div class="formula">
    <h2>1. Ãœmumi BaxÄ±ÅŸ</h2>
    <p>
        GPT-2 (Generative Pretrained Transformer 2) bÃ¶yÃ¼k miqyaslÄ± bir dil modelidir vÉ™ ardÄ±cÄ±l olaraq insan kimi mÉ™tn yarada bilir. Transformer arxitekturasÄ± É™sasÄ±nda qurulmuÅŸ bu model Ã§oxsaylÄ± dil vÉ™ kontekst tapÅŸÄ±rÄ±qlarÄ±nÄ± yerinÉ™ yetirmÉ™k Ã¼Ã§Ã¼n Ã¶yrÉ™dilmiÅŸdir.
    </p>
</div>

<div class="formula">
    <h2>2. Tokenizasiya (BPE)</h2>
    <p>
        Model mÉ™tnlÉ™ri birbaÅŸa iÅŸlÉ™yÉ™ bilmir, ona gÃ¶rÉ™ dÉ™ É™vvÉ™lcÉ™ bu mÉ™tn "tokenlÉ™rÉ™" parÃ§alanÄ±r. GPT-2 Byte Pair Encoding (BPE) metodundan istifadÉ™ edir:
    </p>
    <pre><code>MÉ™tn: "Salam dÃ¼nya"
TokenlÉ™r: ['Sala', 'm', ' ', 'dÃ¼', 'nya']</code></pre>
    <p>
        Daha sonra bu tokenlÉ™r token ID-lÉ™rinÉ™ Ã§evrilir:
    </p>
    <pre><code>Token IDs: [356, 210, 32, 6701, 3982]</code></pre>
</div>

<div class="formula">
    <h2>3. Embedding vÉ™ Pozisional MÉ™lumat</h2>
    <p>
        Token ID-lÉ™r vektorlara (embeddinglÉ™rÉ™) Ã§evrilir. Bu vektorlara ardÄ±cÄ±llÄ±ÄŸÄ± qorumaq Ã¼Ã§Ã¼n pozisional embedding É™lavÉ™ olunur:
    </p>
    <pre><code>input_embedding + position_embedding â†’ final_input</code></pre>
</div>

<div class="formula">
    <h2>4. Transformer BloklarÄ±</h2>
    <p>GPT-2 bir neÃ§É™ eyni strukturlu transformer blokundan ibarÉ™tdir. HÉ™r blok aÅŸaÄŸÄ±dakÄ± hissÉ™lÉ™rdÉ™n ibarÉ™tdir:</p>
    <ul>
        <li><strong>Masked Self-Attention:</strong> Model yalnÄ±z É™vvÉ™lki tokenlÉ™rÉ™ baxaraq nÃ¶vbÉ™ti sÃ¶zÃ¼ proqnozlaÅŸdÄ±rÄ±r.</li>
        <li><strong>Layer Normalization:</strong> HÉ™r qatÄ±n Ã§Ä±xÄ±ÅŸÄ±nÄ± stabillÉ™ÅŸdirir.</li>
        <li><strong>Feedforward ÅÉ™bÉ™kÉ™:</strong> HÉ™r mÃ¶vqedÉ™ki vektoru daha kompleks formada emal edir.</li>
        <li><strong>Residual Connection:</strong> GiriÅŸ vÉ™ Ã§Ä±xÄ±ÅŸ arasÄ±nda É™laqÉ™ni qoruyur.</li>
    </ul>
    <p>Bu proses qatlar boyu tÉ™krarlanÄ±r:</p>
    <pre><code>input â†’ [Multi-head attention â†’ Add & Norm â†’ Feedforward â†’ Add & Norm] Ã— n_layers</code></pre>
</div>

<div class="formula">
    <h2>5. Linear vÉ™ Softmax QatlarÄ±</h2>
    <p>
        Sonuncu transformer blokundan Ã§Ä±xan nÉ™ticÉ™ <code>Linear</code> qatÄ±na verilir vÉ™ nÉ™ticÉ™dÉ™ <code>Softmax</code> ilÉ™ ehtimal paylanmasÄ± alÄ±nÄ±r:
    </p>
    <pre><code>Linear â†’ Softmax â†’ Ehtimallar â†’ Æn yÃ¼ksÉ™k ehtimallÄ± token</code></pre>
    <p>
        MÉ™sÉ™lÉ™n:
    </p>
    <pre><code>GiriÅŸ: "Salam, necÉ™sÉ™n?"
Ã‡Ä±xÄ±ÅŸ: "Salam, necÉ™sÉ™n? MÉ™n yaxÅŸÄ±yam, sÉ™n?"</code></pre>
</div>

<div class="formula">
    <h2>6. Arxitektur ParametrlÉ™ri</h2>
    <ul>
        <li><code>n_embd:</code> Embedding Ã¶lÃ§Ã¼sÃ¼. (mÉ™sÉ™lÉ™n: 768)</li>
        <li><code>n_layer:</code> Transformer qatlarÄ±nÄ±n sayÄ± (mÉ™s: 12)</li>
        <li><code>n_head:</code> Attention baÅŸlarÄ±nÄ±n sayÄ± (mÉ™s: 12)</li>
        <li><code>block_size:</code> Maksimum kontekst uzunluÄŸu (mÉ™s: 1024 token)</li>
    </ul>
</div>

<div class="formula">
    <h2>7. TÉ™lim vÉ™ Ä°stifadÉ™</h2>
    <p>
        Model É™vvÉ™lcÉ™dÉ™n bÃ¶yÃ¼k korpus Ã¼zÉ™rindÉ™ tÉ™lim gÃ¶rÃ¼r (pretraining). Daha sonra spesifik tapÅŸÄ±rÄ±qlara uyÄŸunlaÅŸdÄ±rÄ±la bilÉ™r (fine-tuning).
    </p>
    <ul>
        <li>Ã‡oxdilli tÉ™lim (AzÉ™rbaycan dili dÉ™ daxil olmaqla)</li>
        <li>Domain-spesifik mÉ™lumatlarla fine-tune</li>
        <li>Chatbotlar, tÉ™rcÃ¼mÉ™, kod generasiya vÉ™ s.</li>
    </ul>
</div>

<div class="formula">
    <h2>8. NÉ™ticÉ™</h2>
    <p>
        GPT-2 modeli dilin statistik strukturlarÄ±nÄ± Ã¶yrÉ™nÉ™rÉ™k, mÉ™tn yaratma vÉ™ tamamlamada yÃ¼ksÉ™k performans gÃ¶stÉ™rir. Ã–z tokenizeriniz vÉ™ tÉ™mizlÉ™nmiÅŸ AzÉ™rbaycan dilindÉ™ verilmiÅŸ mÉ™lumatlarla bu modelin Ã¶z versiyanÄ±zÄ± Ã¶yrÉ™dÉ™ bilÉ™rsiniz.
    </p>
</div>

<h2>ğŸ“ŒTam Kod</h2>

<pre class="formula"><code class="formula">

        import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# GPT-2 modelindÉ™ rotary positional embedding istifadÉ™ etmÉ™k Ã¼Ã§Ã¼n yardÄ±mÃ§Ä± funksiya
def apply_rotary_pos_emb(q, k, sin, cos):
    # Rotary É™mÉ™liyyatÄ±: sin vÉ™ cos vektorlarÄ± ilÉ™ dot mÉ™hsulunun tÉ™hrif olunmasÄ±
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

# Vektorun yarÄ±sÄ±nÄ± dÃ¶ndÉ™rmÉ™k Ã¼Ã§Ã¼n yardÄ±mÃ§Ä± funksiya
def rotate_half(x):
    x1, x2 = x.chunk(2, dim=-1)
    return torch.cat([-x2, x1], dim=-1)

# Rotary sinus vÉ™ kosinus vektorlarÄ±nÄ± hesablamaq Ã¼Ã§Ã¼n
def get_rotary_embedding(seq_len, dim, device):
    # Burada logaritmik ÅŸÉ™kildÉ™ artan tezliklÉ™r yaradÄ±lÄ±r
    theta = 10000 ** (-torch.arange(0, dim, 2, dtype=torch.float32) / dim)
    position = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)
    freqs = position * theta.unsqueeze(0)
    sin, cos = torch.sin(freqs), torch.cos(freqs)
    return sin.to(device), cos.to(device)

# Causal (masked) self-attention bloku
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        self.n_head = config.n_head
        self.head_dim = config.n_embd // config.n_head

        # QKV (Query, Key, Value) Ã¼Ã§Ã¼n vahid Linear qat
        self.qkv_proj = nn.Linear(config.n_embd, 3 * config.n_embd)
        self.out_proj = nn.Linear(config.n_embd, config.n_embd)

        # FlashAttention varsa, onu istifadÉ™ et
        self.use_flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        self.attn_dropout = nn.Dropout(config.attn_dropout)
        self.resid_dropout = nn.Dropout(config.resid_dropout)

        # Causal mask (yÉ™ni gÉ™lÉ™cÉ™k tokenlÉ™rÉ™ baxmamaq)
        self.register_buffer("mask", torch.tril(torch.ones(config.block_size, config.block_size))
                                        .view(1, 1, config.block_size, config.block_size))

    def forward(self, x, sin, cos):
        B, T, C = x.size()

        # Q, K, V tensorlarÄ±nÄ± ayÄ±r
        qkv = self.qkv_proj(x)  # [B, T, 3 * C]
        qkv = qkv.view(B, T, self.n_head, 3 * self.head_dim).transpose(1, 2)
        q, k, v = qkv.chunk(3, dim=-1)  # HÉ™r biri [B, n_head, T, head_dim]

        # Rotary positional embedding tÉ™tbiqi
        q, k = apply_rotary_pos_emb(q, k, sin[:, :T, :], cos[:, :T, :])

        if self.use_flash:
            # FlashAttention (native PyTorch sÃ¼rÉ™tli versiyasÄ±)
            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)
        else:
            # Klassik scaled dot-product attention
            att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)
            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v

        # NÉ™ticÉ™ni yenidÉ™n birlÉ™ÅŸdir vÉ™ Ã§Ä±xÄ±ÅŸa yÃ¶nlÉ™ndir
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.resid_dropout(self.out_proj(y))
        return y

# Transformer blokunun É™sas hissÉ™si
class TransformerBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln2 = nn.LayerNorm(config.n_embd)
        self.ff = nn.Sequential(
            nn.Linear(config.n_embd, 4 * config.n_embd),
            nn.GELU(),
            nn.Linear(4 * config.n_embd, config.n_embd),
            nn.Dropout(config.resid_dropout),
        )

    def forward(self, x, sin, cos):
        x = x + self.attn(self.ln1(x), sin, cos)  # Residual connection
        x = x + self.ff(self.ln2(x))
        return x

# GPT-2 modelinin É™sas sinfi
class GPT2Model(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        # Token vÉ™ mÃ¶vqe embeddinglÉ™ri
        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd)
        self.pos_embedding = None  # ArtÄ±q rotary embedding istifadÉ™ edÉ™cÉ™yik

        # Model qatlarÄ±
        self.blocks = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.n_layer)
        ])
        self.ln_f = nn.LayerNorm(config.n_embd)

        # Ã‡Ä±xÄ±ÅŸ qatÄ±nÄ± tokenlÉ™rlÉ™ paylaÅŸÄ±rÄ±q
        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.head.weight = self.token_embedding.weight

        self.dropout = nn.Dropout(config.embd_dropout)

    def forward(self, idx):
        B, T = idx.shape
        device = idx.device

        # TokenlÉ™ri embedding vektorlarÄ±na Ã§evir
        tok_emb = self.token_embedding(idx)  # [B, T, C]
        x = self.dropout(tok_emb)

        # Rotary pozisional embeddinglÉ™r
        sin, cos = get_rotary_embedding(self.config.block_size, self.config.n_embd, device)
        sin = sin.unsqueeze(0)  # [1, T, D]
        cos = cos.unsqueeze(0)

        # BÃ¼tÃ¼n transformer bloklarÄ±ndan keÃ§
        for block in self.blocks:
            x = block(x, sin, cos)

        # Final normalization vÉ™ Ã§Ä±xÄ±ÅŸ
        x = self.ln_f(x)
        logits = self.head(x)  # [B, T, vocab_size]
        return logits

    # Yeni mÉ™tn yaratmaq Ã¼Ã§Ã¼n generate funksiyasÄ±
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, repetition_penalty=1.0):
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -self.config.block_size:]
            logits = self(idx_cond)

            logits = logits[:, -1, :] / temperature

            # TÉ™krar sÃ¶zlÉ™ri azaldan cÉ™za
            if repetition_penalty != 1.0:
                for i in range(logits.size(0)):
                    for token_id in set(idx[i].tolist()):
                        logits[i, token_id] /= repetition_penalty

            # top-k sampling
            if top_k is not None:
                values, _ = torch.topk(logits, top_k)
                min_val = values[:, -1].unsqueeze(1)
                logits[logits < min_val] = -float('Inf')

            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, next_token), dim=1)

        return idx

</code></pre>




</body>
</html>
