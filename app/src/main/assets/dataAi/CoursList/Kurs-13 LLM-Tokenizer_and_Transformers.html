<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Azerbaycan Tokenizer Modeli - Ä°zah</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f9;
            color: #333;
        }
        h1,h2,h3,h4,h5 {
            color: #e74c3c;
        }
        pre {
            background-color: #2d2d2d;  /* QaranlÄ±q fon */
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            white-space: pre-wrap;
        }
        pre code {
            color: #00ffb7;  /* Kod yazÄ±sÄ± â€” parlaq yaÅŸÄ±l/mavi */
            font-family: Consolas, Monaco, monospace;
            font-size: 15px;
            display: block;
        }
        p {
            line-height: 1.6;
        }
        strong {
            color: #e74c3c;
        }
    </style>


</head>
<body>

<h2>ğŸ“Œ<code>__init__()</code> Metodu â€“ AÃ§Ä±qlama</h2>

<pre><code class="language-python">def __init__(self):
    self.VOCAB_SIZE = 130000  # Tokenizer Ã¼Ã§Ã¼n maksimum sÃ¶zlÃ¼k Ã¶lÃ§Ã¼sÃ¼
    self.SPECIAL_TOKENS = {  # XÃ¼susi tokenlÉ™r vÉ™ onlarÄ±n ID-lÉ™ri
        "&lt;s&gt;": 0,
        "&lt;pad&gt;": 1,
        "&lt;/s&gt;": 2,
        "&lt;unk&gt;": 3,
        "&lt;mask&gt;": 4,
        "&lt;|endoftext|&gt;": 5,
        "&lt;case&gt;": 6
    }
</code></pre>

<h3>ğŸ” <code>__init__</code> nÉ™dir?</h3>
<p><code>__init__()</code> â€” Python-da bir sinfin iÃ§indÉ™ avtomatik iÅŸlÉ™yÉ™n konstruktordur. Obyekt yaradÄ±larkÉ™n bu metod Ã§aÄŸÄ±rÄ±lÄ±r vÉ™ baÅŸlanÄŸÄ±c dÉ™yÉ™rlÉ™r verilir.</p>

<h3>âš™ï¸ <code>self.VOCAB_SIZE = 130000</code></h3>
<p>Bu dÉ™yiÅŸÉ™n, tokenizer-in maksimum <strong>sÃ¶zlÃ¼k Ã¶lÃ§Ã¼sÃ¼nÃ¼</strong> tÉ™yin edir. MÉ™sÉ™lÉ™n: Tokenizer 130,000-dÉ™n artÄ±q token saxlamayacaq.</p>

<h3>ğŸ§© <code>self.SPECIAL_TOKENS = {...}</code></h3>
<p>Bu hissÉ™, tokenizer Ã¼Ã§Ã¼n xÃ¼susi mÉ™na daÅŸÄ±yan tokenlÉ™ri vÉ™ onlarÄ±n ID-lÉ™rini saxlayÄ±r. MÉ™sÉ™lÉ™n:</p>

<table border="1" cellpadding="6" style="border-collapse: collapse;">
    <thead>
    <tr style="background-color:#f2f2f2;">
        <th>Token</th>
        <th>ID</th>
        <th>Ä°zah</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td><code>&lt;s&gt;</code></td>
        <td>0</td>
        <td>BaÅŸlanÄŸÄ±c tokeni</td>
    </tr>
    <tr>
        <td><code>&lt;pad&gt;</code></td>
        <td>1</td>
        <td>Pad/boÅŸluq doldurucu token</td>
    </tr>
    <tr>
        <td><code>&lt;/s&gt;</code></td>
        <td>2</td>
        <td>Son tokeni</td>
    </tr>
    <tr>
        <td><code>&lt;unk&gt;</code></td>
        <td>3</td>
        <td>NamÉ™lum token</td>
    </tr>
    <tr>
        <td><code>&lt;mask&gt;</code></td>
        <td>4</td>
        <td>MaskalanmÄ±ÅŸ token (mÉ™s. BERT Ã¼Ã§Ã¼n)</td>
    </tr>
    <tr>
        <td><code>&lt;|endoftext|&gt;</code></td>
        <td>5</td>
        <td>MÉ™tnin sonu tokeni</td>
    </tr>
    <tr>
        <td><code>&lt;case&gt;</code></td>
        <td>6</td>
        <td>BÃ¶yÃ¼k/kiÃ§ik hÉ™rf tÉ™yini Ã¼Ã§Ã¼n istifadÉ™ oluna bilÉ™r</td>
    </tr>
    </tbody>
</table>

<p>Bu xÃ¼susi tokenlÉ™r modelin <strong>baÅŸlanÄŸÄ±c vÉ™ son nÃ¶qtÉ™lÉ™rini tanÄ±masÄ±</strong>, <strong>namÉ™lum sÃ¶zlÉ™rlÉ™ necÉ™ davranmasÄ±</strong> vÉ™ <strong>seqmentlÉ™ri ayÄ±rmasÄ±</strong> Ã¼Ã§Ã¼n vacibdir.</p>


<h2>ğŸ“ŒTam Kod</h2>
<pre><code>
    from transformers import GPT2TokenizerFast
import os
import json
import unicodedata

def create_azerbaijani_tokenizer(vocab_path, merges_path, output_dir):
    """AzÉ™rbaycanca xÃ¼susi simvollarÄ± qoruyaraq tokenizer yaradan funksiya"""
    try:
        # vocab.json faylÄ±nÄ± yÃ¼klÉ™
        with open(vocab_path, 'r', encoding='utf-8') as f:
            vocab = json.load(f)

        # AzÉ™rbaycana xas simvollar siyahÄ±sÄ± (split edilmÉ™mÉ™si Ã¼cÃ¼n)
        az_special_chars = ["É™", "Ã¼", "Ã¶", "Ä±", "ÄŸ", "ÅŸ", "Ã§",
                            "Æ", "Ãœ", "Ã–", "Ä°", "Ä", "Å", "Ã‡"]

        # HÉ™min simvollarÄ±n ID-lÉ™rini al
        char_to_id = {char: vocab[char] for char in az_special_chars if char in vocab}

        # GPT2 tokenizer-i yÃ¼klÉ™ (hazÄ±r fayllarla)
        tokenizer = GPT2TokenizerFast(
            vocab_file=vocab_path,
            merges_file=merges_path,
            bos_token="&lt;s&gt;",
            eos_token="&lt;/s&gt;,
            unk_token="&lt;unk&gt;",
            pad_token="&lt;pad&gt;",
            mask_token="&lt;mask&gt;",
        )

        # XÃ¼susi simvollarÄ± "additional_special_tokens" kimi É™lavÉ™ et ki, split olunmasÄ±n
        tokenizer.add_special_tokens({
            "additional_special_tokens": az_special_chars
        })

        # Orijinal decoder-i saxla
        original_decode = tokenizer.decode

        # XÃ¼susi decoder funksiyasÄ± - simvollarÄ± dÃ¼zgÃ¼n decode etsin
        def fixed_decode(token_ids, skip_special_tokens=False, **kwargs):
            text = original_decode(token_ids, skip_special_tokens=skip_special_tokens, **kwargs)

            # ID-dÉ™n simvola map
            id_to_char = {v: k for k, v in char_to_id.items()}

            # Token ID-lÉ™rini simvollara Ã§evir
            decoded_chars = []
            for token_id in token_ids:
                if token_id in id_to_char:
                    decoded_chars.append(id_to_char[token_id])
                else:
                    decoded_chars.append(original_decode([token_id]))

            # MÉ™tn kimi birlÉ™ÅŸdir
            text = ''.join(decoded_chars)
            text = unicodedata.normalize('NFC', text)

            # Encoding artefaktlarÄ±nÄ± dÃ¼zÉ™lt
            replacements = {
                "uÌˆ": "Ã¼",
                "oÌˆ": "Ã¶",
                "gÌ‡": "ÄŸ",
                "sÌ§": "ÅŸ",
                "cÌ§": "Ã§",
                "iÌ‡": "i",
                "IÌ‡": "Ä°",
                "Ä ": " ",
                "â–": " ",
                "  ": " "
            }

            for wrong, correct in replacements.items():
                text = text.replace(wrong, correct)

            return text.strip()

        # Yeni decode metodunu tokenizer-É™ ver
        tokenizer.decode = fixed_decode

        # Tokenizer-i saxla
        tokenizer.save_pretrained(output_dir)
        print(f"âœ… Successfully created fixed tokenizer in {output_dir}")
        return tokenizer

    except Exception as e:
        print(f"âŒ Error creating tokenizer: {str(e)}")
        return None

def test_tokenizer(tokenizer):
    """Tokenizer-i test et - É™lavÉ™ simvollarÄ± yoxla"""
    test_cases = [
        "AzÉ™rbaycan dilindÉ™ test: É™, Ã¼, Ã¶, Ä±, ÄŸ, ÅŸ, Ã§",
        "BakÄ± ÅŸÉ™hÉ™ri gÃ¶zÉ™ldir",
        "ÃœzÃ¼m, Ã¶rdÉ™k, Ä±spanak, É™rik",
        "Ã‡ox ÅŸirin qÉ™lyanaltÄ±",
        "ÆdÉ™biyyat vÉ™ incÉ™sÉ™nÉ™t",
        "GÃ¶yÃ§ayda Ã¼zÃ¼m festivallarÄ±",
        "ÅÉ™ki xalÃ§a muzeyi",
        "QÉ™ribÉ™ Ã¼nsiÉ™t Ã¶lÃ§Ã¼sÃ¼",
        "Ä°stanbulda ÄŸÉ™zÉ™lin ÅŸÉ™killÉ™ri",
        "Ã–zÉ™l iÅŸlÉ™r Ã¼cÃ¼n Ã§ox gÃ¶zÉ™l"
    ]

    for text in test_cases:
        print(f"\n{'='*50}\nTesting: {text}")

        # TokenlÉ™ÅŸdir
        tokens = tokenizer.tokenize(text)
        print("\nTokens:")
        print(tokens)

        # Encode
        ids = tokenizer.encode(text, add_special_tokens=False)
        print("\nIDs:")
        print(ids)

        # Decode
        decoded = tokenizer.decode(ids)
        print("\nDecoded:")
        print(decoded)
        print("\nMatch:", decoded == text)

        # XÃ¼susi simvollarÄ± yoxla
        az_chars = ["É™", "Ã¼", "Ã¶", "Ä±", "ÄŸ", "ÅŸ", "Ã§", "Æ", "Ãœ", "Ã–", "Ä°", "Ä", "Å", "Ã‡"]
        present_chars = [char for char in az_chars if char in text]

        if present_chars:
            print("\nCharacter Verification:")
            for char in present_chars:
                char_id = tokenizer.encode(char, add_special_tokens=False)
                decoded_char = tokenizer.decode(char_id)
                status = "âœ“" if decoded_char == char else "âœ—"
                print(f"{char}: {status} (ID: {char_id[0] if char_id else 'N/A'})")
                if status == "âœ—":
                    print(f"  Encoding problem: '{char}' â†’ {char_id} â†’ '{decoded_char}'")

if __name__ == "__main__":
    # Fayl yollarÄ±
    vocab_path = "azeri_gpt2_tokenizer/vocab.json"
    merges_path = "azeri_gpt2_tokenizer/merges.txt"
    output_dir = "azeri_fixed_tokenizer_final"

    # Tokenizer-i yarat
    tokenizer = create_azerbaijani_tokenizer(vocab_path, merges_path, output_dir)

    # Tokenizer hazÄ±rdÄ±rsa test et
    if tokenizer:
        test_tokenizer(tokenizer)
    else:
        print("Tokenizator yaradÄ±la bilmÉ™di. XÉ™ta baÅŸ verdi.")

</code></pre>

</body>
</html>
