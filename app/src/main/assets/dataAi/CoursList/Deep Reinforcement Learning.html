<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>TransformerDRLAgent Ä°zahÄ±</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          margin: 40px auto;
          max-width: 900px;
          line-height: 1.6;
          color: #222;
          background-color: #f4f4f4;
          padding: 20px;
        }
        h1, h2 {
          color: #004080;
        }
        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 7px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        code {
            background-color: #eef;
            padding: 3px 6px;
            border-radius: 4px;
        }
        pre {
          background: #eee;
          padding: 10px;
          overflow-x: auto;
        }
        section {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 7px;
            margin: 15px 0;
            font-family: Roboto, sans-serif; /* dÉ™yiÅŸdirildi */
            color: black;
            box-shadow: 0 0 8px rgba(0,0,0,0.1);
        }

        ul {
          margin-top: 0;
        }
    </style>
</head>
<body>
<h1>TransformerDRLAgent Kodu HaqqÄ±nda Tam Ä°zah</h1>

<section>
    <h2>1. Model sinifi: TransformerDRLAgent</h2>
    <p><strong>NÉ™ edir?</strong><br />
        Bu sinif, agentin neyron ÅŸÉ™bÉ™kÉ™sini tÉ™msil edir. Burada vÉ™ziyyÉ™t (state) sekansÄ± qÉ™bul olunur vÉ™ Transformer encoder vasitÉ™silÉ™ iÅŸlÉ™nir. Ã‡Ä±xÄ±ÅŸda iki ÅŸey verilir:</p>
    <ul>
        <li><strong>Policy head</strong> â€” verilÉ™n vÉ™ziyyÉ™tÉ™ gÃ¶rÉ™ hÉ™rÉ™kÉ™t ehtimallarÄ± (softmax ilÉ™).</li>
        <li><strong>Value head</strong> â€” vÉ™ziyyÉ™tin dÉ™yÉ™r qiymÉ™ti (real É™dÉ™d).</li>
    </ul>
    <p><strong>Æsas hissÉ™lÉ™r:</strong></p>
    <ul>
        <li><code>state_embedding</code>: VÉ™ziyyÉ™t vektorlarÄ±nÄ± Transformer Ã¼Ã§Ã¼n uyÄŸun Ã¶lÃ§Ã¼yÉ™ (embed_size) Ã§evirir.</li>
        <li><code>transformer_encoder</code>: Transformer qatlarÄ±ndan ibarÉ™tdir, sekans mÉ™lumatlarÄ±nÄ± emal edir.</li>
        <li><code>policy_head</code>: HÉ™rÉ™kÉ™t ehtimallarÄ±nÄ± Ã§Ä±xarÄ±r.</li>
        <li><code>value_head</code>: VÉ™ziyyÉ™tin dÉ™yÉ™rini qiymÉ™tlÉ™ndirir.</li>
    </ul>
    <p><strong>forward metodu:</strong><br />
        GiriÅŸ kimi <code>(batch_size, seq_len, state_dim)</code> Ã¶lÃ§Ã¼lÃ¼ tensor alÄ±r. VÉ™ziyyÉ™tlÉ™r embedding-lÉ™nir, Transformer vasitÉ™silÉ™ iÅŸlÉ™nir, É™n son zaman addÄ±mÄ± gÃ¶tÃ¼rÃ¼lÃ¼r vÉ™ oradan hÉ™m policy, hÉ™m dÉ™ value Ã§Ä±xarÄ±lÄ±r.</p>
</section>

<section>
    <h2>2. Replay Memory</h2>
    <p>TÉ™crÃ¼bÉ™lÉ™rin (state, action, reward, next_state, done) yadda saxlanmasÄ± Ã¼Ã§Ã¼n istifadÉ™ olunur.</p>
    <ul>
        <li><code>push</code> ilÉ™ tÉ™crÃ¼bÉ™ É™lavÉ™ edilir,</li>
        <li><code>sample</code> ilÉ™ tÉ™sadÃ¼fi minibatch gÃ¶tÃ¼rÃ¼lÃ¼r,</li>
    </ul>
    <p>Bu, agentin tÉ™limi Ã¼Ã§Ã¼n nÃ¼munÉ™ toplamaq Ã¼Ã§Ã¼n zÉ™ruridir.</p>
</section>

<section>
    <h2>3. DRLTransformerAgent</h2>
    <p>Bu, É™sas agent sinifidir, yÉ™ni bÃ¼tÃ¼n tÉ™lim, hÉ™rÉ™kÉ™t seÃ§imi vÉ™ yaddaÅŸ idarÉ™sini burda aparÄ±rsan.</p>
    <p><strong>ParametrlÉ™r vÉ™ ÅŸÉ™bÉ™kÉ™lÉ™r:</strong></p>
    <ul>
        <li><code>policy_net</code>: Cari ÅŸÉ™bÉ™kÉ™ (Ã¶yrÉ™nilir).</li>
        <li><code>target_net</code>: Stabil tÉ™lim Ã¼Ã§Ã¼n target ÅŸÉ™bÉ™kÉ™, periodik olaraq <code>policy_net</code>-in Ã§É™kilÉ™rini kopyalayÄ±r.</li>
    </ul>
    <p><strong>select_action metodu:</strong></p>
    <ul>
        <li><code>epsilon</code>-greedy strategiyasÄ±nÄ± tÉ™tbiq edir:</li>
        <ul>
            <li><code>epsilon</code> ehtimalÄ± ilÉ™ tÉ™sadÃ¼fi hÉ™rÉ™kÉ™t seÃ§ir (exploration),</li>
            <li>Æks halda, <code>policy_net</code>-dÉ™n ehtimallarÄ± alÄ±r vÉ™ onlara gÃ¶rÉ™ hÉ™rÉ™kÉ™t seÃ§ir (exploitation).</li>
        </ul>
    </ul>
    <p><strong>update_epsilon metodu:</strong><br />
        <code>epsilon</code> dÉ™yÉ™rini tÉ™dricÉ™n azaldÄ±r ki, agent zamanla daha Ã§ox exploitation (Ã¶yrÉ™ndiklÉ™rini istifadÉ™ etmÉ™k) etsin.</p>
    <p><strong>update_model metodu:</strong></p>
    <ul>
        <li>Replay memory-dÉ™n nÃ¼munÉ™ gÃ¶tÃ¼rÃ¼r,</li>
        <li>Cari vÉ™ target ÅŸÉ™bÉ™kÉ™lÉ™rdÉ™n dÉ™yÉ™rlÉ™ri hesablayÄ±r,</li>
        <li>Advantage (Ã¼stÃ¼nlÃ¼k) vÉ™ zÉ™rÉ™r funksiyasÄ±nÄ± (policy loss + value loss) hesablayÄ±r,</li>
        <li>Optimizer ilÉ™ ÅŸÉ™bÉ™kÉ™ Ã§É™kilÉ™rini yenilÉ™yir.</li>
    </ul>
    <p><strong>update_target_net metodu:</strong><br />
        <code>target_net</code>-in Ã§É™kilÉ™rini <code>policy_net</code>-lÉ™ sinxronlaÅŸdÄ±rÄ±r.</p>
    <p><strong>save_experience metodu:</strong><br />
        TÉ™crÃ¼bÉ™ni replay memory-yÉ™ É™lavÉ™ edir.</p>
</section>

<section>
    <h2>4. TÉ™lim dÃ¶ngÉ™si train_agent</h2>
    <ul>
        <li>MÃ¼hitdÉ™n vÉ™ziyyÉ™t alÄ±r,</li>
        <li>Son 10 vÉ™ziyyÉ™ti yadda saxlayÄ±r (sekans kimi),</li>
        <li>Agent hÉ™rÉ™kÉ™t seÃ§ir, mÃ¼hitÉ™ tÉ™tbiq edir,</li>
        <li>TÉ™crÃ¼bÉ™ni yaddaÅŸa É™lavÉ™ edir,</li>
        <li>ÅÉ™bÉ™kÉ™ni tÉ™lim etdirir,</li>
        <li>Periodik olaraq target ÅŸÉ™bÉ™kÉ™ni yenilÉ™yir,</li>
        <li><code>epsilon</code>-u azaldÄ±r,</li>
        <li>HÉ™r epizodun mÃ¼kafatÄ±nÄ± Ã§ap edir.</li>
    </ul>
</section>

<section>
    <h2>Ãœmumi xÃ¼lasÉ™</h2>
    <ul>
        <li>Bu kod dÉ™rin gÃ¼clÉ™ndirici Ã¶yrÉ™nmÉ™ Ã¼Ã§Ã¼n Transformer É™saslÄ± bir agentdir.</li>
        <li>Agent zamanla vÉ™ziyyÉ™tlÉ™rin ardÄ±cÄ±llÄ±ÄŸÄ±nÄ± Transformer vasitÉ™silÉ™ Ã¶yrÉ™nir.</li>
        <li>Policy vÉ™ value eyni anda Ã¶yrÉ™dilir (Actor-Critic tipli metod kimi).</li>
        <li>Replay memory ilÉ™ tÉ™limdÉ™ sabitlik tÉ™min edilir.</li>
        <li><code>epsilon</code>-greedy ilÉ™ balans exploration vÉ™ exploitation arasÄ±nda qorunur.</li>
        <li>Target ÅŸÉ™bÉ™kÉ™ isÉ™ Ã¶yrÉ™nmÉ™ni stabillÉ™ÅŸdirir.</li>
    </ul>
</section>

<section>
    <h2>ğŸ“ŒTam Kod</h2>

    <pre class="formula"><code class="formula">

        import torch
        import torch.nn as nn
        import torch.optim as optim
        import torch.nn.functional as F
        from torch.distributions import Categorical
        import numpy as np
        from collections import deque
        import random

        # ===========================
        # HiperparametrlÉ™r (tÉ™lim parametrlÉ™ri)
        # ===========================
        BATCH_SIZE = 32                 # Bir tÉ™lim addÄ±mÄ±nda istifadÉ™ olunan nÃ¼munÉ™ sayÄ±
        LR = 1e-4                      # Ã–yrÉ™nmÉ™ sÃ¼rÉ™ti (learning rate)
        GAMMA = 0.99                   # GÉ™lÉ™cÉ™k mÃ¼kafatÄ±n endirim faktoru
        EPSILON_START = 1.0            # BaÅŸlanÄŸÄ±cda tÉ™sadÃ¼fi hÉ™rÉ™kÉ™t ehtimalÄ± (exploration)
        EPSILON_END = 0.01             # Minimum tÉ™sadÃ¼fi hÉ™rÉ™kÉ™t ehtimalÄ±
        EPSILON_DECAY = 0.995          # HÉ™r epizoddan sonra epsilon-un azalma dÉ™rÉ™cÉ™si
        TARGET_UPDATE = 10             # Target ÅŸÉ™bÉ™kÉ™nin yenilÉ™nmÉ™ epizodu (hÉ™r neÃ§É™ epizodda)
        MEMORY_CAPACITY = 10000        # Replay yaddaÅŸÄ±nÄ±n maksimum tutumu
        EPISODES = 1000                # TÉ™limdÉ™ Ã¼mumi epizod sayÄ±
        HIDDEN_SIZE = 128              # Transformer feedforward qatÄ±nÄ±n Ã¶lÃ§Ã¼sÃ¼
        NUM_HEADS = 4                  # Transformer-in Ã§oxbaÅŸlÄ± diqqÉ™t (multi-head attention) sayÄ±
        NUM_LAYERS = 3                 # Transformer encoder qatlarÄ±nÄ±n sayÄ±
        EMBED_SIZE = 64                # VÉ™ziyyÉ™t embedding Ã¶lÃ§Ã¼sÃ¼ (gizli vektor Ã¶lÃ§Ã¼sÃ¼)


        # ===========================
        # Transformer É™saslÄ± DÉ™rin TÉ™krarlanan Ã–yrÉ™nmÉ™ Agentinin Modeli
        # ===========================
        class TransformerDRLAgent(nn.Module):
            def __init__(self, state_dim, action_dim):
                super(TransformerDRLAgent, self).__init__()
                self.state_dim = state_dim
                self.action_dim = action_dim

                # VÉ™ziyyÉ™ti embedding Ã¶lÃ§Ã¼sÃ¼nÉ™ salmaq Ã¼Ã§Ã¼n tam baÄŸlÄ± qat
                self.state_embedding = nn.Linear(state_dim, EMBED_SIZE)

                # Transformer Encoder qatÄ±nÄ±n tÉ™rifi
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=EMBED_SIZE,      # Transformer daxili Ã¶lÃ§Ã¼sÃ¼
                    nhead=NUM_HEADS,         # DiqqÉ™t baÅŸlÄ±qlarÄ± sayÄ±
                    dim_feedforward=HIDDEN_SIZE,  # Feedforward qatÄ±nÄ±n Ã¶lÃ§Ã¼sÃ¼
                    dropout=0.1              # Dropout faizi (overfitting qarÅŸÄ±sÄ± Ã¼Ã§Ã¼n)
                )

                # Transformer Encoder qatlarÄ±nÄ±n yÄ±ÄŸÄ±lÄ±ÅŸÄ± (stack)
                self.transformer_encoder = nn.TransformerEncoder(
                    encoder_layer,
                    num_layers=NUM_LAYERS
                )

                # ÆmÉ™l (policy) ehtimallarÄ±nÄ± Ã§Ä±xarmaq Ã¼Ã§Ã¼n Ã§Ä±xÄ±ÅŸ qatÄ±
                self.policy_head = nn.Linear(EMBED_SIZE, action_dim)

                # VÉ™ziyyÉ™t dÉ™yÉ™rini qiymÉ™tlÉ™ndirmÉ™k Ã¼Ã§Ã¼n Ã§Ä±xÄ±ÅŸ qatÄ±
                self.value_head = nn.Linear(EMBED_SIZE, 1)

            def forward(self, x):
                # GiriÅŸ: (batch_size, seq_len, state_dim)
                batch_size, seq_len, _ = x.shape

                # VÉ™ziyyÉ™ti embedding Ã¶lÃ§Ã¼sÃ¼nÉ™ Ã§evir
                x = self.state_embedding(x)  # (batch_size, seq_len, embed_size)

                # Transformer input formatÄ±: (seq_len, batch_size, embed_size)
                x = x.transpose(0, 1)

                # Transformer Encoder-un tÉ™tbiqi
                x = self.transformer_encoder(x)

                # YenidÉ™n orijinal forma: (batch_size, seq_len, embed_size)
                x = x.transpose(0, 1)

                # Son zaman addÄ±mÄ±ndakÄ± vÉ™ziyyÉ™t (É™n son state)
                last_state = x[:, -1, :]

                # Policy ehtimallarÄ± (softmax ilÉ™) vÉ™ state dÉ™yÉ™ri
                action_probs = F.softmax(self.policy_head(last_state), dim=-1)
                state_value = self.value_head(last_state)

                return action_probs, state_value


        # ===========================
        # Replay Memory: TÉ™crÃ¼bÉ™lÉ™rin saxlanmasÄ± Ã¼Ã§Ã¼n buffer
        # ===========================
        class ReplayMemory:
            def __init__(self, capacity):
                self.capacity = capacity
                self.memory = deque(maxlen=capacity)  # Æn Ã§ox capacity yaddaÅŸ saxlayÄ±r

            def push(self, state, action, reward, next_state, done):
                # TÉ™crÃ¼bÉ™ni buffer-a É™lavÉ™ et
                self.memory.append((state, action, reward, next_state, done))

            def sample(self, batch_size):
                # TÉ™crÃ¼bÉ™lÉ™rdÉ™n tÉ™sadÃ¼fi nÃ¼munÉ™ gÃ¶tÃ¼r
                return random.sample(self.memory, batch_size)

            def __len__(self):
                # HazÄ±rda buffer-dakÄ± nÃ¼munÉ™ sayÄ±
                return len(self.memory)


        # ===========================
        # DRL Transformer Agent: QÉ™rar vermÉ™, tÉ™lim, vÉ™ hÉ™rÉ™kÉ™t seÃ§mÉ™ funksiyalarÄ±
        # ===========================
        class DRLTransformerAgent:
            def __init__(self, state_dim, action_dim):
                self.state_dim = state_dim
                self.action_dim = action_dim
                self.epsilon = EPSILON_START  # Eksplorasiya ehtimalÄ± (baÅŸlanÄŸÄ±c)

                # Policy vÉ™ target ÅŸÉ™bÉ™kÉ™lÉ™ri yaradÄ±rÄ±q
                self.policy_net = TransformerDRLAgent(state_dim, action_dim)
                self.target_net = TransformerDRLAgent(state_dim, action_dim)
                self.target_net.load_state_dict(self.policy_net.state_dict())  # Target ÅŸÉ™bÉ™kÉ™ni policy-nin kopyasÄ± edirik
                self.target_net.eval()  # Target ÅŸÉ™bÉ™kÉ™ eval rejimindÉ™dir (tÉ™lim deyil)

                # Optimizer: Adam istifadÉ™ olunur
                self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)

                # Replay memory (tÉ™crÃ¼bÉ™ yaddaÅŸÄ±)
                self.memory = ReplayMemory(MEMORY_CAPACITY)

                # Value loss Ã¼Ã§Ã¼n MSE
                self.value_loss = nn.MSELoss()

            def select_action(self, state_sequence):
                """
                HÉ™rÉ™kÉ™t seÃ§imi (epsilon-greedy)
                :param state_sequence: Son vÉ™ziyyÉ™tlÉ™r zÉ™nciri, shape=(seq_len, state_dim)
                :return: seÃ§ilÉ™n hÉ™rÉ™kÉ™tin indeksi (int)
                """
                if random.random() < self.epsilon:
                    # TÉ™sadÃ¼fi hÉ™rÉ™kÉ™t (exploration)
                    return random.randint(0, self.action_dim - 1)

                # Eksploitasiya (modelin proqnozu É™sasÄ±nda)
                with torch.no_grad():
                    # Batch Ã¶lÃ§Ã¼sÃ¼ É™lavÉ™ et (1 batch)
                    state_sequence = torch.FloatTensor(state_sequence).unsqueeze(0)  # (1, seq_len, state_dim)
                    probs, _ = self.policy_net(state_sequence)
                    m = Categorical(probs)  # HÉ™rÉ™kÉ™t paylanmasÄ±
                    action = m.sample()
                    return action.item()

            def update_epsilon(self):
                # Epsilon-un tÉ™dricÉ™n azalmasÄ± (explorationâ†’exploitation keÃ§idi)
                self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)

            def update_model(self):
                # ÆgÉ™r kifayÉ™t qÉ™dÉ™r tÉ™crÃ¼bÉ™ yoxdursa, tÉ™lim etmirik
                if len(self.memory) < BATCH_SIZE:
                    return

                # Replay memory-dÉ™n batch gÃ¶tÃ¼r
                transitions = self.memory.sample(BATCH_SIZE)
                batch = list(zip(*transitions))  # states, actions, rewards, next_states, dones ayrÄ±lÄ±r

                # Tensorlara Ã§evir
                state_seqs = torch.FloatTensor(np.array(batch[0]))
                actions = torch.LongTensor(np.array(batch[1])).unsqueeze(1)
                rewards = torch.FloatTensor(np.array(batch[2])).unsqueeze(1)
                next_state_seqs = torch.FloatTensor(np.array(batch[3]))
                dones = torch.FloatTensor(np.array(batch[4])).unsqueeze(1)

                # MÃ¶vcud ÅŸÉ™bÉ™kÉ™dÉ™n cari Q dÉ™yÉ™rlÉ™ri (policy vÉ™ value)
                action_probs, state_values = self.policy_net(state_seqs)

                # Target ÅŸÉ™bÉ™kÉ™dÉ™n gÉ™lÉ™cÉ™k vÉ™ziyyÉ™tlÉ™rin dÉ™yÉ™rlÉ™ri
                _, next_state_values = self.target_net(next_state_seqs)

                # GÃ¶zlÉ™nilÉ™n Q dÉ™yÉ™rlÉ™ri (reward + discount * next_state_value)
                expected_state_values = rewards + GAMMA * next_state_values * (1 - dones)

                # Advantage hesabla (fÉ™rq)
                advantages = expected_state_values - state_values

                # Policy loss - PPO tipli mÉ™ntiqÉ™ bÉ™nzÉ™r (log prob * advantage)
                m = Categorical(action_probs)
                policy_loss = -m.log_prob(actions.squeeze()).unsqueeze(1) * advantages.detach()
                policy_loss = policy_loss.mean()

                # Value loss (MSE)
                value_loss = self.value_loss(state_values, expected_state_values.detach())

                # Ãœmumi zÉ™rÉ™r (loss)
                loss = policy_loss + value_loss

                # Geri yayÄ±lÄ±m vÉ™ optimizasiya
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

            def update_target_net(self):
                # Target ÅŸÉ™bÉ™kÉ™ni policy ÅŸÉ™bÉ™kÉ™nin parametrlÉ™ri ilÉ™ yenilÉ™
                self.target_net.load_state_dict(self.policy_net.state_dict())

            def save_experience(self, state_seq, action, reward, next_state_seq, done):
                # Replay memory-É™ tÉ™crÃ¼bÉ™ É™lavÉ™ et
                self.memory.push(state_seq, action, reward, next_state_seq, done)


        # ===========================
        # TÉ™lim dÃ¶ngÉ™si (training loop)
        # ===========================
        def train_agent(env, agent, episodes=EPISODES):
            for episode in range(episodes):
                # MÃ¼hitin baÅŸlanÄŸÄ±cÄ±
                state_seq = deque(maxlen=10)  # Son 10 vÉ™ziyyÉ™ti saxlayÄ±rÄ±q (sequence length = 10)
                state = env.reset()
                state_seq.append(state)

                total_reward = 0
                done = False

                while not done:
                    # AgentdÉ™n hÉ™rÉ™kÉ™t seÃ§
                    action = agent.select_action(np.array(state_seq))

                    # HÉ™rÉ™kÉ™ti mÃ¼hitÉ™ tÉ™tbiq et
                    next_state, reward, done, _ = env.step(action)

                    # SonrakÄ± vÉ™ziyyÉ™tlÉ™r zÉ™ncirini yenilÉ™
                    next_state_seq = state_seq.copy()
                    next_state_seq.append(next_state)

                    # TÉ™crÃ¼bÉ™ni yaddaÅŸa É™lavÉ™ et
                    agent.save_experience(np.array(state_seq), action, reward, np.array(next_state_seq), done)

                    # Modeli tÉ™lim etdir
                    agent.update_model()

                    # VÉ™ziyyÉ™ti yenilÉ™
                    state_seq = next_state_seq
                    total_reward += reward

                # Target ÅŸÉ™bÉ™kÉ™ni mÃ¼tÉ™madi olaraq yenilÉ™
                if episode % TARGET_UPDATE == 0:
                    agent.update_target_net()

                # Epsilon-u azaldÄ±rÄ±q (exploration azalÄ±r)
                agent.update_epsilon()

                print(f"Episode {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}")


        # ===========================
        # Misal Ã¼Ã§Ã¼n istifadÉ™
        # ===========================
        if __name__ == "__main__":
            # Burada Ã¶z mÃ¼hitini (env) yaratmalÄ±san.
            # MÉ™sÉ™lÉ™n:
            # env = YourEnvironment()
            # state_dim = env.observation_space.shape[0]
            # action_dim = env.action_space.n

            # Test Ã¼Ã§Ã¼n dummy Ã¶lÃ§Ã¼lÉ™r:
            state_dim = 10
            action_dim = 4

            agent = DRLTransformerAgent(state_dim, action_dim)

            # TÉ™lim dÃ¶ngÉ™sini baÅŸlatmaq Ã¼Ã§Ã¼n
            # train_agent(env, agent)


</code></pre>

</section>
</body>
</html>
