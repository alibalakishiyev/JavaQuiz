<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>n_layer AÃ§Ä±qlamasÄ±</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          padding: 30px;
          background-color: #f9f9f9;
          color: #333;
        }
        h1, h2 {
          color: #2c3e50;
        }
        .code-block {
          background-color: #eef;
          padding: 10px;
          border-radius: 6px;
          font-family: Consolas, monospace;
          margin: 10px 0;
        }
        .highlight {
          background-color: #fff3cd;
          border-left: 6px solid #ffc107;
          padding: 10px;
          margin: 20px 0;
        }
        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 10px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        .note {
          background-color: #e8f5e9;
          padding: 10px;
          border-left: 6px solid #66bb6a;
          margin: 10px 0;
        }
    </style>
</head>
<body>

<h1>ğŸ” <code>n_layer = 10</code> â€” Transformer BloklarÄ±nÄ±n SayÄ±</h1>

<div class="formula">
    config = GPT2Config(<br>
    &nbsp;&nbsp;&nbsp;&nbsp;n_layer = 10,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;...<br>
    )
</div>

<h2>ğŸ“Œ NÉ™ demÉ™kdir?</h2>
<div class="formula">
    <strong>n_layer</strong> â€” modeldÉ™ neÃ§É™ É™dÉ™d Transformer bloku (katÄ±) olduÄŸunu gÃ¶stÉ™rir.
    <br>
    HÉ™r blok Ã¶zÃ¼ndÉ™ self-attention + feed-forward (MLP) qatlarÄ±nÄ± birlÉ™ÅŸdirir.
</div>

<h2>ğŸ”¬ Bir Transformer Bloku nÉ™ edir?</h2>
<div class="formula">
    HÉ™r blokda bu É™mÉ™liyyatlar var:
    <ul>
        <li>â¤ LayerNorm</li>
        <li>â¤ Multi-Head Self-Attention (MHA)</li>
        <li>â¤ Residual Connection</li>
        <li>â¤ Feedforward Neural Network (FFN)</li>
    </ul>
</div>

<h2>ğŸ’¡ Riyazi Hesablamalar</h2>
<div class="formula">
    Total Parametr = <strong>n_layer Ã— (MHA + FFN)</strong><br><br>
    Harada:<br>
    - MHA = Multi-head attention parametrlÉ™ri<br>
    - FFN = Fully connected layer parametrlÉ™ri
</div>

<h3>MÉ™sÉ™lÉ™n (n_embd = 768, n_head = 12):</h3>
<div class="formula">
    â¤ MHA tÉ™xmini: 4 Ã— (768 Ã— 768) = 2,359,296 parametr<br>
    â¤ FFN (2 qatlÄ±): 768Ã—3072 + 3072Ã—768 = 4,718,592 + 2,359,296 = 7,077,888<br><br>
    â¤ Bir blokda toplam â‰ˆ 9.4 milyon parametr
</div>

<h2>ğŸ”¢ Toplam Model ParametrlÉ™ri</h2>
<div class="formula">
    n_layer = 10<br>
    â‰ˆ 10 Ã— 9.4M = <strong>94 milyon</strong> parametr (yalnÄ±z transformer bloklarÄ± Ã¼Ã§Ã¼n)
</div>

<h2>ğŸ“ˆ DÉ™rinlik nÉ™ verir?</h2>
<div class="formula">
    <ul>
        <li>ğŸ“Š DÉ™rinlik â†’ model daha Ã§ox kontekst vÉ™ kompleks nÃ¼munÉ™lÉ™ri Ã¶yrÉ™nÉ™ bilir.</li>
        <li>ğŸš€ Daha Ã§ox qat = daha yÃ¼ksÉ™k Ã¶yrÉ™nmÉ™ gÃ¼cÃ¼</li>
        <li>âš ï¸ Amma hÉ™mÃ§inin daha Ã§ox <strong>RAM</strong>, <strong>hesablama vaxtÄ±</strong> vÉ™ <strong>riskli overfitting</strong></li>
    </ul>
</div>

<h2>ğŸ§  DÉ™rin Model vÉ™ SÉ™thi Model fÉ™rqi</h2>
<div class="formula">
    <strong>SÉ™thi model:</strong> 2-4 qatlÄ± (n_layer=2~4) â€” SadÉ™ tapÅŸÄ±rÄ±qlar Ã¼Ã§Ã¼n<br>
    <strong>DÉ™rin model:</strong> 10+ qatlÄ± (n_layer=10~48) â€” Qrammatika, kontekst vÉ™ kreativlik Ã¼Ã§Ã¼n
</div>

<h2>âœ… NÉ™ticÉ™</h2>
<div class="formula">
    <ul>
        <li><code>n_layer</code> modelin intellektual dÉ™rinliyini vÉ™ Ã¶yrÉ™nmÉ™ gÃ¼cÃ¼nÃ¼ mÃ¼É™yyÉ™n edir.</li>
        <li>SÉ™nin 10 qatlÄ± modelin orta sÉ™viyyÉ™li gÃ¼cÉ™ malikdir.</li>
        <li>Daha bÃ¶yÃ¼k datalarda vÉ™ uzun tÉ™limlÉ™rdÉ™ daha yÃ¼ksÉ™k qat sayÄ± daha yaxÅŸÄ± nÉ™ticÉ™ verir.</li>
    </ul>
</div>

</body>
</html>
