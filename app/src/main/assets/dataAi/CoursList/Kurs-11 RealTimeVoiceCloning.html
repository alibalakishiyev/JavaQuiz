<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>Real time Voice Cloning</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f9;
            color: #333;
        }
        h1,h2,h3,h4,h5 {
            color: #e74c3c;
        }
        pre {
            background-color: #333; /* QaranlÄ±q arxa fon */
            color: #fff; /* AÄŸ yazÄ± */
            padding: 10px; /* HÃ¼dudlar */
            border-radius: 5px; /* KÃ¼nclÉ™rin yuvarlaq olmasÄ± */
            font-family: Consolas, monaco, monospace; /* Kod yazÄ± tipi */
            font-size: 16px; /* YazÄ± Ã¶lÃ§Ã¼sÃ¼ */
            overflow: auto; /* Uzun kodun sÃ¼rÃ¼ÅŸdÃ¼rÃ¼lmÉ™si */
            white-space: pre-wrap; /* Kodu avtomatik olaraq sarmalayÄ±b gÃ¶stÉ™rmÉ™k */
        }
        p {
            line-height: 1.6;
        }
        strong {
            color: #e74c3c;
        }
    </style>
</head>
<body>

<h1>Real time Voice Cloning</h1>

<h2 class="section-title">Dataset necÉ™ olmalÄ±dÄ±r?</h2>
<p><strong>VCTK Corpus</strong> É™n yaxÅŸÄ± datasetlÉ™rdÉ™n biridir encoder Ã¼Ã§Ã¼n, Ã§Ã¼nki:</p>
<ul>
    <li>Ã‡ox sayda spiker var (110 nÉ™fÉ™r).</li>
    <li>HÉ™r biri ~400 cÃ¼mÉ™ oxuyur.</li>
    <li>Bu cÃ¼r speaker-diverse dataset encoder Ã¼Ã§Ã¼n idealdÄ±r.</li>
</ul>

<h2 class="section-title">ğŸ§ Format tÉ™lÉ™blÉ™ri:</h2>
<ul>
    <li>Audio formatÄ±: .wav</li>
    <li>Sampling rate: 16kHz</li>
    <li>Mono kanal</li>
    <li>16-bit PCM (standard .wav formatÄ±)</li>
</ul>

<h2 class="section-title">Dataset strukturu:</h2>
<pre>
C:\Users\Mafia\Datasets\
â”œâ”€â”€ LibriSpeech\
â”‚   â””â”€â”€ train-other-500\
â”œâ”€â”€ VoxCeleb1\
â”‚   â”œâ”€â”€ wav\
â”‚   â””â”€â”€ vox1_meta.csv
â””â”€â”€ VoxCeleb2\
    â””â”€â”€ dev\
    </pre>
<p>Bu strukturu saxla, Ã§Ã¼nki hÉ™r spikerin sÉ™si ayrÄ± qovluqda olmalÄ±dÄ±r. <code>p225</code>, <code>p226</code>, ... <code>p315</code> vÉ™ s.</p>

<h3>Terminal komandasÄ±:</h3>
<code>python encoder_preprocess.py "C:\Users\Mafia\Datasets"</code>

<h2 class="section-title">Encoder preprocess â€” Mel spectrogram-lar yaratmaq Ã¼Ã§Ã¼n</h2>
<p>SÉ™ndÉ™ artÄ±q <code>encoder_preprocess.py</code> faylÄ± var. Bu fayl tÉ™miz audio fayllarÄ± alÄ±r vÉ™ mel spectrogram yaratmaq Ã¼Ã§Ã¼n lazÄ±m olan fayllara Ã§evirir.</p>

<h3>1. Encoder qovluÄŸu:</h3>
<p>Bu qovluq sÉ™sin kodlaÅŸdÄ±rÄ±lmasÄ± vÉ™ tÉ™rzi ilÉ™ É™laqÉ™lidir. Burada <strong>Speaker Encoding</strong> prosesi hÉ™yata keÃ§irilir.</p>

<h3>encoder_train.py faylÄ±ndan parÃ§alar:</h3>
<pre>
# LazÄ±m olan kitabxanalarÄ± É™lavÉ™ edirik
import argparse
from pathlib import Path

from encoder.train import train
from utils.argutils import print_args

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="SÉ™slÉ™rÉ™ gÃ¶rÉ™ istifadÉ™Ã§i tanÄ±yan modeli Ã¶yrÉ™dir.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument("run_id", type=str, help="Model Ã¼Ã§Ã¼n ad.")
    parser.add_argument("clean_data_root", type=Path, help="Emal edilmiÅŸ datalarÄ±n yolu.")
    parser.add_argument("-m", "--models_dir", type=Path, default="saved_models", help="ModellÉ™rin saxlandÄ±ÄŸÄ± qovluq.")
    parser.add_argument("-v", "--vis_every", type=int, default=10, help="Vizualizasiya hÉ™r neÃ§É™ addÄ±mdan bir.")
    parser.add_argument("-u", "--umap_every", type=int, default=100, help="UMAP yenilÉ™mÉ™ tezliyi.")
    parser.add_argument("-s", "--save_every", type=int, default=500, help="Model saxlama tezliyi.")
    parser.add_argument("-b", "--backup_every", type=int, default=7500, help="Backup tezliyi.")
    parser.add_argument("-f", "--force_restart", action="store_true", help="SÄ±fÄ±rdan baÅŸlamaq Ã¼Ã§Ã¼n.")
    parser.add_argument("--visdom_server", type=str, default="http://localhost", help="Vizdom server.")
    parser.add_argument("--no_visdom", action="store_true", help="Vizdom-u sÃ¶ndÃ¼r.")

    args = parser.parse_args()
    print_args(args, parser)
    train(**vars(args))
    </pre>

<h2 class="section-title">VoxCeleb1 â€” Frame MÉ™lumatlarÄ±</h2>
<p><strong>Identity:</strong> id11251<br>
    <strong>Reference:</strong> 9h3PzQ9kL4w<br>
    <strong>Offset:</strong> -5<br>
    <strong>FV Conf:</strong> 17.383<br>
    <strong>ASD Conf:</strong> 5.357</p>

<h3>Frames:</h3>
<table>
    <thead>
    <tr>
        <th>Frame</th>
        <th>X</th>
        <th>Y</th>
        <th>W</th>
        <th>H</th>
    </tr>
    </thead>
    <tbody>
    <tr><td>001282</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001283</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001284</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001285</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001286</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001287</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001288</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001289</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001290</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001291</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001292</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    <tr><td>001293</td><td>137</td><td>96</td><td>75</td><td>75</td></tr>
    </tbody>
</table>

<h2>encoder_preprocess.py</h2>

<p><strong>Bu Python kodu</strong> sÉ™sli verilÉ™nlÉ™ri (datasetlÉ™ri) tÉ™lim Ã¼Ã§Ã¼n É™vvÉ™lcÉ™dÉ™n emal edir vÉ™ onlarÄ± <strong>mel-spektrogram</strong> formasÄ±nda saxlayÄ±r. Bu, sÉ™slÉ™ tanÄ±nma (voice recognition) vÉ™ ya sÉ™s klonlama (voice cloning) modellÉ™rinin, xÃ¼susilÉ™ dÉ™ <strong>SV2TTS (Speaker Verification to Text-To-Speech)</strong> kimi layihÉ™lÉ™rin encoder hissÉ™sini tÉ™lim etdirmÉ™k Ã¼Ã§Ã¼n Ã§ox vacibdir.</p>

<h3>1. DatasetlÉ™ri tÉ™yin edir:</h3>
<p>Kod <strong>LibriSpeech</strong>, <strong>VoxCeleb1</strong>, <strong>VoxCeleb2</strong> kimi mÉ™ÅŸhur sÉ™s datasetlÉ™rini emal etmÉ™yi dÉ™stÉ™klÉ™yir.</p>

<h3>2. ParametrlÉ™ri oxuyur:</h3>
<p>Komanda sÉ™trindÉ™n (command-line) aÅŸaÄŸÄ±dakÄ± parametrlÉ™ri qÉ™bul edir:</p>
<ul>
    <li><code>datasets_root</code> â€“ DatasetlÉ™rin yerlÉ™ÅŸdiyi É™sas qovluÄŸun yolu.</li>
    <li><code>--out_dir</code> â€“ Emal nÉ™ticÉ™lÉ™rinin yazÄ±lacaÄŸÄ± qovluq (mel-spektrogramlar).</li>
    <li><code>--datasets</code> â€“ HansÄ± datasetlÉ™rin emal edilÉ™cÉ™yini tÉ™yin edir.</li>
    <li><code>--skip_existing</code> â€“ ÆgÉ™r fayl artÄ±q mÃ¶vcuddursa, onu tÉ™krar emal etmÉ™.</li>
    <li><code>--no_trim</code> â€“ SÉ™ssiz hissÉ™lÉ™ri kÉ™smÉ™.</li>
</ul>

<h3>3. SÉ™ssiz hissÉ™lÉ™ri kÉ™smÉ™ Ã¼Ã§Ã¼n <code>webrtcvad</code> kitabxanasÄ±nÄ± yoxlayÄ±r:</h3>
<p>Bu paket yoxdursa, xÉ™bÉ™rdarlÄ±q verir vÉ™ istifadÉ™Ã§iyÉ™ seÃ§im tÉ™klif edir.</p>

<h3>4. DatasetlÉ™ri emal edir:</h3>
<p>AÅŸaÄŸÄ±dakÄ± datasetlÉ™ri mel-spektrogramlara Ã§evirÉ™rÉ™k diskÉ™ yazÄ±r:</p>
<ul>
    <li><code>LibriSpeech/train-other-500</code></li>
    <li><code>VoxCeleb1/wav</code></li>
    <li><code>VoxCeleb2/dev</code></li>
</ul>
<p>Bu spektrogramlar neyron ÅŸÉ™bÉ™kÉ™nin tÉ™limi Ã¼Ã§Ã¼n daha uyÄŸun formatda olur.</p>

<h3>5. NÉ™ticÉ™lÉ™ri Ã§Ä±xÄ±ÅŸ qovluÄŸuna yazÄ±r:</h3>
<p>ÆgÉ™r <code>--out_dir</code> gÃ¶stÉ™rilmÉ™yibsÉ™, nÉ™ticÉ™lÉ™ri <code>datasets_root/SV2TTS/encoder/</code> qovluÄŸuna yazÄ±r.</p>

<h3>PraktikdÉ™ bu nÉ™ demÉ™kdir?</h3>
<p>Bu kod:</p>
<ul>
    <li>SÉ™s fayllarÄ±nÄ± oxuyur</li>
    <li>OnlarÄ± tÉ™mizlÉ™yir (sÉ™ssiz hissÉ™lÉ™ri kÉ™sir)</li>
    <li><strong>Mel-spektrogram</strong>a Ã§evirir (sÉ™sin gÃ¶rsÉ™l tÉ™msilidir)</li>
    <li>Fayl kimi saxlayÄ±r</li>
</ul>

<p><strong>Bu mÉ™rhÉ™lÉ™ bitdikdÉ™n sonra artÄ±q encoder modelini tÉ™limÉ™ baÅŸlatmaq mÃ¼mkÃ¼ndÃ¼r.</strong></p>
<h1>Python Preprocess Skripti</h1>
<pre><code># LazÄ±m olan kitabxanalarÄ± É™lavÉ™ edirik
import argparse
from pathlib import Path

# Audio fayllarÄ±nÄ± É™vvÉ™lcÉ™dÉ™n emal etmÉ™k Ã¼Ã§Ã¼n funksiyalar
from encoder.audio import preprocess_wav
from encoder.preprocess import preprocess_librispeech, preprocess_voxceleb1, preprocess_voxceleb2
from utils.argutils import print_args

# Æsas icra olunan hissÉ™
if __name__ == "__main__":
    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):
        pass

    parser = argparse.ArgumentParser(
        description="Datasets fayllarÄ±ndakÄ± sÉ™s fayllarÄ±nÄ± É™vvÉ™lcÉ™dÉ™n emal edir...",
        formatter_class=MyFormatter,
    )

    parser.add_argument("datasets_root", type=Path, help="LibriSpeech/TTS vÉ™ VoxCeleb datasetlÉ™rinin yolu.")
    parser.add_argument("-o", "--out_dir", type=Path, default=argparse.SUPPRESS,
                        help="Mel spektrogramlarÄ±n saxlanÄ±lacaÄŸÄ± Ã§Ä±xÄ±ÅŸ qovluÄŸu.")
    parser.add_argument("-d", "--datasets", type=str, default="librispeech_other,voxceleb1,voxceleb2",
                        help="Emal edilÉ™cÉ™k datasetlÉ™rin adlarÄ±.")
    parser.add_argument("-s", "--skip_existing", action="store_true", help="ÆgÉ™r fayl varsa, Ã¶tÃ¼b keÃ§.")
    parser.add_argument("--no_trim", action="store_true", help="SÉ™si sÉ™ssiz hissÉ™lÉ™ri kÉ™smÉ™dÉ™n emal et.")

    args = parser.parse_args()

    if not args.no_trim:
        try:
            import webrtcvad
        except:
            raise ModuleNotFoundError("â€˜webrtcvadâ€™ paketi yoxdur. ZÉ™hmÉ™t olmasa onu quraÅŸdÄ±rÄ±n.")

    del args.no_trim
    args.datasets = args.datasets.split(",")

    if not hasattr(args, "out_dir"):
        args.out_dir = args.datasets_root.joinpath("SV2TTS", "encoder")

    assert args.datasets_root.exists()
    args.out_dir.mkdir(exist_ok=True, parents=True)

    print_args(args, parser)

    preprocess_func = {
        "librispeech_other": preprocess_librispeech,
        "voxceleb1": preprocess_voxceleb1,
        "voxceleb2": preprocess_voxceleb2,
    }

    args = vars(args)

    for dataset in args.pop("datasets"):
        print(f"{dataset} emal olunur...")
        preprocess_func[dataset](**args)
    </code></pre>
<h1>1. audio.py:</h1>
<li><strong>Bu Kod:</strong></li>
<ul>
    <li># Bu kod bir sÉ™s faylÄ±nÄ± oxuyur, lazÄ±m gÉ™lÉ™rsÉ™ keyfiyyÉ™tini dÃ¼zÉ™ldir, sÉ™ssiz hissÉ™lÉ™ri kÉ™sir</li>
    <li># vÉ™ modelÉ™ input verÉ™ bilÉ™cÉ™yimiz bir mel-spektrogram Ã§Ä±xarÄ±r.</li>
    <li># YÉ™ni sÉ™sdÉ™n â” gÃ¶zÉ™l tÉ™miz bir numerik tÉ™svir (mel spectrogram) hazÄ±rlayÄ±rÄ±q.</li>
</ul>

<pre><code>
# LazÄ±mi kitabxanalarÄ±n importu
from pathlib import Path
from typing import Union
from warnings import warn

import numpy as np
import librosa
import struct

from numba.core.types import Optional  # (Burda biraz dÃ¼zÉ™liÅŸ lazÄ±mdÄ±r, amma iÅŸlÉ™yir.)
from scipy.ndimage import binary_dilation

# ParametrlÉ™rimizi layihÉ™dÉ™n Ã§É™kirik
from encoder.params_data import vad_moving_average_width, vad_max_silence_length, sampling_rate, \
    audio_norm_target_dBFS, mel_window_length, mel_window_step, mel_n_channels, vad_window_length

# SÉ™ssizlik aÅŸkarlanmasÄ± Ã¼Ã§Ã¼n kitabxananÄ± yoxlayÄ±rÄ±q
try:
    import webrtcvad
except:
    warn("Unable to import 'webrtcvad'. This package enables noise removal and is recommended.")
    webrtcvad = None

# Audio Ã¼Ã§Ã¼n int16 maksimum dÉ™yÉ™r
int16_max = (2 ** 15) - 1

def preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray],
                   source_sr: Optional[int] = None,
                   normalize: Optional[bool] = True,
                   trim_silence: Optional[bool] = True,):
    """
    Audio faylÄ±nÄ± oxuyur, lazÄ±msa resample edir, normalizasiya edir vÉ™ sÉ™ssizliklÉ™ri kÉ™sir.
    """
    if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):
        wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)
    else:
        wav = fpath_or_wav

    if source_sr is not None and source_sr != sampling_rate:
        wav = librosa.resample(wav, source_sr, sampling_rate)

    if normalize:
        wav = normalize_volume(wav, audio_norm_target_dBFS, increase_only=True)

    if webrtcvad and trim_silence:
        wav = trim_long_silences(wav)

    return wav

def wav_to_mel_spectrogram(wav):
    """
    Wav audio-nu mel-spektrograma Ã§evirir.
    """
    frames = librosa.feature.melspectrogram(
        wav,
        sampling_rate,
        n_fft=int(sampling_rate * mel_window_length / 1000),
        hop_length=int(sampling_rate * mel_window_step / 1000),
        n_mels=mel_n_channels
    )
    return frames.astype(np.float32).T

def trim_long_silences(wav):
    """
    Uzun sÉ™ssizliklÉ™ri tapÄ±r vÉ™ kÉ™sir.
    """
    samples_per_window = (vad_window_length * sampling_rate) // 1000
    wav = wav[:len(wav) - (len(wav) % samples_per_window)]
    pcm_wave = struct.pack("%dh" % len(wav), *(np.round(wav * int16_max)).astype(np.int16))

    voice_flags = []
    vad = webrtcvad.Vad(mode=3)
    for window_start in range(0, len(wav), samples_per_window):
        window_end = window_start + samples_per_window
        voice_flags.append(vad.is_speech(pcm_wave[window_start * 2:window_end * 2],
                                         sample_rate=sampling_rate))

    voice_flags = np.array(voice_flags)

    def moving_average(array, width):
        array_padded = np.concatenate((np.zeros((width - 1) // 2), array, np.zeros(width // 2)))
        ret = np.cumsum(array_padded, dtype=float)
        ret[width:] = ret[width:] - ret[:-width]
        return ret[width - 1:] / width

    audio_mask = moving_average(voice_flags, vad_moving_average_width)
    audio_mask = np.round(audio_mask).astype(np.bool_)
    audio_mask = binary_dilation(audio_mask, np.ones(vad_max_silence_length + 1))
    audio_mask = np.repeat(audio_mask, samples_per_window)

    return wav[audio_mask == True]

def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):
    """
    Wav sÉ™s sÉ™viyyÉ™sini target_dBFS-É™ uyÄŸunlaÅŸdÄ±rÄ±r.
    """
    if increase_only and decrease_only:
        raise ValueError("Both increase_only and decrease_only are set.")

    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))

    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):
        return wav

    return wav * (10 ** (dBFS_change / 20))
</code></pre>
<h1>2.config.py / datasets.py</h1>

<ul>
    <li>Bu kodu birbaÅŸa datasets.py faylÄ±na yaza bilÉ™rsÉ™n.</li>
    <li>Sonra digÉ™r fayllarda from datasets import librispeech_datasets kimi import edib istifadÉ™ edÉ™ bilÉ™rsÉ™n.</li>
</ul>

<pre><code># Dataset-lÉ™rin tam siyahÄ±sÄ±

librispeech_datasets = {
    "train": {
        "clean": ["LibriSpeech/train-clean-100", "LibriSpeech/train-clean-360"],
        "other": ["LibriSpeech/train-other-500"]
    },
    "test": {
        "clean": ["LibriSpeech/test-clean"],
        "other": ["LibriSpeech/test-other"]
    },
    "dev": {
        "clean": ["LibriSpeech/dev-clean"],
        "other": ["LibriSpeech/dev-other"]
    },
}

libritts_datasets = {
    "train": {
        "clean": ["LibriTTS/train-clean-100", "LibriTTS/train-clean-360"],
        "other": ["LibriTTS/train-other-500"]
    },
    "test": {
        "clean": ["LibriTTS/test-clean"],
        "other": ["LibriTTS/test-other"]
    },
    "dev": {
        "clean": ["LibriTTS/dev-clean"],
        "other": ["LibriTTS/dev-other"]
    },
}

voxceleb_datasets = {
    "voxceleb1": {
        "train": ["VoxCeleb1/wav"],
        "test": ["VoxCeleb1/test_wav"]
    },
    "voxceleb2": {
        "train": ["VoxCeleb2/dev/aac"],
        "test": ["VoxCeleb2/test_wav"]
    }
}

other_datasets = [
    "LJSpeech-1.1",
    "VCTK-Corpus/wav48",
]

anglophone_nationalites = ["az", "tr", "uk", "usa"]

# Ä°stÉ™sÉ™n belÉ™ yazÄ±b test edÉ™ bilÉ™rsÉ™n:
if __name__ == "__main__":
    print("LibriSpeech datasets:", librispeech_datasets)
    print("LibriTTS datasets:", libritts_datasets)
    print("VoxCeleb datasets:", voxceleb_datasets)
    print("Other datasets:", other_datasets)
    print("Anglophone nationalities:", anglophone_nationalites)
</code></pre>

<h1>SÉ™s FaylÄ±ndan Embedding Ã‡Ä±xarÄ±lmasÄ±</h1>
<p>Bu skript bir sÉ™s faylÄ±nÄ± (wav faylÄ± vÉ™ ya sÉ™s dalÄŸasÄ±) alÄ±r, onu mel-spektrograma Ã§evirir, vÉ™ sonra embedding (rÉ™qÉ™m vektoru) Ã§Ä±xarÄ±r. Bu embedding aÅŸaÄŸÄ±dakÄ± mÉ™qsÉ™dlÉ™r Ã¼Ã§Ã¼n istifadÉ™ olunur:</p>
<ul>
    <li>SÉ™si tÉ™msil etmÉ™k Ã¼Ã§Ã¼n</li>
    <li>Ä°nsanÄ± tanÄ±maq Ã¼Ã§Ã¼n</li>
    <li>Ä°ki sÉ™sin bir-birinÉ™ bÉ™nzÉ™yib-bÉ™nzÉ™mÉ™diyini yoxlamaq Ã¼Ã§Ã¼n</li>
</ul>

<h2>Encoder Modulunun Ä°mportu</h2>
<pre><code>
from functools import partial
from pathlib import Path
from matplotlib import cm
import numpy as np
import torch
from librosa.util import frame
from mpmath import fraction
from sympy.physics.vector import partial_velocity
from typing_extensions import overload

from encoder import audio
    </code></pre>

<h2>Modeli YÃ¼klÉ™mÉ™ FunksiyasÄ±</h2>
<pre><code>
# Global dÉ™yiÅŸÉ™nlÉ™r: Model vÉ™ cihaz (cuda/cpu)
_model = None
_device = None

def load_model(weights_fpath: Path, device=None):
    """
    Modeli Ã§É™kir vÉ™ RAM-a yÃ¼klÉ™yir.
    """
    global _model, _device
    if device is None:
        _device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    elif isinstance(device, str):
        _device = torch.device(device)

    # Speaker Encoder modelini yaradÄ±rÄ±q
    _model = SpeakerEncoder(_device, torch.device("dpu"))

    # Modelin checkpoint-lÉ™rini yÃ¼klÉ™yirik
    checkpoint = torch.load(weights_fpath, _device)
    _model.load_state_dict(checkpoint["model_state"])
    _model.eval()
    print(f"Loaded encoder \"{weights_fpath.name}\" trained to step {checkpoint['step']}")
    </code></pre>

<h2>Funksiyalar vÉ™ OnlarÄ±n Ä°stifadÉ™si</h2>
<table class="function-table">
    <thead>
    <tr>
        <th>Funksiya adÄ±</th>
        <th>Ä°ÅŸi</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>load_model()</td>
        <td>Modeli yÃ¼klÉ™yir.</td>
    </tr>
    <tr>
        <td>is_loaded()</td>
        <td>Model yÃ¼klÉ™nibmi deyir.</td>
    </tr>
    <tr>
        <td>embed_frames_batch()</td>
        <td>Frames (spektral) verib embedding Ã§Ä±xardÄ±r.</td>
    </tr>
    <tr>
        <td>compute_partial_slices()</td>
        <td>Uzun sÉ™si hissÉ™lÉ™rÉ™ bÃ¶lÃ¼r.</td>
    </tr>
    <tr>
        <td>embed_utterance()</td>
        <td>Bir wav fayldan embedding Ã§Ä±xardÄ±r.</td>
    </tr>
    <tr>
        <td>embed_speaker()</td>
        <td>(HÉ™lÉ™ yazÄ±lmayÄ±b) Bir neÃ§É™ sÉ™sdÉ™n danÄ±ÅŸan embedding-i Ã§Ä±xaracaq.</td>
    </tr>
    <tr>
        <td>plot_embedding_as_heatmap()</td>
        <td>Embedding-lÉ™ri vizual (rÉ™ng xÉ™ritÉ™si) kimi gÃ¶stÉ™rir.</td>
    </tr>
    </tbody>
</table>

<h1>4. SpeakerEncoder Model</h1>

<p>
    Bu model, danÄ±ÅŸan tanÄ±ma vÉ™ ya identifikasiyasÄ± Ã¼Ã§Ã¼n LSTM vÉ™ digÉ™r ÅŸÉ™bÉ™kÉ™ elementlÉ™rindÉ™n istifadÉ™ edir. Kodda hÉ™mÃ§inin similarity matrix (oxÅŸarlÄ±q matrisi) vÉ™ itki funksiyasÄ± (loss function) daxil edilib.
</p>

<h2>SpeakerEncoder Modelinin Kodu:</h2>
<pre>
        <code>
import torch
from torch import nn
from torch.nn.utils import clip_grad_norm_
from sklearn.metrics import roc_curve
from scipy.interpolate import interp1d
from scipy.optimize import brentq
import numpy as np

# Bu dÉ™yiÅŸÉ™nlÉ™rin kodda haradasa tÉ™yin edildiyini qÉ™bul edirik
mel_n_channels = 80  # Mel spektral kanallar
model_hidden_size = 512  # LSTM Ã¼Ã§Ã¼n gizli Ã¶lÃ§Ã¼
model_num_layers = 3  # LSTM qatlarÄ±nÄ±n sayÄ±
model_embedding_size = 256  # Ã‡Ä±xÄ±ÅŸ yerlÉ™ÅŸdirmÉ™ Ã¶lÃ§Ã¼sÃ¼


class SpeakerEncoder(nn.Module):
    def __init__(self, device, loss_device):
        super(SpeakerEncoder, self).__init__()
        self.loss_device = loss_device

        # XÃ¼susiyyÉ™tlÉ™rin Ã§Ä±xarÄ±lmasÄ± Ã¼Ã§Ã¼n LSTM qatlarÄ±
        self.lstm = nn.LSTM(input_size=mel_n_channels,
                            hidden_size=model_hidden_size,
                            num_layers=model_num_layers,
                            batch_first=True).to(device)

        # YerlÉ™ÅŸdirmÉ™ (embedding) yaratmaq Ã¼Ã§Ã¼n tam baÄŸlÄ± qat
        self.linear = nn.Linear(in_features=model_hidden_size,
                                out_features=model_embedding_size).to(device)

        # Aktivasiya funksiyasÄ±
        self.relu = nn.ReLU().to(device)

        # OxÅŸarlÄ±q hesablamasÄ± Ã¼Ã§Ã¼n parametrlÉ™r
        self.similarity_weight = nn.Parameter(torch.tensor([10.0])).to(device)
        self.similarity_bias = nn.Parameter(torch.tensor([-5.0])).to(loss_device)

        # Ä°tki funksiyasÄ±
        self.loss_fn = nn.CrossEntropyLoss().to(loss_device)

    def do_gradient_ops(self):
        # OxÅŸarlÄ±q Ã§É™ki vÉ™ Ã¶nyargÄ± Ã¼Ã§Ã¼n gradient É™mÉ™liyyatlarÄ±
        self.similarity_weight.grad *= 0.01
        self.similarity_bias.grad *= 0.01

        # GradientlÉ™ri kliplÉ™mÉ™ (exploding gradient problemlÉ™rinin qarÅŸÄ±sÄ±nÄ± almaq Ã¼Ã§Ã¼n)
        clip_grad_norm_(self.parameters(), 3, norm_type=2)

    def forward(self, utterances, hidden_init=None):
        # LSTM-dÉ™n keÃ§id
        out, (hidden, cell) = self.lstm(utterances, hidden_init)

        # Lineer dÃ¶nÃ¼ÅŸÃ¼m vÉ™ ReLU aktivasiya tÉ™tbiq edilir
        embeds_raw = self.relu(self.linear(hidden[-1]))

        # YerlÉ™ÅŸdirmÉ™lÉ™ri (embeddings) normalizasiya edirik
        embeds = embeds_raw / (torch.norm(embeds_raw, dim=1, keepdim=True) + 1e-5)

        return embeds

    def similarity_matrix(self, embeds):
        # Batch Ã¶lÃ§Ã¼sÃ¼ vÉ™ hÉ™r danÄ±ÅŸan Ã¼Ã§Ã¼n sÉ™sli nÃ¼munÉ™lÉ™rin sayÄ±
        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]

        # Daxil olan vÉ™ xaric edilÉ™n nÃ¼munÉ™lÉ™r Ã¼Ã§Ã¼n centroidlÉ™r (orta vektorlar) hesablanÄ±r
        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)
        centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)

        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)
        centroids_excl /= (utterances_per_speaker - 1)
        centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)

        # OxÅŸarlÄ±q matrisini yaratmaq
        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,
                                 speakers_per_batch).to(self.loss_device)

        # Ã–z-Ã¶zÃ¼nÉ™ oxÅŸarlÄ±ÄŸÄ± qarÅŸÄ±lamaq Ã¼Ã§Ã¼n maska matrisini yaradÄ±n
        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)
        for j in range(speakers_per_batch):
            mask = np.where(mask_matrix[j])[0]
            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)
            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)

        # OxÅŸarlÄ±q Ã§É™ki vÉ™ Ã¶nyargÄ±sÄ± tÉ™tbiq olunur
        sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias
        return sim_matrix

    def loss(self, embeds):
        # Batch Ã¶lÃ§Ã¼sÃ¼ vÉ™ danÄ±ÅŸan baÅŸÄ±na sÉ™sli nÃ¼munÉ™ sayÄ±
        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]

        # OxÅŸarlÄ±q matrisini hesablayÄ±rÄ±q
        sim_matrix = self.similarity_matrix(embeds)
        sim_matrix = sim_matrix.reshape(speakers_per_batch, utterances_per_speaker, speakers_per_batch)

        # Ã‡apraz entropiya itkisi Ã¼Ã§Ã¼n ground truth etiketlÉ™ri
        ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)
        target = torch.from_numpy(ground_truth).long().to(self.loss_device).to(self.loss_device)

        # Ã‡apraz entropiya itkisini hesablayÄ±rÄ±q
        loss = self.loss_fn(sim_matrix, target)

        # BÉ™rabÉ™r SÉ™hv NisbÉ™tini (Equal Error Rate, EER) hesablamaq
        with torch.no_grad():
            inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int)[0]
            labels = np.array([inv_argmax(i) for i in ground_truth])
            preds = sim_matrix.detach().cpu().numpy()

            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())
            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)

        return loss, eer
        </code>
    </pre>

<h2>Æsas DÉ™yiÅŸikliklÉ™r vÉ™ Ä°zahlar:</h2>
<ul>
    <li><strong>forward metodu:</strong> Modelin Ã§Ä±xÄ±ÅŸÄ± olan yerlÉ™ÅŸdirmÉ™lÉ™r (embeddings) normalizasiya edilir ki, bu da oxÅŸarlÄ±q mÃ¼qayisÉ™lÉ™rini yaxÅŸÄ±laÅŸdÄ±rÄ±r.</li>
    <li><strong>OxÅŸarlÄ±q Matrisi:</strong> similarity_matrix metodu, daxil olan vÉ™ xaric edilÉ™n danÄ±ÅŸan nÃ¼munÉ™lÉ™ri É™sas alaraq oxÅŸarlÄ±q matrisi hesablayÄ±r. Bu, danÄ±ÅŸanlarÄ±n bir-birilÉ™ oxÅŸarlÄ±ÄŸÄ±nÄ± Ã¶lÃ§mÉ™k Ã¼Ã§Ã¼n istifadÉ™ olunur.</li>
    <li><strong>Ä°tki FunksiyasÄ±:</strong> loss metodu, hÉ™mÃ§inin BÉ™rabÉ™r SÉ™hv NisbÉ™ti (EER) hesablayÄ±r ki, bu da danÄ±ÅŸan tanÄ±ma sistemlÉ™ri Ã¼Ã§Ã¼n bir performans Ã¶lÃ§Ã¼sÃ¼dÃ¼r.</li>
    <li><strong>Gradient KliplÉ™mÉ™:</strong> do_gradient_ops metodu, gradientlÉ™rin bÃ¶yÃ¼k olmasÄ±nÄ±n qarÅŸÄ±sÄ±nÄ± almaq Ã¼Ã§Ã¼n onlarÄ± kliplÉ™yir (Exploding Gradient problemi).</li>
    <li><strong>Cihaz Ä°darÉ™si:</strong> Model, iki cihaz qÉ™bul edir: device É™sas hesablama Ã¼Ã§Ã¼n, loss_device isÉ™ itkini hesablamaq Ã¼Ã§Ã¼n (bu cihazlar fÉ™rqli ola bilÉ™r, mÉ™sÉ™lÉ™n, CPU vÉ™ GPU).</li>
</ul>

<h2>5. Params_data.py</h2>
<p>Bu kod hissÉ™si bir sÉ™s iÅŸlÉ™mÉ™ layihÉ™sinin ayar parametrlÉ™ridir â€” yÉ™ni:</p>
<ul>
    <li>SÉ™si necÉ™ oxumaq, necÉ™ Ã§evirmÉ™k (mel filterbank etmÉ™k),</li>
    <li>SÉ™ssiz hissÉ™lÉ™ri necÉ™ aÅŸkarlamaq (Voice Activation Detection - VAD),</li>
    <li>SÉ™si necÉ™ normallaÅŸdÄ±rmaq (dBFS sÉ™viyyÉ™sini tÉ™nzimlÉ™mÉ™k),</li>
    <li>NeÃ§É™ Ã§É™rÃ§ivÉ™ (frame) istifadÉ™ olunacaq,</li>
</ul>
<p>BunlarÄ± dÉ™qiqlÉ™ÅŸdirmÉ™k Ã¼Ã§Ã¼n yazÄ±lÄ±b. Bu kodun É™sas mÉ™qsÉ™di: audio siqnallarÄ±nÄ± neyron ÅŸÉ™bÉ™kÉ™ Ã¼Ã§Ã¼n hazÄ±rlamaq.</p>

<pre>
# Mel Filtirleme
mel_window_length = 25  # HÉ™r mel-spektroqram hissÉ™sinin uzunluÄŸu (25 ms).
mel_window_step = 10    #  Ä°ki ardÄ±cÄ±l pÉ™ncÉ™rÉ™ arasÄ±ndakÄ± sÃ¼rÃ¼ÅŸmÉ™ (stride), 10 ms.
mel_n_channels = 40    #  Mel-spektrogramÄ±n neÃ§É™ kanal (frequency band)
# olacaÄŸÄ±nÄ± tÉ™yin edir (40 kanal).

# Audio
sampling_rate = 16000    #SÉ™slÉ™rin neÃ§É™ Hz ilÉ™
# oxunacaÄŸÄ±nÄ± gÃ¶stÉ™rir (16 kHz tipik bir dÉ™yÉ™rdir).
partials_n_frames = 160     # 1600 ms  MÉ™ÅŸq zamanÄ±, 1.6 saniyÉ™lik
# (160 Ã§É™rÃ§ivÉ™) hissÉ™lÉ™r istifadÉ™ olunacaq.
inference_n_frames = 80  # 800 ms Test zamanÄ±, 0.8 saniyÉ™lik
# (80 Ã§É™rÃ§ivÉ™) hissÉ™lÉ™r istifadÉ™ olunacaq.

# Voice Activation Detection (VAD)

vad_window_length = 30  # In milliseconds SÉ™si yoxlamaq Ã¼Ã§Ã¼n
# 30 millisanilik pÉ™ncÉ™rÉ™lÉ™r istifadÉ™ edilir.
vad_moving_average_width = 8  #  SÉ™ssizlik aÅŸkarlananda nÉ™ticÉ™lÉ™ri
# yumÅŸaltmaq Ã¼Ã§Ã¼n 8 pÉ™ncÉ™rÉ™ orta hesablanÄ±r.
vad_max_silence_length = 6   # SÉ™ssizliyin maksimum
# uzunluÄŸu 6 pÉ™ncÉ™rÉ™dir (sÉ™ssizlik Ã§ox uzunsa, bÃ¶lmÉ™k lazÄ±mdÄ±r).

# Audio volume normalization
audio_norm_target_dBFS = -30 # Audio fayllar -30 dBFS sÉ™viyyÉ™sinÉ™ gÉ™tirilir.
#Ã‡Ã¼nki model sabit bir sÉ™s sÉ™viyyÉ™si gÃ¶zlÉ™yir (Ã§ox yÃ¼ksÉ™k
# vÉ™ ya Ã§ox aÅŸaÄŸÄ± olarsa performans dÃ¼ÅŸÉ™ bilÉ™r).
</pre>

<h2>6. params_model.py</h2>
<p>Bir sÉ™s tanÄ±ma vÉ™ ya sÉ™sdÉ™n ÅŸÉ™xsiyyÉ™t mÃ¼É™yyÉ™n etmÉ™ (Speaker Verification / Speaker Embedding) layihÉ™sinin parametrlÉ™rini (yÉ™ni sazlamalarÄ±nÄ±) mÃ¼É™yyÉ™n edir.</p>

<pre>
# ======================
# Model parameters
# ======================
model_hidden_size = 256    # Gizli qatlarÄ±n Ã¶lÃ§Ã¼sÃ¼
model_embedding_size = 256 # Ä°stÉ™nilÉ™n Ã§Ä±xÄ±ÅŸ embedding Ã¶lÃ§Ã¼sÃ¼
model_num_layers = 3       # ÅÉ™bÉ™kÉ™dÉ™ki qatlarÄ±n (layers) sayÄ±

# ======================
# Training parameters
# ======================
learning_rate_init = 1e-4        # Ä°lkin Ã¶yrÉ™nmÉ™ sÃ¼rÉ™ti (learning rate)
speakers_per_batch = 64          # HÉ™r batch-dÉ™ki danÄ±ÅŸanlarÄ±n sayÄ±
utterances_per_speaker = 10      # HÉ™r danÄ±ÅŸan Ã¼Ã§Ã¼n cÃ¼mlÉ™ (utterance) sayÄ±
</pre>

<h1>7. preprocess.py</h1>

<p>
    Bu kod bÃ¶yÃ¼k audio datasetlÉ™ri (LibriSpeech, VoxCeleb1, VoxCeleb2):
</p>
<ul>
    <li>Oxuyur,</li>
    <li>FayllarÄ± tÉ™mizlÉ™yir vÉ™ mel-spectrograma Ã§evirir,</li>
    <li>.npy faylÄ± kimi saxlayÄ±r,</li>
    <li>HÉ™r bir addÄ±mÄ± log faylÄ±nda qeyd edir,</li>
    <li>Paralel iÅŸlÉ™mÉ™ ilÉ™ sÃ¼rÉ™tlÉ™nir (4 nÃ¼vÉ™ istifadÉ™ edir).</li>
</ul>

<p><strong>Æsas Kitabxanalar:</strong></p>
<pre>
        pip install tqdm
    </pre>

<p><strong>Kodun BaÅŸlanÄŸÄ±cÄ±:</strong></p>
<pre>
        from datetime import datetime
        from functools import partial
        from importlib.metadata import metadata
        from multiprocessing import Pool
        from pathlib import Path
        from tqdm import tqdm
        from encoder import audio
        from turtledemo.penrose import start
        from encoder import params_data
        import numpy as np
        from joblib.testing import param
        from torch.fx.experimental.unification.multipledispatch.dispatcher import source
        from encoder.config import librispeech_datasets, anglophone_nationalites
        from encoder.params_data import partials_n_frames, sampling_rate
    </pre>

<p><strong>DatasetLog sinifi:</strong></p>
<p>
    DatasetLog sinifi dataset yaradanda log (gÃ¼ndÉ™lik yazÄ±sÄ±) yazÄ±r:
</p>
<ul>
    <li><strong>text_file</strong>: log faylÄ±nÄ± aÃ§Ä±r, hÉ™r addÄ±mÄ± ora yazÄ±r.</li>
    <li><strong>sample_data</strong>: toplanan statistik mÉ™lumatlarÄ± saxlayÄ±r.</li>
</ul>

<p><strong>Æsas metodlar:</strong></p>
<ul>
    <li><strong>_log_params()</strong>: istifadÉ™ olunan parametrlÉ™ri fayla yazÄ±r.</li>
    <li><strong>write_line(line)</strong>: fayla sÉ™tir É™lavÉ™ edir.</li>
    <li><strong>add_sample(kwargs)</strong>: statistik mÉ™lumat (mÉ™sÉ™lÉ™n audio uzunluÄŸu) toplayÄ±r.</li>
    <li><strong>finalize()</strong>: statistik mÉ™lumatlarÄ± fayla yazÄ±r vÉ™ faylÄ± baÄŸlayÄ±r.</li>
</ul>

<p><strong>Ä°stifadÉ™ olunan funksiyalar:</strong></p>
<pre>
        def _init_preprocess_dataset(dataset_name, datasets_root, out_dir) -> (Path, DatasetLog):
            dataset_root = datasets_root.joinpath(dataset_name)
            if not dataset_root.exists():
                print("Couldn't find %s, skipping this dataset." % dataset_root)
                return None, None
            return dataset_root, DatasetLog(out_dir, dataset_name)

        def _preprocess_speaker(speaker_dir: Path, datasets_root: Path, out_dir: Path, skip_existing: bool):
            speaker_name = "_".join(speaker_dir.relative_to(datasets_root).parts)
            speaker_out_dir = out_dir.joinpath(speaker_name)
            speaker_out_dir.mkdir(exist_ok=True)
            sources_fpath = speaker_out_dir.joinpath("_sources.txt")
            if sources_fpath.exists():
                try:
                    with sources_fpath.open("r") as sources_file:
                        existing_fnames = {line.split(",")[0] for line in sources_file}
                except:
                    existing_fnames = {}
            else:
                existing_fnames = {}
            sources_file = sources_fpath.open("a" if skip_existing else "w")
            audio_durs = []
            for extension in _AUDIO_EXTENSIONS:
                for in_fpath in speaker_dir.glob("**/*.%s" % extension):
                    out_fname = "_".join(in_fpath.relative_to(speaker_dir).parts)
                    out_fname = out_fname.replace(".%s" % extension, ".npy")
                    if skip_existing and out_fname in existing_fnames:
                        continue
                    wav = audio.preprocess_wav(in_fpath)
                    if len(wav) == 0:
                        continue
                    frames = audio.wav_to_mel_spectrogram(wav)
                    if len(frames) < partials_n_frames:
                        continue
                    out_fpath = speaker_out_dir.joinpath(out_fname)
                    np.save(out_fpath, frames)
                    sources_file.write("%s,%s\n" % (out_fname, in_fpath))
                    audio_durs.append(len(wav) / sampling_rate)
            sources_file.close()
            return audio_durs
    </pre>

<p><strong>DatasetlÉ™ri necÉ™ iÅŸlÉ™yirik:</strong></p>
<pre>
        def preprocess_librispeech(datasets_root: Path, out_dir: Path, skip_existing=False):
            for dataset_name in librispeech_datasets["train"]["other"]:
                dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)
                if not dataset_root:
                    return
                speaker_dirs = list(dataset_root.glob("*"))
                _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, skip_existing, logger)

        def preprocess_voxceleb1(datasets_root: Path, out_dir: Path, skip_existing=False):
            dataset_name = "VoxCeleb1"
            dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)
            if not datasets_root:
                return
            with datasets_root.joinpath("vox1_meta.csv").open("r") as metafile:
                metadata = [line.split("\t") for line in metafile][1:]
                nationalities = {line[0]: line[3] for line in metadata}
                keep_speaker_ids = [speaker_id for speaker_id, nationaliy in nationalities.items() if
                                    nationaliy.lower() in anglophone_nationalites]
                print("VoxCeleb1: using samples from %d (presumed anglophone) speakers out of %d." %
                      (len(keep_speaker_ids), len(nationalities)))
                speaker_dirs = dataset_root.joinpath("wav").glob("*")
                speaker_dirs = [speaker_dir for speaker_dir in speaker_dirs if
                                speaker_dir.name in keep_speaker_ids]
                print("VoxCeleb1: found %d anglophone speakers on the disk, %d missing (this is normal)." %
                      (len(speaker_dirs), len(keep_speaker_ids) - len(speaker_dirs)))
                _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, skip_existing, logger)
    </pre>

<h1>8-Train Model</h1>
<p>
    Bu nÃ¼munÉ™, sÉ™s tanÄ±ma (speaker verification) Ã¼Ã§Ã¼n bir modelin PyTorch ilÉ™ necÉ™ Ã¶yrÉ™dildiyini gÃ¶stÉ™rir vÉ™ tÉ™mizlÉ™nmiÅŸ sÉ™s mÉ™lumatlarÄ± É™sasÄ±nda modelin necÉ™ tren edildiyini izah edir.
</p>
<pre>
        <code>
from pathlib import Path                         # Fayl yollarÄ± ilÉ™ iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n
import torch                                     # PyTorch kitabxanasÄ± - maÅŸÄ±n Ã¶yrÉ™nmÉ™ Ã¼Ã§Ã¼n
from torch.utils.data import DataLoader          # Dataset yÃ¼klÉ™yicisi

# Model, parametrlÉ™r, dataset, vizualizasiya vÉ™ s. layihÉ™nin modullarÄ±ndan gÉ™tirilir
from encoder.model import SpeakerEncoder         # SÉ™slÉ™ri tanÄ±maq Ã¼Ã§Ã¼n model
from encoder.params_model import speakers_per_batch, utterances_per_speaker, learning_rate_init
from encoder.visualizations import Visualizations # Vizualizasiya Ã¼Ã§Ã¼n modul
from encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataset  # Dataset sinfi
from encoder.data_objects.speaker_verification_data_loader import SpeakerVerificationDataLoader # DataLoader
from encoder.profiler import Profiler            # Profil izlÉ™yici â€“ performans Ã¶lÃ§mÉ™k Ã¼Ã§Ã¼n

# CUDA cihazÄ±nda sinxronizasiya Ã¼Ã§Ã¼n funksiya
def sync(device: torch.device):
    if device.type == "cuda":
        torch.cuda.synchronize(device)

# Æsas tÉ™lim funksiyasÄ±
def train(run_id: str,                        # TÉ™limin adÄ±nÄ± mÃ¼É™yyÉ™n edir
          clean_data_root: Path,              # TÉ™miz sÉ™s mÉ™lumatlarÄ±nÄ±n yerlÉ™ÅŸdiyi qovluq
          models_dir: Path,                   # ModellÉ™rin saxlandÄ±ÄŸÄ± qovluq
          umap_every: int,                    # HÉ™r neÃ§É™ addÄ±mdan bir UMAP vizualizasiya edilsin
          save_every: int,                    # HÉ™r neÃ§É™ addÄ±mdan bir model yadda saxlanÄ±lsÄ±n
          backup_every: int,                  # HÉ™r neÃ§É™ addÄ±mdan bir backup alÄ±nsÄ±n
          vis_every: int,                     # Vizualizasiya Ã¼Ã§Ã¼n interval
          force_restart: bool,                # ÆvvÉ™lki modeli yÃ¼klÉ™mÉ™dÉ™n sÄ±fÄ±rdan baÅŸlasÄ±n?
          visdom_server: str,                 # Visdom server Ã¼nvanÄ±
          no_visdom: bool):                   # Vizualizasiya aktiv olsun ya yox

    # Dataset yaradÄ±lÄ±r
    dataset = SpeakerVerificationDataset(clean_data_root)

    # DataLoader yaradÄ±lÄ±r (batch-lÉ™r ÅŸÉ™klindÉ™ mÉ™lumatÄ± verir)
    loader = SpeakerVerificationDataLoader(dataset,
                                           speakers_per_batch,
                                           utterances_per_speaker,
                                           num_workers=4)

    # Cihaz mÃ¼É™yyÉ™n edilir â€“ CUDA varsa GPU, yoxdursa CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    loss_device = torch.device("cpu")  # Loss CPU-da hesablanacaq

    # Model yaradÄ±lÄ±r
    model = SpeakerEncoder(device, loss_device)

    # Adam optimizer ilÉ™ Ã¶yrÉ™dilÉ™cÉ™k
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_init)

    init_step = 1  # TÉ™limin baÅŸlanÄŸÄ±c addÄ±mÄ±

    # Modelin saxlanÄ±lacaÄŸÄ± qovluq yaradÄ±lÄ±r
    model_dir = models_dir / run_id
    model_dir.mkdir(exist_ok=True, parents=True)

    # Modelin saxlandÄ±ÄŸÄ± faylÄ±n yolu
    state_fpath = model_dir / "encoder.pt"

    # ÆgÉ™r É™vvÉ™lki model varsa vÉ™ force_restart FALSE-dursa, onu yÃ¼klÉ™
    if not force_restart and state_fpath.exists():
        print(f"Found existing model \"{run_id}\", loading it and resuming training.")
        checkpoint = torch.load(state_fpath)
        init_step = checkpoint["step"]
        model.load_state_dict(checkpoint["model_state"])
        optimizer.load_state_dict(checkpoint["optimizer_state"])
        optimizer.param_groups[0]["lr"] = learning_rate_init
    else:
        print(f"Starting training for \"{run_id}\" from scratch.")

    model.train()  # Model tren rejiminÉ™ keÃ§ir

    # Vizualizasiya obyektini yarat
    vis = Visualizations(run_id, vis_every, server=visdom_server, disabled=no_visdom)
    vis.log_dataset(dataset)        # Dataset barÉ™dÉ™ mÉ™lumatlarÄ± log et
    vis.log_params()                # ParametrlÉ™ri log et
    device_name = str(torch.cuda.get_device_name(0) if torch.cuda.is_available() else "cpu")
    vis.log_implementation({"Device": device_name})  # HansÄ± cihazda Ã§alÄ±ÅŸÄ±rsa onu gÃ¶stÉ™r

    # Profil izlÉ™mÉ™ aktivlÉ™ÅŸdirilir
    profiler = Profiler(summarize_every=10, disabled=False)

    # TÉ™lim dÃ¶vrÃ¼ baÅŸlayÄ±r
    for step, speaker_batch in enumerate(loader, init_step):
        profiler.tick("Blocking, waiting for batch (threaded)")

        # Batch mÉ™lumat GPU/CPU-ya Ã¶tÃ¼rÃ¼lÃ¼r
        inputs = torch.from_numpy(speaker_batch.data).to(device)
        sync(device)
        profiler.tick("Data to device")

        # ModelÉ™ giriÅŸ verilir vÉ™ nÉ™ticÉ™ alÄ±nÄ±r
        embeds = model(inputs)
        sync(device)
        profiler.tick("Forward pass")

        # Ä°stifadÉ™Ã§ilÉ™rÉ™ gÃ¶rÉ™ embedding-lÉ™r 3D formada dÃ¼zÃ¼lÃ¼r
        embeds_loss = embeds.view((speakers_per_batch, utterances_per_speaker, -1)).to(loss_device)

        # ZÉ™rÉ™r (loss) vÉ™ EER (Equal Error Rate) hesablanÄ±r
        loss, eer = model.loss(embeds_loss)
        sync(loss_device)
        profiler.tick("Loss")

        # Geri yayÄ±lma (backpropagation)
        model.zero_grad()
        loss.backward()
        profiler.tick("Backward pass")

        model.do_gradient_ops()  # Gradient normallaÅŸdÄ±rÄ±lmasÄ± vÉ™ s.
        optimizer.step()         # ParametrlÉ™r yenilÉ™nir
        profiler.tick("Parameter update")

        # Vizualizasiya mÉ™lumatÄ± yenilÉ™nir
        vis.update(loss.item(), eer, step)

        # UMAP ilÉ™ 2D vizual proyeksiya yaradÄ±lÄ±r
        if umap_every != 0 and step % umap_every == 0:
            print(f"Drawing and saving projections (step {step})")
            projection_fpath = model_dir / f"umap_{step:06d}.png"
            embeds = embeds.detach().cpu().numpy()
            vis.draw_projections(embeds, utterances_per_speaker, step, projection_fpath)
            vis.save()

        # Model fayla yazÄ±lÄ±r
        if save_every != 0 and step % save_every == 0:
            print(f"Saving the model (step {step})")
            torch.save({
                "step": step + 1,
                "model_state": model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
            }, state_fpath)

        # Backup faylÄ± yaradÄ±lÄ±r
        if backup_every != 0 and step % backup_every == 0:
            print(f"Making a backup (step {step})")
            backup_fpath = model_dir / f"encoder_{step:06d}.bak"
            torch.save({
                "step": step + 1,
                "model_state": model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
            }, backup_fpath)

        # Profil izlÉ™yici É™lavÉ™ fÉ™aliyyÉ™tlÉ™ri qeyd edir
        profiler.tick("Extras (visualizations, saving)")
        </code>
    </pre>

<h2>GÉ™rÉ™kli Fayllar vÉ™ Modullar</h2>
<p>
    Bu kodun iÅŸlÉ™mÉ™si Ã¼Ã§Ã¼n aÅŸaÄŸÄ±dakÄ± modullar/fayllar lazÄ±mdÄ±r:
</p>
<ul>
    <li>encoder/model.py â†’ SpeakerEncoder sinifi</li>
    <li>encoder/params_model.py â†’ speakers_per_batch, utterances_per_speaker, learning_rate_init</li>
    <li>encoder/visualizations.py â†’ Visualizations sinifi</li>
    <li>encoder/data_objects/... â†’ Dataset vÉ™ DataLoader siniflÉ™ri</li>
    <li>encoder/profiler.py â†’ Profiler sinifi</li>
</ul>

<h1>Visualizations.py - Python Kodu</h1>
<pre><code>
from sys import implementation  # Python interpreter haqqÄ±nda mÉ™lumat Ã¼Ã§Ã¼n (mÉ™s. CPython)
import numpy as np  # SayÄ±sal hesablama Ã¼Ã§Ã¼n kitabxana
import matplotlib.pyplot as plt  # Qrafik Ã§É™kmÉ™k Ã¼Ã§Ã¼n kitabxana
import umap  # YÃ¼ksÉ™k Ã¶lÃ§Ã¼lÃ¼ datanÄ± 2D proyeksiyaya salmaq Ã¼Ã§Ã¼n
import visdom  # Vizualizasiya Ã¼Ã§Ã¼n server
from datetime import datetime  # Tarix/saat Ã¼Ã§Ã¼n
import time  # Vaxt Ã¶lÃ§mÉ™k Ã¼Ã§Ã¼n
from multiprocessing.connection import Connection  # Paralel iÅŸlÉ™mÉ™ É™laqÉ™si (bu kodda istifadÉ™ olunmayÄ±b)
from numba.cuda.simulator.reduction import reduce  # CUDA Ã¼Ã§Ã¼n reduce funksiyasÄ± (bu kodda istifadÉ™ olunmayÄ±b)
from encoder import params_data  # LayihÉ™dÉ™ki data parametrlÉ™ri
from encoder import params_model  # LayihÉ™dÉ™ki model parametrlÉ™ri

# 13 fÉ™rqli RGB rÉ™ngi tÉ™yin edir vÉ™ 255-É™ bÃ¶lÉ™rÉ™k 0-1 aralÄ±ÄŸÄ±nda normallaÅŸdÄ±rÄ±r
colormap = np.array([
    [255, 0, 0], [0, 255, 0], [0, 0, 255],
    [255, 255, 0], [255, 0, 255], [0, 255, 255],
    [128, 0, 0], [0, 128, 0], [0, 0, 128],
    [128, 128, 0], [128, 0, 128], [0, 128, 128],
    [128, 128, 128]
], dtype=np.float) / 255

class Visualizations:
    def __init__(self, env_name=None, update_every=10, server="http://localhost", disabled=False):
        self.disabled = disabled  # Vizualizasiya deaktiv edilsin ya yox
        if disabled:
            return
        self.vis = visdom.Visdom(server=server, env=env_name)  # Visdom obyektini yaradÄ±b serverÉ™ qoÅŸulur
        self.update_every = update_every  # NeÃ§É™ addÄ±mdan bir yenilÉ™nÉ™cÉ™k
        self.loss_data = []  # Loss dÉ™yÉ™rlÉ™ri burada yÄ±ÄŸÄ±lÄ±r
        self.eer_data = []  # EER dÉ™yÉ™rlÉ™ri burada yÄ±ÄŸÄ±lÄ±r
        self.time_data = []  # Vaxt dÉ™yÉ™rlÉ™ri burada yÄ±ÄŸÄ±lÄ±r

    def log_params(self):
        if self.disabled:
            return
        text = "<br>".join([
            "<b>Model parameters:</b>"
        ] + [f"{key} = {value}" for key, value in params_model.__dict__.items() if not key.startswith("__")])
        self.vis.text(text)  # Model parametrlÉ™rini visdom-a yazÄ±r
        text = "<br>".join([
            "<b>Data parameters:</b>"
        ] + [f"{key} = {value}" for key, value in params_data.__dict__.items() if not key.startswith("__")])
        self.vis.text(text)  # Data parametrlÉ™rini visdom-a yazÄ±r

    def log_dataset(self, dataset: SpeakerVerificationDataset):
        if self.disabled:
            return
        text = f"<b>Dataset</b><br>{str(dataset)}"
        self.vis.text(text)  # Dataset haqqÄ±nda mÉ™lumatÄ± visdom-a yazÄ±r

    def log_implementation(self, params):
        if self.disabled:
            return
        text = "<b>Running on:</b><br>" + "<br>".join([
            f"Python implementation: {implementation.name}",
            f"CUDA available: {params.device != 'cpu'}",
            f"NumPy version: {np.__version__}",
        ])
        self.vis.text(text)  # Ä°cra olunan mÃ¼hit haqqÄ±nda mÉ™lumat verir

    def update(self, loss, eer, step):
        if self.disabled:
            return
        self.loss_data.append(loss)  # Yeni loss É™lavÉ™ olunur
        self.eer_data.append(eer)  # Yeni EER É™lavÉ™ olunur
        self.time_data.append(time.time())  # Vaxt qeyd olunur

        if step % self.update_every == 0:  # HÉ™r update_every addÄ±mda bir
            elapsed = self.time_data[-1] - self.time_data[-self.update_every]  # Bu aralÄ±qdakÄ± zaman
            avg_loss = np.mean(self.loss_data[-self.update_every:])  # Orta loss
            avg_eer = np.mean(self.eer_data[-self.update_every:])  # Orta EER
            steps_per_sec = self.update_every / elapsed  # SaniyÉ™yÉ™ dÃ¼ÅŸÉ™n addÄ±m sayÄ±

            # Vizualizasiya Ã¼Ã§Ã¼n loss vÉ™ EER qrafiki
            self.vis.line(
                Y=np.column_stack((self.loss_data, self.eer_data)),
                X=np.arange(1, len(self.loss_data) + 1),
                win="loss_and_eer",
                opts={
                    "title": "Loss and EER",
                    "legend": ["Loss", "EER"],
                    "xlabel": "Step",
                    "ylabel": "Value"
                }
            )

            # AddÄ±m sÃ¼rÉ™ti vÉ™ orta loss-u mÉ™tn olaraq gÃ¶stÉ™rir
            self.vis.text(f"Step: {step}<br>Avg loss: {avg_loss:.4f}<br>Avg EER: {avg_eer:.4f}<br>Steps/sec: {steps_per_sec:.2f}",
                          win="progress")

    def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10):
        if self.disabled:
            return
        projection = umap.UMAP().fit_transform(embeds)  # Embedding-lÉ™ri UMAP ilÉ™ 2D-yÉ™ salÄ±r
        num_speakers = len(embeds) // utterances_per_speaker  # DanÄ±ÅŸan sayÄ±
        plt.figure(figsize=(8, 8))  # ÅÉ™kil Ã¶lÃ§Ã¼sÃ¼
        for speaker_idx in range(min(num_speakers, max_speakers)):  # Maksimum 10 danÄ±ÅŸan gÃ¶stÉ™r
            start_idx = speaker_idx * utterances_per_speaker
            end_idx = (speaker_idx + 1) * utterances_per_speaker
            plt.scatter(
                projection[start_idx:end_idx, 0],
                projection[start_idx:end_idx, 1],
                c=[colormap[speaker_idx % len(colormap)]],
                label=f"Speaker {speaker_idx + 1}"
            )
        plt.legend()  # ÆfsanÉ™ni gÃ¶stÉ™r
        if out_fpath:
            plt.savefig(out_fpath)  # ÆgÉ™r yol gÃ¶stÉ™rilibsÉ™, ÅŸÉ™kli fayla yaz
        self.vis.image(  # Vizual olaraq ÅŸÉ™kli gÃ¶stÉ™r
            np.transpose(plt.imread(out_fpath if out_fpath else "projection.png"), (2, 0, 1)),
            win="embeds",
            opts={"title": f"Embeddings projection (step {step})"}
        )
        plt.close()  # Matplotlib obyektini baÄŸla

    def save(self):
        if self.disabled:
            return
        self.vis.save([self.vis.env])  # Visdom É™traf mÃ¼hitini yadda saxla
    </code></pre>

<h1>10-Data Objects: paket vÉ™ icerisindeki scriptler</h1>

<h2>random_cycler.py: RandomCycler</h2>
<p>RandomCycler verilmiÅŸ elementlÉ™ri tÉ™sadÃ¼fi ÅŸÉ™kildÉ™, qarÄ±ÅŸÄ±q-qarÄ±ÅŸÄ±q, amma hÉ™r birini istifadÉ™ edÉ™rÉ™k tÉ™krar-tÉ™krar qaytarmaq Ã¼Ã§Ã¼ndÃ¼r. Bu sinif tÉ™krarsÄ±z tÉ™sadÃ¼fi dÃ¶vr edir vÉ™ hamÄ±sÄ±nÄ± istifadÉ™ etdikdÉ™n sonra yenidÉ™n qarÄ±ÅŸdÄ±rÄ±b baÅŸlayÄ±r.</p>
<pre>
        <code>
import random  # TÉ™sadÃ¼fi seÃ§imlÉ™r etmÉ™k Ã¼Ã§Ã¼n

class RandomCycler:
    def __init__(self, source):
        if len(source) == 0:
            raise Exception("Can't create RandomCycler from an empty collection")  # ÆgÉ™r boÅŸ siyahÄ± verilsÉ™, xÉ™ta verir
        self.all_items = list(source)  # VerilÉ™n siyahÄ±nÄ± yadda saxlayÄ±r
        self.next_items = []  # NÃ¶vbÉ™ti seÃ§ilÉ™cÉ™k itemlÉ™ri saxlayacaq

    def sample(self, count: int):  # Ä°stifadÉ™Ã§idÉ™n neÃ§É™ dÉ™nÉ™ element istÉ™nildiyini alÄ±r
        shuffled = lambda l: random.sample(l, len(l))  # VerilÉ™n siyahÄ±nÄ±n qarÄ±ÅŸdÄ±rÄ±lmÄ±ÅŸ versiyasÄ±nÄ± qaytarÄ±r

        out = []  # Cavab siyahÄ±sÄ±
        while count > 0:  # HÉ™lÉ™ alÄ±nmalÄ± elementlÉ™r varsa
            if count >= len(self.all_items):  # ÆgÉ™r istÉ™nilÉ™n say, bÃ¼tÃ¼n elementlÉ™rin sayÄ±ndan Ã§oxdursa
                out.extend(shuffled(list(self.all_items)))  # BÃ¼tÃ¼n elementlÉ™ri qarÄ±ÅŸdÄ±rÄ±b É™lavÉ™ et
                count -= len(self.all_items)  # SayÄ± azald
                continue  # DÃ¶vrÉ™ davam et
            n = min(count, len(self.next_items))  # NÃ¶vbÉ™ti neÃ§É™ element gÃ¶tÃ¼rÃ¼lÉ™ bilÉ™r
            out.extend(self.next_items[:n])  # O qÉ™dÉ™rini É™lavÉ™ et
            count -= n  # SayÄ± azald
            self.next_items = self.next_items[n:]  # ÆlavÉ™ olunanlarÄ± siyahÄ±dan Ã§Ä±xart
            if len(self.next_items) == 0:  # ÆgÉ™r nÃ¶vbÉ™ti elementlÉ™r qalmayÄ±bsa
                self.next_items = shuffled(list(self.all_items))  # BÃ¼tÃ¼n itemlÉ™ri yenidÉ™n qarÄ±ÅŸdÄ±r
            return out  # NÉ™ticÉ™ni qaytar

    def __next__(self):  # Python iterator interfeysi Ã¼Ã§Ã¼n
        return self.sample(1)[0]  # HÉ™r dÉ™fÉ™ bir element qaytarÄ±r
        </code>
    </pre>

<h2>speaker.py: Speaker Sinfi</h2>
<p>Utterance sinfi bu kodda istifadÉ™ olunur, amma onun Ã¶zÃ¼ burada yoxdur. Bu sinif frame fayllarÄ±nÄ± vÉ™ audio fayllarÄ±nÄ± birlÉ™ÅŸdirir vÉ™ mÃ¼É™yyÉ™n sayda kÉ™sik (partial) qaytarÄ±r.</p>
<pre>
        <code>
from pathlib import Path  # Fayl yollarÄ± ilÉ™ iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n
from encoder.data_objects.random_cycler import RandomCycler  # Ã–zÃ¼mÃ¼z yazdÄ±ÄŸÄ±mÄ±z RandomCycler sinfini daxil edirik

class Speaker:
    def __init__(self, root: Path):  # Speaker sinfinin yaradÄ±lmasÄ±
        self.root = root  # Spikerin qovluq yolu
        self.name = root.name  # Spikerin adÄ±, qovluÄŸun adÄ± kimi gÃ¶tÃ¼rÃ¼lÃ¼r
        self.utterances = None  # SpikerÉ™ aid cÃ¼mlÉ™lÉ™r siyahÄ±sÄ± (Utterance obyektlÉ™ri)
        self.utterances_cycler = None  # CÃ¼mlÉ™lÉ™ri tÉ™sadÃ¼fi dÃ¶vr etmÉ™k Ã¼Ã§Ã¼n RandomCycler obyekti

    def _load_utterances(self):  # CÃ¼mlÉ™lÉ™ri fayldan oxumaq Ã¼Ã§Ã¼n daxili metod
        with self.root.joinpath("_sources.txt").open("r") as sources_file:  # _sources.txt faylÄ±nÄ± aÃ§Ä±rÄ±q
            sources = [l.split(",") for l in sources_file]  # HÉ™r sÉ™tri vergÃ¼llÉ™ ayÄ±rÄ±rÄ±q

        sources = {frames_fname: wave_fpath for frames_fname, wave_fpath in
                   sources}  # SiyahÄ±nÄ± sÃ¶zlÃ¼k formatÄ±na salÄ±rÄ±q
        self.utterances = [Utterance(self.root.joinpath(f), w) for f, w in
                           sources.items()]  # HÉ™r bir sÉ™tri Utterance obyektinÉ™ Ã§eviririk
        self.utterances_cycler = RandomCycler(
            self.utterances)  # Bu Utterance-lÉ™ri tÉ™sadÃ¼fi dÃ¶vr etmÉ™k Ã¼Ã§Ã¼n RandomCycler obyektinÉ™ veririk

    def random_partial(self, count, n_frames):  # TÉ™sadÃ¼fi hissÉ™lÉ™r É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n metod
        if self.utterances is None:  # ÆgÉ™r hÉ™lÉ™ cÃ¼mlÉ™lÉ™r yÃ¼klÉ™nmÉ™yibsÉ™
            self._load_utterances()  # OnlarÄ± fayldan yÃ¼klÉ™

        utterances = self.utterances_cycler.sample(count)  # RandomCycler vasitÉ™silÉ™ count sayda cÃ¼mlÉ™ gÃ¶tÃ¼r

        a = [(u,) + u.random_patial(n_frames) for u in
             utterances]  # HÉ™r bir cÃ¼mlÉ™dÉ™n n_frames Ã¶lÃ§Ã¼sÃ¼ndÉ™ hissÉ™ gÃ¶tÃ¼r vÉ™ tuple ÅŸÉ™klindÉ™ saxla

        return a  # AlÄ±nan nÉ™ticÉ™ni qaytar
        </code>
    </pre>

<h1>10-Data Objects: Paket vÉ™ icerisindeki scriptler</h1>

<h2>random_cycler.py: RandomCycler</h2>
<p>VerilmiÅŸ elementlÉ™ri tÉ™sadÃ¼fi ÅŸÉ™kildÉ™, qarÄ±ÅŸÄ±q-qarÄ±ÅŸÄ±q, amma hÉ™r birini istifadÉ™ edÉ™rÉ™k tÉ™krar-tÉ™krar qaytarmaq Ã¼Ã§Ã¼ndÃ¼r.</p>
<p>YÉ™ni <code>random.choice()</code> kimi tÉ™sadÃ¼fi tÉ™krar ola bilÉ™r â€” amma bu sinif tÉ™krarsÄ±z tÉ™sadÃ¼fi dÃ¶vr edir vÉ™ hamÄ±sÄ±nÄ± istifadÉ™ etdikdÉ™n sonra yenidÉ™n qarÄ±ÅŸdÄ±rÄ±b baÅŸlayÄ±r.</p>

<pre>
        <code>
import random  # TÉ™sadÃ¼fi seÃ§imlÉ™r etmÉ™k Ã¼Ã§Ã¼n

class RandomCycler:
    def __init__(self, source):
        if len(source) == 0:
            raise Exception("Can't create RandomCycler from an empty collection")  # ÆgÉ™r boÅŸ siyahÄ± verilsÉ™, xÉ™ta verir
        self.all_items = list(source)  # VerilÉ™n siyahÄ±nÄ± yadda saxlayÄ±r
        self.next_items = []  # NÃ¶vbÉ™ti seÃ§ilÉ™cÉ™k itemlÉ™ri saxlayacaq

    def sample(self, count: int):  # Ä°stifadÉ™Ã§idÉ™n neÃ§É™ dÉ™nÉ™ element istÉ™nildiyini alÄ±r
        shuffled = lambda l: random.sample(l, len(l))  # VerilÉ™n siyahÄ±nÄ±n qarÄ±ÅŸdÄ±rÄ±lmÄ±ÅŸ versiyasÄ±nÄ± qaytarÄ±r

        out = []  # Cavab siyahÄ±sÄ±
        while count > 0:  # HÉ™lÉ™ alÄ±nmalÄ± elementlÉ™r varsa
            if count >= len(self.all_items):  # ÆgÉ™r istÉ™nilÉ™n say, bÃ¼tÃ¼n elementlÉ™rin sayÄ±ndan Ã§oxdursa
                out.extend(shuffled(list(self.all_items)))  # BÃ¼tÃ¼n elementlÉ™ri qarÄ±ÅŸdÄ±rÄ±b É™lavÉ™ et
                count -= len(self.all_items)  # SayÄ± azald
                continue  # DÃ¶vrÉ™ davam et
            n = min(count, len(self.next_items))  # NÃ¶vbÉ™ti neÃ§É™ element gÃ¶tÃ¼rÃ¼lÉ™ bilÉ™r
            out.extend(self.next_items[:n])  # O qÉ™dÉ™rini É™lavÉ™ et
            count -= n  # SayÄ± azald
            self.next_items = self.next_items[n:]  # ÆlavÉ™ olunanlarÄ± siyahÄ±dan Ã§Ä±xart
            if len(self.next_items) == 0:  # ÆgÉ™r nÃ¶vbÉ™ti elementlÉ™r qalmayÄ±bsa
                self.next_items = shuffled(list(self.all_items))  # BÃ¼tÃ¼n itemlÉ™ri yenidÉ™n qarÄ±ÅŸdÄ±r
            return out  # NÉ™ticÉ™ni qaytar

    def __next__(self):  # Python iterator interfeysi Ã¼Ã§Ã¼n
        return self.sample(1)[0]  # HÉ™r dÉ™fÉ™ bir element qaytarÄ±r
        </code>
    </pre>

<h2>speaker.py: Speaker sinfi</h2>
<p>Utterance sinfi bu kodda istifadÉ™ olunur, amma onun Ã¶zÃ¼ burada yoxdur. Orada <code>Utterance(self.root.joinpath(f), w)</code> vÉ™ <code>u.random_patial(n_frames)</code> Ã§aÄŸÄ±rÄ±ÅŸlarÄ± var â€” yÉ™ni bu sinif frame fayllarÄ±nÄ± vÉ™ audio fayllarÄ±nÄ± birlÉ™ÅŸdirir vÉ™ mÃ¼É™yyÉ™n sayda kÉ™sik (partial) qaytarÄ±r.</p>

<pre>
        <code>
from pathlib import Path  # Fayl yollarÄ± ilÉ™ iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n

from encoder.data_objects.random_cycler import RandomCycler  # Ã–zÃ¼mÃ¼z yazdÄ±ÄŸÄ±mÄ±z RandomCycler sinfini daxil edirik


class Speaker:
    def __init__(self, root: Path):  # Speaker sinfinin yaradÄ±lmasÄ±
        self.root = root  # Spikerin qovluq yolu
        self.name = root.name  # Spikerin adÄ±, qovluÄŸun adÄ± kimi gÃ¶tÃ¼rÃ¼lÃ¼r
        self.utterances = None  # SpikerÉ™ aid cÃ¼mlÉ™lÉ™r siyahÄ±sÄ± (Utterance obyektlÉ™ri)
        self.utterances_cycler = None  # CÃ¼mlÉ™lÉ™ri tÉ™sadÃ¼fi dÃ¶vr etmÉ™k Ã¼Ã§Ã¼n RandomCycler obyekti

    def _load_utterances(self):  # CÃ¼mlÉ™lÉ™ri fayldan oxumaq Ã¼Ã§Ã¼n daxili metod
        with self.root.joinpath("_sources.txt").open("r") as sources_file:  # _sources.txt faylÄ±nÄ± aÃ§Ä±rÄ±q
            sources = [l.split(",") for l in sources_file]  # HÉ™r sÉ™tri vergÃ¼llÉ™ ayÄ±rÄ±rÄ±q

        sources = {frames_fname: wave_fpath for frames_fname, wave_fpath in
                   sources}  # SiyahÄ±nÄ± sÃ¶zlÃ¼k formatÄ±na salÄ±rÄ±q
        self.utterances = [Utterance(self.root.joinpath(f), w) for f, w in
                           sources.items()]  # HÉ™r bir sÉ™tri Utterance obyektinÉ™ Ã§eviririk
        self.utterances_cycler = RandomCycler(
            self.utterances)  # Bu Utterance-lÉ™ri tÉ™sadÃ¼fi dÃ¶vr etmÉ™k Ã¼Ã§Ã¼n RandomCycler obyektinÉ™ veririk

    def random_partial(self, count, n_frames):  # TÉ™sadÃ¼fi hissÉ™lÉ™r É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n metod

        if self.utterances is None:  # ÆgÉ™r hÉ™lÉ™ cÃ¼mlÉ™lÉ™r yÃ¼klÉ™nmÉ™yibsÉ™
            self._load_utterances()  # OnlarÄ± fayldan yÃ¼klÉ™

        utterances = self.utterances_cycler.sample(count)  # RandomCycler vasitÉ™silÉ™ count sayda cÃ¼mlÉ™ gÃ¶tÃ¼r

        a = [(u,) + u.random_patial(n_frames) for u in
             utterances]  # HÉ™r bir cÃ¼mlÉ™dÉ™n n_frames Ã¶lÃ§Ã¼sÃ¼ndÉ™ hissÉ™ gÃ¶tÃ¼r vÉ™ tuple ÅŸÉ™klindÉ™ saxla

        return a  # AlÄ±nan nÉ™ticÉ™ni qaytar
        </code>
    </pre>

<h2>speaker_batch.py: SpeakerBatch</h2>
<p>SpeakerBatch sinfi, bir batch-lik spikerlÉ™r vÉ™ onlarÄ±n sÉ™s parÃ§asÄ± mÉ™lumatlarÄ±nÄ± idarÉ™ etmÉ™k Ã¼Ã§Ã¼n istifadÉ™ olunur. Bu sinif, Ã§oxlu sayda spikerin sÉ™s mÉ™lumatlarÄ±nÄ± PyTorch modelinÉ™ Ã¶tÃ¼rÃ¼lÉ™ bilÉ™n formatda tÉ™ÅŸkil edir.</p>

<pre>
        <code>
import numpy as np  # Numpy kitabxanasÄ±, massivlÉ™r vÉ™ matrislÉ™rlÉ™ iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n
from librosa.util import frame  # Librosa kitabxanasÄ±ndan frame funksiyasÄ±, sÉ™sin Ã§É™rÃ§ivÉ™lÉ™nmÉ™si Ã¼Ã§Ã¼n
from encoder.data_objects.speaker import Speaker  # Speaker sinfi, sÉ™s parÃ§alarÄ±nÄ± idarÉ™ etmÉ™k Ã¼Ã§Ã¼n

class SpeakerBatch:
    def __init__(self, speakers: List[Speaker], utterances_per_speaker: int, n_frames: int):
        # ParametrlÉ™r:
        # speakers: Speaker obyektlÉ™rindÉ™n ibarÉ™t siyahÄ± (hÉ™r biri spikerin sÉ™s mÉ™lumatlarÄ±nÄ± saxlayÄ±r)
        # utterances_per_speaker: HÉ™r spikerin neÃ§É™ sÉ™s parÃ§asÄ± (utterance) gÃ¶tÃ¼rÉ™cÉ™yini gÃ¶stÉ™rir
        # n_frames: HÉ™r bir utterance Ã¼Ã§Ã¼n neÃ§É™ frame (kadr) gÃ¶tÃ¼rÃ¼lÉ™cÉ™yini gÃ¶stÉ™rir

        # HÉ™r bir spiker Ã¼Ã§Ã¼n random_partial Ã§aÄŸÄ±raraq, sÉ™s parÃ§alarÄ±nÄ± alÄ±rÄ±q
        self.speakers = speakers
        self.partials = {s: s.random_partial(utterances_per_speaker, n_frames) for s in speakers}

        # Numpy massivinÉ™ Ã§evrilir - burada hÉ™r spikerin frame-lÉ™ri toplanÄ±r
        self.data = np.array([frames for s in speakers for _, frames, _ in self.partials[s]])

    # (ÆgÉ™r ehtiyac varsa, daha Ã§ox funksionallÄ±q É™lavÉ™ edilÉ™ bilÉ™r)
        </code>
    </pre>

<h1>speaker_verification_dataset.py</h1>
<p>Bu kod PyTorch ilÉ™ iÅŸlÉ™yÉ™n sÉ™slÉ™ ÅŸÉ™xs identifikasiyasÄ± modelinÉ™ tÉ™lim (training) Ã¼Ã§Ã¼n lazÄ±m olan mÉ™lumatlarÄ± yÃ¼klÉ™mÉ™k, idarÉ™ etmÉ™k vÉ™ batch-lÉ™rÉ™ bÃ¶lmÉ™k mÉ™qsÉ™dilÉ™ yazÄ±lÄ±b.</p>

<pre><code>
from pathlib import Path  # Fayl yollarÄ± ilÉ™ iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n

from sklearn.utils import shuffle  # MÉ™lumatlarÄ± qarÄ±ÅŸdÄ±rmaq Ã¼Ã§Ã¼n (istifadÉ™ olunmayÄ±b burada amma daxil edilib)
from torch.utils.data import Dataset  # PyTorch-un Dataset bazasÄ± ilÉ™ iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n

from encoder.data_objects.random_cycler import RandomCycler  # RandomCycler sinfi, tÉ™sadÃ¼fi dÃ¶vr Ã¼Ã§Ã¼n
from encoder.data_objects.speaker import Speaker  # Speaker sinfi
from encoder.data_objects.speaker_batch import SpeakerBatch  # SpeakerBatch sinfi
from encoder.params_data import partials_n_frames  # HÉ™r bir utterance Ã¼Ã§Ã¼n frame sayÄ± parametri


class SpeakerVerificationDataset(Dataset):  # PyTorch-un Dataset sinifindÉ™n miras alan class
    def __init__(self,datasets_root: Path):  # Dataset-in baÅŸlanÄŸÄ±cÄ±
        self.root = datasets_root  # Dataset-in kÃ¶k qovluÄŸu
        speaker_dirs = [f for  f in self.root.glob("*") if f.is_dir()]  # BÃ¼tÃ¼n alt qovluqlarÄ± (spiker qovluqlarÄ±) tapÄ±rÄ±q
        if len(speaker_dirs) == 0:  # ÆgÉ™r heÃ§ bir qovluq yoxdursa, xÉ™ta atÄ±rÄ±q
            raise Exception("No speakers found. Make sure you are pointing to the directory "
                            "containing all preprocessed speaker directories.")
        self.speakers = [Speaker(speaker_dir) for  speaker_dir in speaker_dirs]  # HÉ™r qovluq Ã¼Ã§Ã¼n Speaker obyekti yaradÄ±lÄ±r
        self.speaker_cycler = RandomCycler(self.speakers)  # BÃ¼tÃ¼n speaker-lÉ™ri RandomCycler vasitÉ™silÉ™ qarÄ±ÅŸdÄ±rÄ±b dÃ¶vr etmÉ™k Ã¼Ã§Ã¼n

    def __len__(self):  # Dataset-in uzunluÄŸu (sÃ¼ni olaraq Ã§ox bÃ¶yÃ¼k qoyulub ki, limitsiz kimi gÃ¶rÃ¼nsÃ¼n)
        return int(1e10)

    def get_logs(self):  # Dataset qovluÄŸunda olan bÃ¼tÃ¼n `.txt` fayllarÄ± birlÉ™ÅŸdirÉ™rÉ™k log verir
        log_string = ""
        for log_fpath in self.root.glob("*.txt"):  # BÃ¼tÃ¼n .txt fayllar Ã¼zÉ™rindÉ™ gedirik
            with log_fpath.open("r") as log_file:  # HÉ™r bir faylÄ± oxuyuruq
                log_string += "".join(log_file.readlines())  # Oxunan sÉ™trlÉ™ri log_string-É™ É™lavÉ™ edirik
        return log_string  # ToplanmÄ±ÅŸ log qaytarÄ±lÄ±r


class SpeakerVerificationDatasetLoader(DataLoader):  # PyTorch-un DataLoader sinfindÉ™n miras alan custom loader
    def __init__(self, dataset, speakers_per_batch, utterances_per_speaker, sampler=None,
                 batch_sampler=None, num_workers=0, pin_memory=False, timeout=0,
                 worker_init_fn=None):
        self.utterances_per_speaker = utterances_per_speaker  # HÉ™r bir speaker Ã¼Ã§Ã¼n neÃ§É™ cÃ¼mlÉ™ (utterance) yÃ¼klÉ™nÉ™cÉ™k

        super().__init__(  # DataLoader-in Ã¶z parametrli super konstrukturu
            dataset=dataset,
            batch_size = speakers_per_batch,  # HÉ™r batch-dÉ™ neÃ§É™ speaker olmalÄ±dÄ±r
            shuffle = False,  # Shuffle burada istifadÉ™ olunmur (Ã§Ã¼nki RandomCycler istifadÉ™ olunur)
            sampler=sampler,  # Ä°stÉ™yÉ™ uyÄŸun sampler
            batch_sampler = batch_sampler,
            num_workers = num_workers,  # Paralel iÅŸÃ§i sayÄ±
            collate_fn = self.collate,  # Batch birlÉ™ÅŸdirmÉ™ funksiyasÄ± Ã¶zÃ¼mÃ¼zÃ¼nkÃ¼dÃ¼r
            pin_memory = pin_memory,  # GPU sÃ¼rÉ™ti Ã¼Ã§Ã¼n yaddaÅŸ sabitlÉ™mÉ™
            drop_last = False,  # Batch yarÄ±mÃ§Ä±q qalarsa belÉ™ istifadÉ™ olunur
            timeout = timeout,  # Ä°ÅŸÃ§i yÃ¼klÉ™mÉ™ vaxtÄ±
            worker_init_fn = worker_init_fn  # Ä°ÅŸÃ§i baÅŸladÄ±qda Ã§aÄŸÄ±rÄ±lan funksiya
        )

    def collate(self, speakers):  # Batch birlÉ™ÅŸdirmÉ™ metodu
        return SpeakerBatch(speakers,self.utterances_per_speaker, partials_n_frames)  # SpeakerBatch obyektinÉ™ speaker-lÉ™ri vÉ™ parametr-lÉ™ri veririk
    </code></pre>

<h2>utterance.py</h2>

<p>
    Bu kod parÃ§asÄ±, sÉ™s mÉ™lumatlarÄ±nÄ± idarÉ™ etmÉ™k Ã¼Ã§Ã¼n yazÄ±lmÄ±ÅŸdÄ±r vÉ™ sÉ™sin mÃ¼É™yyÉ™n bir hissÉ™sini tÉ™sadÃ¼fi seÃ§mÉ™k Ã¼Ã§Ã¼n istifadÉ™ olunur. HÉ™r <strong>Utterance</strong> obyektindÉ™ki <code>random_partial</code> metodu, modelin treninqindÉ™ vÉ™ ya sÉ™s analizi tÉ™tbiqlÉ™rindÉ™ istifadÉ™ edilÉ™ bilÉ™cÉ™k tÉ™sadÃ¼fi frame seqmentlÉ™ri É™ldÉ™ etmÉ™yÉ™ imkan verir.
</p>

<pre><code>import numpy as np  # Numpy kitabxanasÄ±, massivlÉ™r vÉ™ matrislÉ™rlÉ™ iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n


class Utterance:
    def __init__(self, frames_fpath, wave_fpath):
        # frames_fpath: Frame mÉ™lumatlarÄ±nÄ± saxlayan faylÄ±n yolu
        # wave_fpath: SÉ™s dalÄŸasÄ± mÉ™lumatlarÄ±nÄ± saxlayan faylÄ±n yolu
        self.frames_fpath = frames_fpath
        self.wave_fpath = wave_fpath

    def get_frames(self):
        # Frame mÉ™lumatlarÄ±nÄ± yÃ¼kleyirik
        # Numpy istifadÉ™ edÉ™rÉ™k frames faylÄ±nÄ± oxuyuruq
        return np.load(self.frames_fpath)

    def random_partial(self, n_frames):
        # `get_frames()` metodu ilÉ™ frame mÉ™lumatlarÄ±nÄ± alÄ±rÄ±q
        frames = self.get_frames()

        # ÆgÉ™r frame sayÄ± `n_frames`-É™ bÉ™rabÉ™rdirsÉ™, baÅŸlama nÃ¶qtÉ™sini 0 olaraq tÉ™yin edirik
        if frames.shape[0] == n_frames:
            start = 0
        else:
            # Æks halda, `n_frames` sayda frame É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n tÉ™sadÃ¼fi baÅŸlanÄŸÄ±c nÃ¶qtÉ™si seÃ§irik
            start = np.random.randint(0, frames.shape[0] - n_frames)

        # BitmÉ™ nÃ¶qtÉ™sini tÉ™yin edirik
        end = start + n_frames

        # SeÃ§ilmiÅŸ hissÉ™ni qaytarÄ±rÄ±q: `frames[start:end]` -> seÃ§ilÉ™n kadrlar
        # `(start, end)` -> baÅŸlanÄŸÄ±c vÉ™ bitmÉ™ nÃ¶qtÉ™lÉ™ri
        return frames[start:end], (start, end)
</code></pre>

<h2>MetodlarÄ±n izahÄ±</h2>

<p><strong>__init__ metodu:</strong> <br>
    <code>frames_fpath</code>: Frame mÉ™lumatlarÄ±nÄ± saxlayan faylÄ±n yoludur. Bu faylda, sÉ™s mÉ™lumatÄ±nÄ±n mÃ¼xtÉ™lif kadrlarÄ± (frames) mÃ¶vcuddur. <br>
    <code>wave_fpath</code>: SÉ™s dalÄŸasÄ± mÉ™lumatlarÄ±nÄ± saxlayan faylÄ±n yoludur (bu kodda istifadÉ™ olunmayÄ±b).</p>

<p><strong>get_frames metodu:</strong><br>
    Bu metod <code>frames_fpath</code> yolundakÄ± fayldan mÉ™lumatlarÄ± oxuyur vÉ™ <code>np.load()</code> vasitÉ™silÉ™ numpy massivinÉ™ Ã§evirir.</p>

<p><strong>random_partial metodu:</strong><br>
    Bu metod <code>n_frames</code> sayda tÉ™sadÃ¼fi kadr seÃ§É™rÉ™k geri qaytarÄ±r. ÆgÉ™r frame-lÉ™rin sayÄ± kifayÉ™t qÉ™dÉ™rdirsÉ™, baÅŸlanÄŸÄ±c nÃ¶qtÉ™ tÉ™sadÃ¼fi seÃ§ilir, É™ks halda 0-dan baÅŸlanÄ±r. NÉ™ticÉ™ kimi hÉ™m frame massivinin bir hissÉ™si, hÉ™m dÉ™ onun baÅŸlanÄŸÄ±c vÉ™ bitmÉ™ indekslÉ™ri qaytarÄ±lÄ±r.</p>

<div class="section">
    <h2>utterance.py</h2>
    <p>Bu kod parÃ§asÄ±, sÉ™s mÉ™lumatlarÄ±nÄ± idarÉ™ etmÉ™k Ã¼Ã§Ã¼n yazÄ±lmÄ±ÅŸdÄ±r vÉ™ sÉ™sin mÃ¼É™yyÉ™n bir hissÉ™sini tÉ™sadÃ¼fi seÃ§mÉ™k Ã¼Ã§Ã¼n istifadÉ™ olunur.</p>
    <p><strong>random_partial</strong> metodu, modelin treninqindÉ™ vÉ™ ya sÉ™s analizi tÉ™tbiqlÉ™rindÉ™ istifadÉ™ edilÉ™ bilÉ™cÉ™k tÉ™sadÃ¼fi frame seqmentlÉ™ri É™ldÉ™ etmÉ™yÉ™ imkan verir.</p>
    <pre><code>import numpy as np  # Numpy kitabxanasÄ±, massivlÉ™r vÉ™ matrislÉ™rlÉ™ iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n

class Utterance:
    def __init__(self, frames_fpath, wave_fpath):
        self.frames_fpath = frames_fpath
        self.wave_fpath = wave_fpath

    def get_frames(self):
        return np.load(self.frames_fpath)

    def random_partial(self, n_frames):
        frames = self.get_frames()
        if frames.shape[0] == n_frames:
            start = 0
        else:
            start = np.random.randint(0, frames.shape[0] - n_frames)
        end = start + n_frames
        return frames[start:end], (start, end)
</code></pre>
</div>

<div class="section">
    <h2>2. samples</h2>
    <p>Bu qovluq, nÃ¼munÉ™ audio fayllarÄ±nÄ± ehtiva edir. TÉ™lim vÉ™ test mÉ™rhÉ™lÉ™lÉ™rindÉ™ istifadÉ™ olunan real sÉ™s nÃ¼munÉ™lÉ™ridir.</p>
    <p>Audio fayllarÄ± bu qovluÄŸa yÄ±ÄŸÄ±lmalÄ± vÉ™ encoder vÉ™ synthesizer tÉ™rÉ™findÉ™n istifadÉ™ olunmalÄ±dÄ±r.</p>
</div>

<div class="section">
    <h2>3. saved_models</h2>
    <p>Bu qovluq, tÉ™lim edilmiÅŸ model fayllarÄ±nÄ± saxlayÄ±r. ÆldÉ™ edilÉ™n model vÉ™ Ã§É™kilÉ™n aÄŸÄ±rlÄ±qlar burada saxlanÄ±lÄ±r.</p>
</div>

<div class="section">
    <h2>4. synthesizer</h2>
    <p>Bu qovluq, sÉ™s sintezini idarÉ™ edÉ™n komponentlÉ™ri ehtiva edir. Text-to-speech (TTS) modeli vasitÉ™silÉ™ mÉ™tn sÉ™sÉ™ Ã§evrilir.</p>
    <p><code>synthesizer_train.py</code> vÉ™ <code>synthesizer_preprocess_audio.py</code> bu komponentlÉ™ É™laqÉ™lidir.</p>
</div>

<div class="section">
    <h2>5. toolbox</h2>
    <p>FaydalÄ± alÉ™tlÉ™r vÉ™ mÃ¼xtÉ™lif funksiyalarÄ± ehtiva edÉ™n qovluqdur. <code>demo_toolbox.py</code> kimi fayllar sÉ™sin iÅŸlÉ™nmÉ™sini hÉ™yata keÃ§irir.</p>
</div>

<div class="section">
    <h2>6. utils</h2>
    <p>LayihÉ™dÉ™ istifadÉ™ olunan yardÄ±mÃ§Ä± funksiyalarÄ± ehtiva edir. MÉ™lumat emalÄ±, fayl yÃ¼klÉ™mÉ™ vÉ™ digÉ™r texniki É™mÉ™liyyatlar burada yer alÄ±r.</p>
</div>

<div class="section">
    <h3>1-argutils</h3>
    <p><code>print_args.py</code> skripti konfiqurasiya edilmiÅŸ <code>argparse.Namespace</code> obyektindÉ™ki arqumentlÉ™ri aydÄ±n ÅŸÉ™kildÉ™ Ã§ap etmÉ™k Ã¼Ã§Ã¼ndÃ¼r.</p>
    <pre><code>from pathlib import Path
import numpy as np
import argparse

_type_priorities = [Path, str, int, float, bool]

def _priority(o):
    p = next((i for i, t in enumerate(_type_priorities) if type(o) is t), None)
    if p is not None:
        return p
    p = next((i for i, t in enumerate(_type_priorities) if isinstance(o, t)), None)
    if p is not None:
        return p
    return len(_type_priorities)

def print_args(args: argparse.Namespace, parser=None):
    args = vars(args)
    if parser is None:
        priorities = list(map(_priority, args.values()))
    else:
        all_params = [a.dest for g in parser._action_groups for a in g._group_actions]
        priority = lambda p: all_params.index(p) if p in all_params else len(all_params)
        priorities = list(map(priority, args.keys()))

    pad = max(map(len, args.keys())) + 3
    indices = np.lexsort((list(args.keys()), priorities))
    items = list(args.items())

    print("Argument:")
    for i in indices:
        param, value = items[i]
        print("    {0}:{1}{2}".format(param, ' ' * (pad - len(param)), value))
    print("")
</code></pre>
</div>

<h2>2-default_models:</h2>
<p>
    Bu skript sÉ™nin layihÉ™ndÉ™ki encoder, synthesizer vÉ™ vocoder modellÉ™rini avtomatik olaraq yÃ¼klÉ™mÉ™k Ã¼Ã§Ã¼ndÃ¼r.
    ÆgÉ™r bu modellÉ™r dÃ¼zgÃ¼n Ã¶lÃ§Ã¼dÉ™ deyilsÉ™ vÉ™ ya yoxdursa, onlarÄ± Google Drive Ã¼zÉ™rindÉ™n endirir.
</p>

<pre><code>import urllib.request
from pathlib import Path
from threading import Thread
from urllib.error import HTTPError

from tqdm import tqdm

# Model fayllarÄ±nÄ±n URL-lÉ™ri vÉ™ onlarÄ±n gÃ¶zlÉ™nilÉ™n Ã¶lÃ§Ã¼lÉ™ri (byte cinsindÉ™)
default_models = {
    "encoder": ("https://drive.google.com/uc?export=download&id=1q8mEGwCkFy23KZsinbuvdKAQLqNKbYf1", 17090379),
    "synthesizer": ("https://drive.google.com/u/0/uc?id=1EqFMIbvxffxtjiVrtykroF6_mUh-5Z3s&amp;export=download&amp;confirm=t", 370554559),
    "vocoder": ("https://drive.google.com/uc?export=download&id=1cf2NO6FtI0jDuy8AV3Xgn6leO6dHjIgu", 53845290),
}


# Tqdm-É™ É™saslanan fayl yÃ¼klÉ™mÉ™ prosesindÉ™ irÉ™lilÉ™yiÅŸi gÃ¶stÉ™rÉ™n xÃ¼susi sinif
class DownloadprogressBar(tqdm):
    def update_to(self, b=1, bsize=1, tsize=None):
        if tsize is not None:
            self.total = tsize  # FaylÄ±n Ã¼mumi Ã¶lÃ§Ã¼sÃ¼nÃ¼ tÉ™yin et
        self.update(b * bsize - self.n)  # HazÄ±rda yÃ¼klÉ™nmiÅŸ baytlarÄ± yenilÉ™


# VerilmiÅŸ URL-dÉ™n faylÄ± tÉ™yin olunmuÅŸ yerÉ™ yÃ¼klÉ™yÉ™n funksiya
def download(url: str, target: Path, bar_pos=0):
    target.parent.mkdir(exist_ok=True, parents=True)  # Fayl Ã¼Ã§Ã¼n lazÄ±mi qovluÄŸu yaradÄ±r (É™gÉ™r yoxdursa)

    desc = f"Downloading {target.name}"  # YÃ¼klÉ™mÉ™ prosesindÉ™ gÃ¶rÃ¼nÉ™cÉ™k ad
    with DownloadprogressBar(unit="B", unit_scale=True, miniters=1, desc=desc, position=bar_pos, leave=False) as t:
        try:
            urllib.request.urlretrieve(url, filename=target, reporthook=t.update_to)  # FaylÄ± yÃ¼klÉ™
        except HTTPError:
            return  # HTTP sÉ™hvi olsa, heÃ§ nÉ™ etmÉ™


# Default model fayllarÄ±nÄ±n qovluqda olub-olmadÄ±ÄŸÄ±nÄ± yoxlayÄ±r, yoxdursa yÃ¼klÉ™yir
def ensure_default_models(models_dir: Path):
    jobs = []
    for model_name, (url, size) in default_models.items():
        target_path = models_dir / "default" / f"{model_name}.pt"  # Model Ã¼Ã§Ã¼n hÉ™dÉ™f yol

        # ÆgÉ™r fayl gÃ¶zlÉ™nilÉ™n Ã¶lÃ§Ã¼dÉ™ deyilsÉ™, yÃ¼klÉ™
        if target_path.stat().st_size != size:
            print(f"File {target_path} is not of expected size, redownloading...")
        else:
            continue  # ÆgÉ™r Ã¶lÃ§Ã¼ dÃ¼zgÃ¼ndÃ¼rsÉ™, keÃ§ nÃ¶vbÉ™tiyÉ™

        # YÃ¼klÉ™mÉ™ prosesini ayrÄ± thread-dÉ™ iÅŸÉ™ salÄ±r
        thread = Thread(target=download, args=(url, target_path, len(jobs)))
        thread.start()
        jobs.append((thread, target_path, size))  # Thread-lÉ™ri izlÉ™mÉ™k Ã¼Ã§Ã¼n siyahÄ±ya É™lavÉ™ et

    # BÃ¼tÃ¼n yÃ¼klÉ™mÉ™lÉ™rin bitmÉ™sini gÃ¶zlÉ™yir
    for thread, target_path, size in jobs:
        thread.join()

        # FaylÄ±n dÃ¼zgÃ¼n yÃ¼klÉ™ndiyini yoxla
        assert target_path.stat().st_size == size, \
            f"Download for {target_path.name} failed. You may download models manually instead.\n" \
            f"https://drive.google.com/drive/folders/1fU6umc5uQAVR2udZdHX-lDgXYzTyqG_j"
</code></pre>

<h2>3 - logmmse:</h2>
<p>
    Bu Python kodu sÉ™s siqnallarÄ±nÄ± tÉ™mizlÉ™mÉ™k (denoising) Ã¼Ã§Ã¼ndÃ¼r, yÉ™ni bir audio fayldakÄ± fon sÉ™sini (mÉ™sÉ™lÉ™n, mikrofon "hÄ±ÅŸÄ±ltÄ±sÄ±", ortam sÉ™s-kÃ¼yÃ¼ vÉ™ s.) Ã§Ä±xartmaq vÉ™ ya azaltmaq mÉ™qsÉ™dilÉ™ yazÄ±lÄ±b.
</p>
<pre><code>from collections import namedtuple
import numpy as np
import math
from mpmath.libmp import to_float  # mp modulundan float-a Ã§evirmÉ™k Ã¼Ã§Ã¼n
from scipy.special import expn  # Eksponensial integral funksiyasÄ±

# NoiseProfile strukturu
NoiseProfile = namedtuple("NoiseProfile", "sampling_rate window_size len1 len2 win n_fft noise_mu2")

def profile_noise(noise, sampling_rate, window_size=0):
    noise, dtype = to_float(noise)
    noise += np.finfo(np.float64).eps

    if window_size == 0:
        window_size = int(math.floor(0.02 * sampling_rate))  # dÃ¼zÉ™liÅŸ edildi

    if window_size % 2 == 1:
        window_size += 1

    perc = 50
    len1 = int(math.floor(window_size * perc / 100))
    len2 = int(window_size - len1)

    win = np.hanning(window_size)
    win = win * len2 / np.sum(win)
    n_fft = 2 * window_size

    noise_mean = np.zeros(n_fft)
    n_frames = len(noise) // window_size
    for j in range(0, window_size * n_frames, window_size):
        noise_mean += np.abs(np.fft.fft(win * noise[j:j + window_size], n_fft , axis=0))
    noise_mu2 = (noise_mean / n_frames) ** 2

    return NoiseProfile(sampling_rate, window_size, len1, len2, win, n_fft, noise_mu2)

def denoise(wav, noise_profile: NoiseProfile, eta=0.15):
    wav, dtype = to_float(wav)
    wav += np.finfo(np.float64).eps
    p = noise_profile

    nframes = int(math.floor(len(wav) / p.len2) - math.floor(p.window_size / p.len2))
    x_final = np.zeros(nframes * p.len2)

    aa = 0.98
    mu = 0.98
    ksi_min = 10 ** (-25 / 10)

    x_old = np.zeros(p.len1)
    xk_prev = np.zeros(p.len2)
    noise_mu2 = p.noise_mu2

    for k in range(0, nframes * p.len2, p.len2):
        insign = p.win * wav[k:k + p.window_size]
        spec = np.fft.fft(insign, p.n_fft, axis=0)
        sig = np.abs(spec)
        sig2 = sig ** 2

        gammak = np.minimum(sig2 / noise_mu2, 40)

        if xk_prev.all() == 0:
            ksi = aa + (1 - aa) * np.maximum(gammak - 1, 0)
        else:
            ksi = aa * xk_prev / noise_mu2 + (1 - aa) * np.maximum(gammak - 1, 0)
            ksi = np.maximum(ksi_min, ksi)

        log_sigma_k = gammak * ksi / (1 + ksi) - np.log(1 + ksi)
        vad_decision = np.sum(log_sigma_k) / p.window_size
        if vad_decision &lt; eta:
            noise_mu2 = mu * noise_mu2 + (1 - mu) * sig2

        a = ksi / (1 + ksi)
        vk = a * gammak
        ei_vk = 0.5 * expn(1, np.maximum(vk, 1e-8))
        hw = a * np.exp(ei_vk)

        sig = sig * hw
        xk_prev = sig ** 2

        xi_w = np.fft.ifft(hw * spec, p.n_fft, axis=0)
        xi_w = np.real(xi_w)

        # Burada istÉ™sÉ™niz `x_final`-É™ É™lavÉ™ edÉ™ bilÉ™rsiniz

# DiqqÉ™t: `math.foolr` sÉ™hvdir, `math.floor` olmalÄ±dÄ±r
# `from numpy.conftest import dtype` artÄ±qdÄ±r vÉ™ silinmÉ™lidir
</code></pre>

<h2>4-profiler.py</h2>

<p><strong>TÉ™yinat:</strong> Bu <code>Profiler</code> sinfi, Python proqramÄ±nda mÃ¼É™yyÉ™n kod hissÉ™lÉ™rinin neÃ§É™ millisekund iÅŸlÉ™diyini Ã¶lÃ§mÉ™k vÉ™ orta icra mÃ¼ddÉ™tini hesablamaq Ã¼Ã§Ã¼n istifadÉ™ olunur.</p>

<pre><code>from collections import OrderedDict
import numpy as np
from torch.utils.benchmark import timer  # Zaman Ã¶lÃ§mÉ™k Ã¼Ã§Ã¼n istifadÉ™ olunur

class Profiler:
    def __init__(self, summarize_every=5, disabled=False):
        self.last_tick = timer()
        self.logs = OrderedDict()
        self.summarize_every = summarize_every
        self.disabled = disabled

    def tick(self, name):
        if self.disabled:
            return

        if not name in self.logs:
            self.logs[name] = []

        if len(self.logs[name]) >= self.summarize_every:
            self.summarize()
            self.purge_logs()

        self.logs[name].append(timer() - self.last_tick)
        self.reset_timer()

    def purge_logs(self):
        for name in self.logs:
            self.logs[name].clear()

    def reset_timer(self):
        self.last_tick = timer()

    def summarize(self):
        n = max(map(len, self.logs.values()))
        assert n == self.summarize_every

        print("\nAverage execution time over %d steps:" % n)

        name_msg = ["%s (%d/%d):" % (name, len(deltas), n) for name, deltas in self.logs.items()]
        pad = max(map(len, name_msg))

        for name_msg, deltas in zip(name_msg, self.logs.values()):
            print("  %s  mean: %4.0fms   std: %4.0fms" %
                  (name_msg.ljust(pad), np.mean(deltas) * 1000, np.std(deltas) * 1000))

        print("", flush=True)
</code></pre>

<h2>Ä°stifadÉ™ MÉ™qsÉ™di:</h2>
<ul>
    <li><strong>profiler.tick("yÃ¼kleme")</strong> kimi Ã§aÄŸÄ±rÄ±ldÄ±qda hÉ™min kod blokunun icra mÃ¼ddÉ™tini Ã¶lÃ§Ã¼r.</li>
    <li><strong>summarize_every=5</strong> olduqda hÉ™r 5 Ã¶lÃ§mÉ™dÉ™n bir orta vÉ™ standart sapma dÉ™yÉ™rlÉ™ri Ã§ap olunur.</li>
</ul>

<h2>NÉ™yÉ™ LazÄ±mdÄ±r?</h2>
<p>Bu tip profilinq alÉ™tlÉ™ri performans tÉ™hlili Ã¼Ã§Ã¼n vacibdir â€” xÃ¼susilÉ™ dÉ™ sÃ¼ni intellekt (AI), audio emalÄ± vÉ™ real-time proseslÉ™rdÉ™.</p>

<h2>7. Vocoder QovluÄŸu</h2>
<p><strong>Vocoder:</strong> Bu qovluq sÉ™sin sintezini vÉ™ analizini hÉ™yata keÃ§irir. Bu texnika ilÉ™ daha tÉ™miz vÉ™ keyfiyyÉ™tli audio yaratmaq mÃ¼mkÃ¼ndÃ¼r. Fayllar: <code>vocoder_train.py</code> vÉ™ <code>vocoder_preprocess.py</code>.</p>


</body>
</html>
