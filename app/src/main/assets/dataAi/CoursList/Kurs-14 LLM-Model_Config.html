<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>GPT-2 Konfiqurasiya Ä°zahedici</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f9f9f9;
            color: #222;
            margin: 20px;
            line-height: 1.6;
        }
        h2 {
            color: #e74c3c;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            background-color: #fff;
        }
        th, td {
            padding: 10px;
            border: 1px solid #ddd;
            vertical-align: top;
        }
        th {
            background-color: #f2f2f2;
            color: #444;
            text-align: left;
        }
        code {
            padding: 2px 5px;
            border-radius: 3px;
            font-family: Consolas, monospace;
        }
        pre {
            background-color: #2d2d2d;  /* QaranlÄ±q fon */
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            white-space: pre-wrap;
        }
        pre code {
            color: #00ffb7;  /* Kod yazÄ±sÄ± â€” parlaq yaÅŸÄ±l/mavi */
            font-family: Consolas, Monaco, monospace;
            font-size: 15px;
            display: block;
        }
        p {
            line-height: 1.6;
        }
        strong {
            color: #e74c3c;
        }
    </style>
</head>
<body>

<h2>ğŸ“„ GPT-2 <code>config.json</code> Ä°zahedici</h2>

<table>
    <tr>
        <th>AÃ§ar</th>
        <th>MÉ™na vÉ™ Ä°stifadÉ™si</th>
    </tr>
    <tr>
        <td><code>model_type</code></td>
        <td>Modelin tipi â€“ burada <strong>GPT-2</strong> gÃ¶stÉ™rilir.</td>
    </tr>
    <tr>
        <td><code>architectures</code></td>
        <td>Ä°stifadÉ™ edilÉ™n model arxitekturasÄ±: <code>GPT2LMHeadModel</code>.</td>
    </tr>
    <tr>
        <td><code>vocab_size</code></td>
        <td>Tokenizer-dÉ™ki tokenlÉ™rin Ã¼mumi sayÄ±. Bu sÉ™nÉ™ddÉ™ 100000-dir.</td>
    </tr>
    <tr>
        <td><code>n_ctx</code> vÉ™ <code>n_positions</code></td>
        <td>Bir dÉ™fÉ™lik giriÅŸdÉ™ maksimum token sayÄ±. Burada hÉ™r ikisi 2048-dir.</td>
    </tr>
    <tr>
        <td><code>n_embd</code></td>
        <td>Embedding Ã¶lÃ§Ã¼sÃ¼. HÉ™r token Ã¼Ã§Ã¼n vektorun uzunluÄŸu: 2048.</td>
    </tr>
    <tr>
        <td><code>n_head</code></td>
        <td>Attention mexanizminin "baÅŸ" sayÄ±: 16.</td>
    </tr>
    <tr>
        <td><code>n_layer</code></td>
        <td>Transformer qatlarÄ±nÄ±n sayÄ±. Burada 24 qatlÄ± dÉ™rin modeldir.</td>
    </tr>
    <tr>
        <td><code>activation_function</code></td>
        <td>Aktivasiya funksiyasÄ±: <code>gelu_new</code>. Transformer qatlarÄ±nda istifadÉ™ olunur.</td>
    </tr>
    <tr>
        <td><code>attn_pdrop</code>, <code>embd_pdrop</code>, <code>resid_pdrop</code></td>
        <td>Dropout dÉ™yÉ™rlÉ™ri â€“ overfitting-i azaltmaq Ã¼Ã§Ã¼n qatlarda tÉ™tbiq olunur (0.1 dÉ™yÉ™ri).</td>
    </tr>
    <tr>
        <td><code>layer_norm_epsilon</code></td>
        <td>Layer Normalization Ã¼Ã§Ã¼n Ã§ox kiÃ§ik sabit (1e-5).</td>
    </tr>
    <tr>
        <td><code>initializer_range</code></td>
        <td>Ã‡É™ki matrislÉ™rinin ilkin tÉ™sadÃ¼fi aralÄ±ÄŸÄ±.</td>
    </tr>
    <tr>
        <td><code>bos_token_id</code> / <code>eos_token_id</code></td>
        <td>BaÅŸlanÄŸÄ±c vÉ™ sonlandÄ±rÄ±cÄ± tokenlÉ™rin ID-lÉ™ri. Default GPT-2-dÉ™ 50256 olur, amma sÉ™nin vocab.json-a gÃ¶rÉ™ dÉ™yiÅŸÉ™ bilÉ™r!</td>
    </tr>
    <tr>
        <td><code>unk_token</code>, <code>pad_token</code>, <code>mask_token</code></td>
        <td>BilinmÉ™yÉ™n, doldurucu vÉ™ maska tokenlÉ™ri â€“ xÃ¼susi tokenlÉ™rdir.</td>
    </tr>
    <tr>
        <td><code>use_cache</code></td>
        <td>Model Ã§Ä±xÄ±ÅŸlarÄ± Ã¼Ã§Ã¼n cache istifadÉ™sini aktivlÉ™ÅŸdirir. Inference zamanÄ± sÃ¼rÉ™ti artÄ±rÄ±r.</td>
    </tr>
    <tr>
        <td><code>gradient_checkpointing</code></td>
        <td>ÆgÉ™r <code>true</code> olsa, daha az RAM istifadÉ™ edir, lakin treninq yavaÅŸdÄ±r.</td>
    </tr>
    <tr>
        <td><code>torch_dtype</code></td>
        <td>Torch tensorlarÄ±nÄ±n tipi. Burada <code>float32</code>, amma <code>float16</code> da ola bilÉ™r (GPU optimizasiyasÄ± Ã¼Ã§Ã¼n).</td>
    </tr>
    <tr>
        <td><code>summary_*</code> sahÉ™lÉ™ri</td>
        <td>BÉ™zi downstream tapÅŸÄ±rÄ±qlarda (mÉ™s: classification) modelin Ã§Ä±xÄ±ÅŸÄ±nÄ± necÉ™ xÃ¼lasÉ™ edÉ™cÉ™yini tÉ™yin edir.</td>
    </tr>
</table>

<p><strong>Qeyd:</strong> ÆgÉ™r sÉ™nin <code>vocab.json</code> faylÄ±nda <code>&lt;s&gt;</code>, <code>&lt;pad&gt;</code> vÉ™ s. tokenlÉ™r baÅŸqa ID ilÉ™ gÃ¶stÉ™rilibsÉ™, <strong>bos_token_id</strong> vÉ™ <strong>eos_token_id</strong> kimi sahÉ™lÉ™ri uyÄŸun ÅŸÉ™kildÉ™ dÃ¼zÉ™ltmÉ™k vacibdir.</p>

<h2>ğŸ“ŒTam Kod</h2>

<pre><code>
    {
    "activation_function": "gelu_new",  // Aktivasiya funksiyasÄ± olaraq GELU-nun yeni versiyasÄ± istifadÉ™ olunur.
    "architectures": [
    "GPT2LMHeadModel"                 // Model tipi: Dil modellÉ™ÅŸdirmÉ™ baÅŸlÄ±ÄŸÄ± olan GPT-2 (language modeling head).
    ],
    "attn_pdrop": 0.1,                  // Attention qatÄ±nda dropout ehtimalÄ±. Overfitting-i azaltmaq Ã¼Ã§Ã¼n.
    "bos_token_id": 50256,             // BaÅŸlanÄŸÄ±c token ID-si. Model sorÄŸunu buradan baÅŸlayÄ±r oxumaÄŸa.
    "embd_pdrop": 0.1,                 // Embedding qatÄ±nda tÉ™tbiq edilÉ™n dropout nisbÉ™ti.
    "eos_token_id": 50256,             // SonlandÄ±rÄ±cÄ± token ID-si. Model cavabÄ± buradan sonlandÄ±rÄ±r.
    "gradient_checkpointing": false,   // RAM yaddaÅŸÄ±na qÉ™naÉ™t Ã¼Ã§Ã¼n qatlarÄ±n aralÄ±q nÉ™ticÉ™lÉ™ri saxlanÄ±lÄ±r. False = deaktivdir.
    "initializer_range": 0.02,         // Modelin Ã§É™kilÉ™rinin ilkin random aralÄ±ÄŸÄ±.
    "layer_norm_epsilon": 1e-05,       // Layer normalization Ã¼Ã§Ã¼n epsilon dÉ™yÉ™ri (kiÃ§ik sabit).
    "model_type": "gpt2",              // Modelin tipi (Transformers kitabxanasÄ± Ã¼Ã§Ã¼n É™hÉ™miyyÉ™tlidir).
    "n_ctx": 2048,                     // KontekstdÉ™ baxÄ±la bilÉ™cÉ™k maksimum token sayÄ±.
    "n_embd": 2048,                    // HÉ™r bir token Ã¼Ã§Ã¼n embedding Ã¶lÃ§Ã¼sÃ¼ (dÉ™rinlik).
    "n_head": 16,                      // Multi-head attention-da baÅŸ sayÄ±. Paralel diqqÉ™t qatlarÄ±.
    "n_inner": null,                   // Feed-forward qatÄ±nÄ±n daxili Ã¶lÃ§Ã¼sÃ¼ (null = avtomatik hesablanÄ±r).
    "n_layer": 24,                     // Transformer qatlarÄ±nÄ±n sayÄ±. Bu model 24 qatlÄ±dÄ±r.
    "n_positions": 2048,               // Maksimum mÃ¶vqe kodlamasÄ± (positional encoding).
    "resid_pdrop": 0.1,                // Rezedual baÄŸlantÄ±larda (skip connections) dropout ehtimalÄ±.
    "scale_attn_weights": true,        // Attention Ã§É™ki dÉ™yÉ™rlÉ™ri Ã¶lÃ§Ã¼lÉ™ndirilÉ™cÉ™kmi?
    "summary_activation": null,        // Model Ã§Ä±xÄ±ÅŸ xÃ¼lasÉ™si Ã¼Ã§Ã¼n aktivasiya funksiyasÄ± (null = istifadÉ™ olunmur).
    "summary_first_dropout": 0.1,      // XÃ¼lasÉ™ Ã§Ä±xÄ±ÅŸÄ±nda ilk dropout nisbÉ™ti.
    "summary_proj_to_labels": true,    // XÃ¼lasÉ™ Ã§Ä±xÄ±ÅŸÄ± etiketlÉ™rÉ™ proyeksiya edilÉ™cÉ™kmi? (classification Ã¼Ã§Ã¼n)
    "summary_type": "cls_index",       // HansÄ± Ã¼sulla Ã§Ä±xÄ±ÅŸ xÃ¼lasÉ™lÉ™ÅŸdirilÉ™cÉ™k: burda CLS token indeksi ilÉ™.
    "summary_use_proj": true,          // Proyeksiya qatÄ±ndan istifadÉ™ edilÉ™cÉ™kmi?
    "torch_dtype": "float32",          // Modelin istifadÉ™ etdiyi mÉ™lumat tipi (Tensor data type).
    "transformers_version": "4.10.3",  // Transformers kitabxanasÄ±nÄ±n versiyasÄ±.
    "use_cache": true,                 // Inference zamanÄ± cache istifadÉ™si aktivdir (sÃ¼rÉ™t Ã¼Ã§Ã¼n).
    "vocab_size": 100000               // Tokenizer sÃ¶zlÃ¼k Ã¶lÃ§Ã¼sÃ¼ â€“ modelin tanÄ±dÄ±ÄŸÄ± Ã¼mumi token sayÄ±.
    }
</code></pre>

</body>
</html>
