<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8" />
    <title>Activation Function (Aktivasiya FunksiyasÄ±) - Riyazi Ä°zah</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          padding: 30px;
          background-color: #f7f9fc;
          color: #2c3e50;
          line-height: 1.7;
        }
        h1, h2 {
          color: #2c3e50;
        }
        .code {
          background: #eef;
          padding: 8px 12px;
          border-left: 4px solid #3498db;
          font-family: Consolas, monospace;
          margin: 10px 0;

        }
        .note {
          background: #e8f5e9;
          border-left: 6px solid #4caf50;
          padding: 10px;
          margin: 20px 0;
        }
        .warn {
          background: #fff3cd;
          border-left: 6px solid #ffc107;
          padding: 10px;
          margin: 20px 0;
        }
        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 10px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        ul {
          margin-left: 20px;
        }
    </style>
</head>
<body>

<div class="container">
    <h1>Ã‡ox yaxÅŸÄ± sualdÄ± ğŸ‘</h1>
    <p class="lead">Bu, <strong>neyron ÅŸÉ™bÉ™kÉ™sinin É™sas hesablamasÄ±nÄ±n</strong> riyazi formulu. GÉ™lin bunu sadÉ™ dildÉ™ + bir az rÉ™smi ÅŸÉ™kildÉ™ izah edim:</p>

    <hr/>

    <h2>ğŸ”¹ Formula</h2>
    <div class="box">
        <p>z = W . x + b</p>
        <p>h = {activation}(z)</p>
    </div>

    <hr/>

    <h2>ğŸ”¹ BuradakÄ± elementlÉ™r:</h2>
    <ul>
        <li><strong>x (input)</strong> â†’ GiriÅŸ mÉ™lumatÄ± (mÉ™sÉ™lÉ™n, bir ÅŸÉ™kil piksellÉ™ri, bir cÃ¼mlÉ™nin embedding-i vÉ™ s.).</li>
        <li style="margin-top:6px;">
            <strong>W (weights / Ã§É™ki matrisi)</strong> â†’ Modelin Ã¶yrÉ™nmÉ™yÉ™ Ã§alÄ±ÅŸdÄ±ÄŸÄ± parametrlÉ™r.
            <div class="code">HÉ™r giriÅŸin Ã§Ä±xÄ±ÅŸa necÉ™ tÉ™sir gÃ¶stÉ™rÉ™cÉ™yini mÃ¼É™yyÉ™nlÉ™ÅŸdirir â€” elÉ™ bil ki, "hansÄ± xÃ¼susiyyÉ™t vacibdir" bunu Ã¶yrÉ™nir.</div>
        </li>
        <li style="margin-top:6px;">
            <strong>b (bias / qÉ™rÉ™z vektoru)</strong> â†’ Sabit termin.
            <div class="code">Hesablamaya azacÄ±q â€œkÃ¶Ã§Ã¼rmÉ™â€ É™lavÉ™ edir; balansÄ± dÃ¼zÉ™ldÉ™n dÃ¼ymÉ™ kimi dÃ¼ÅŸÃ¼n.</div>
        </li>
        <li style="margin-top:6px;">
            <strong>z (linear output / xam Ã§Ä±xÄ±ÅŸ)</strong> â†’ GiriÅŸlÉ™rlÉ™ Ã§É™kilÉ™rin hasili Ã¼stÉ™gÉ™l bias.
            <div class="code">HÉ™lÉ™ ki, neyron heÃ§ bir qÉ™rar vermÉ™yib â€” bu, xam hesablamadÄ±r.</div>
        </li>
        <li style="margin-top:6px;">
            <strong>activation(z)</strong> â†’ Aktivasiya funksiyasÄ± (ReLU, GELU vÉ™ s.).
            <div class="code">Bu, neyronu qeyri-xÉ™tti edir. Aktivasiya olmazsa, model sadÉ™cÉ™ linear regression kimi iÅŸlÉ™yÉ™r vÉ™ mÃ¼rÉ™kkÉ™b nÃ¼munÉ™lÉ™ri Ã¶yrÉ™nÉ™ bilmÉ™z.</div>
        </li>
    </ul>

    <hr/>

    <h2>ğŸ”¹ MÉ™cazi izah</h2>
    <ul>
        <li><strong>x</strong> â†’ SÉ™nÉ™ gÉ™lÉ™n mÉ™lumat (mÉ™sÉ™lÉ™n, 16 nÉ™fÉ™r tÉ™lÉ™bÉ™nin imtahan cavablarÄ±).</li>
        <li><strong>W</strong> â†’ MÃ¼É™llimin qiymÉ™tlÉ™ndirmÉ™ meyarÄ± (hansÄ± suala daha Ã§ox É™hÉ™miyyÉ™t verilir).</li>
        <li><strong>b</strong> â†’ ÆlavÉ™ dÃ¼zÉ™liÅŸ (mÉ™sÉ™lÉ™n, mÃ¼É™llim Ã¼mumi 5 bal bonus verir).</li>
        <li><strong>z</strong> â†’ "Xam bal" â€” heÃ§ bir dÃ¼zÉ™liÅŸ edilmÉ™miÅŸ nÉ™ticÉ™.</li>
        <li><strong>activation(z)</strong> â†’ Æsl qÉ™rarÄ± verÉ™n funksiya â€” kimin keÃ§diyi, kimin qaldÄ±ÄŸÄ± vÉ™ s.</li>
    </ul>

    <hr/>

    <h2>âš¡ QÄ±saca desÉ™k:</h2>
    <ul>
        <li><strong>W vÉ™ b</strong> â†’ modelin Ã¶yrÉ™ndiyi parametrlÉ™rdir</li>
        <li><strong>x</strong> â†’ giriÅŸ mÉ™lumatÄ±dÄ±r</li>
        <li><strong>z</strong> â†’ linear nÉ™ticÉ™dir</li>
        <li><strong>activation(z)</strong> â†’ modeli aÄŸÄ±llÄ± edÉ™n addÄ±mdÄ±r</li>
    </ul>

    <div class="code">Ä°stÉ™yirsÉ™n mÉ™n sÉ™nÉ™ bunu <em>rÉ™qÉ™mlÉ™rlÉ™</em> kiÃ§ik bir misalda da gÃ¶stÉ™rim (mÉ™sÉ™lÉ™n, 3 Ã¶lÃ§Ã¼lÃ¼ giriÅŸ, sadÉ™ W vÉ™ b ilÉ™ hesablama)?</div>

    <footer>â€” HazÄ±rlayan: SÉ™nin kÃ¶mÉ™kÃ§in â€¢ Ä°stifadÉ™ Ã¼Ã§Ã¼n saytda aÃ§ vÉ™ brauzerdÉ™ bax.</footer>
</div>

<h1>ğŸ”† Aktivasiya FunksiyasÄ± (Activation Function) vÉ™ Riyazi Ä°zahÄ±</h1>

<h2>1. Aktivasiya funksiyasÄ± nÉ™dir?</h2>
<p>
    Aktivasiya funksiyasÄ± neyron ÅŸÉ™bÉ™kÉ™sinin daxilindÉ™ qeyri-xÉ™tti transformasiya yaradÄ±r. Bu, modelin mÃ¼rÉ™kkÉ™b nÃ¼munÉ™lÉ™ri Ã¶yrÉ™nmÉ™sinÉ™ imkan verir.
</p>

<h2>2. Transformer modellÉ™rindÉ™ É™sas aktivasiya funksiyalarÄ±</h2>
<ul>
    <li><strong>ReLU</strong>: <code>f(x) = max(0, x)</code></li>
    <li><strong>GELU</strong> (Gaussian Error Linear Unit): <code>f(x) = x * P(X â‰¤ x)</code>, burada <code>P</code> normal paylanmanÄ±n paylanma funksiyasÄ±dÄ±r.</li>
</ul>

<h2>3. GELU funksiyasÄ±nÄ±n riyazi ifadÉ™si</h2>
<p>
    GELU, daxil olan <code>x</code> dÉ™yÉ™rinin standart normal paylanmada mÉ™nfi vÉ™ ya mÃ¼sbÉ™t olma ehtimalÄ±nÄ± É™sas alÄ±r:
</p>

<div class="formula">
    <code>
        \displaystyle
        \mathrm{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2} \left[ 1 + \operatorname{erf} \left( \frac{x}{\sqrt{2}} \right) \right]
    </code>
</div>

<p>Burada:</p>
<ul>
    <li><code>\Phi(x)</code> â€” standart normal paylanmanÄ±n kÃ¼mÃ¼latif paylanma funksiyasÄ± (CDF)</li>
    <li><code>\operatorname{erf}(z)</code> â€” sÉ™hv funksiyasÄ± (error function)</li>
</ul>

<h2>4. YaxÄ±nlaÅŸdÄ±rma formulu</h2>
<p>Hesablama sÉ™mÉ™rÉ™liliyi Ã¼Ã§Ã¼n GELU aÅŸaÄŸÄ±dakÄ± yaxÄ±nlaÅŸma ilÉ™ istifadÉ™ edilir:</p>

<div class="formula">
    <code>
        \displaystyle
        \mathrm{GELU}(x) \approx 0.5 \cdot x \left( 1 + \tanh \left[ \sqrt{\frac{2}{\pi}} \left( x + 0.044715 \cdot x^3 \right) \right] \right)
    </code>
</div>

<h2>5. ReLU ilÉ™ mÃ¼qayisÉ™</h2>
<ul>
    <li>ReLU sadÉ™dir vÉ™ <code>f(x) = \max(0, x)</code></li>
    <li>GELU isÉ™ daha hamar vÉ™ statistika É™saslÄ± qeyri-xÉ™tti funksiya olaraq tÉ™klif edilir</li>
    <li>GELU aÅŸaÄŸÄ±dakÄ± xassÉ™lÉ™rÉ™ malikdir:
        <ul>
            <li>KiÃ§ik mÉ™nfi dÉ™yÉ™rlÉ™ri sÄ±fÄ±ra Ã§evirmir, yavaÅŸca sÄ±fÄ±ra yaxÄ±nlaÅŸdÄ±rÄ±r</li>
            <li>ReLU-dan daha hamar tÉ™rcÃ¼mÉ™ yaratmaqla, optimallaÅŸdÄ±rmanÄ± vÉ™ Ã¼mumi performansÄ± artÄ±rÄ±r</li>
        </ul>
    </li>
</ul>

<h2>6. Riyazi nÃ¼munÉ™</h2>
<p>MÉ™sÉ™lÉ™n, <code>x = 1.0</code> Ã¼Ã§Ã¼n GELU:</p>

<div class="formula">
    <code>
        \Phi(1.0) = \frac{1}{2} \left[ 1 + \operatorname{erf} \left( \frac{1.0}{\sqrt{2}} \right) \right] \approx 0.8413
    </code>
</div>
<p>BelÉ™liklÉ™,</p>
<div class="formula">
    <code>
        \mathrm{GELU}(1.0) = 1.0 \times 0.8413 = 0.8413
    </code>
</div>

<h2>7. NiyÉ™ Transformer-dÉ™ GELU istifadÉ™ olunur?</h2>
<ul>
    <li>Modelin daha hamar vÉ™ stabillÉ™ÅŸmiÅŸ Ã¶yrÉ™nmÉ™sini tÉ™min edir</li>
    <li>DÉ™rin neyron ÅŸÉ™bÉ™kÉ™lÉ™rdÉ™ ReLU ilÉ™ mÃ¼qayisÉ™dÉ™ daha yaxÅŸÄ± Ã¼mumilÉ™ÅŸdirmÉ™ qabiliyyÉ™ti verir</li>
</ul>

<h2>8. Yekun</h2>
<p>
    Aktivasiya funksiyasÄ± Transformer modellÉ™rindÉ™ daxil olan x-lÉ™ri qeyri-xÉ™tti ÅŸÉ™kildÉ™ dÉ™yiÅŸdirÉ™rÉ™k modelin mÃ¼rÉ™kkÉ™b nÃ¼munÉ™lÉ™ri Ã¶yrÉ™nmÉ™sini tÉ™min edir.
    GELU, ReLU-nun daha inkiÅŸaf etmiÅŸ formasÄ±dÄ±r vÉ™ modern Transformer modellÉ™rindÉ™ standartdÄ±r.
</p>

<h2>Neural Network Hesablama NÃ¼munÉ™si</h2>

<h3>Setup</h3>
<p>
    x = [1, 2, 3] <br>
    W = [[0.2, -0.5, 1.0],<br>
    &nbsp;&nbsp;&nbsp;&nbsp;[-1.5, 0.7, 0.3]] <br>
    b = [0.1, -0.2]
</p>

<hr>

<h3>1) Linear Hesablama: <code>z = WÂ·x + b</code></h3>
<p>
    Neyron 1: 0.2*1 + (-0.5)*2 + 1.0*3 + 0.1 = <b>2.3</b><br>
    Neyron 2: -1.5*1 + 0.7*2 + 0.3*3 - 0.2 = <b>0.6</b><br>
    <br>
    <b>z = [2.3, 0.6]</b>
</p>

<hr>

<h3>2) Aktivasiya</h3>

<p><b>ReLU:</b> max(0, z)</p>
<pre>h_relu = [2.3, 0.6]</pre>

<p><b>GELU:</b> yumÅŸaq keÃ§id</p>
<pre>h_gelu â‰ˆ [2.276, 0.435]</pre>

<hr>

<h3>3) Ã‡Ä±xÄ±ÅŸ QatÄ±</h3>
<p>
    w_out = [1.2, -0.7], b_out = 0.05
</p>

<p>
    ÆgÉ™r h_relu istifadÉ™ etsÉ™k:<br>
    o = 1.2*2.3 -0.7*0.6 + 0.05 = <b>2.39</b><br>
    sigmoid(2.39) â‰ˆ <b>0.917</b>
</p>

<p>
    ÆgÉ™r h_gelu istifadÉ™ etsÉ™k:<br>
    o â‰ˆ 2.4767 â†’ sigmoid(2.4767) â‰ˆ <b>0.923</b>
</p>

<hr>

<h3>Yekun</h3>
<ul>
    <li><code>WÂ·x + b</code> â†’ xam xÉ™tti hesablamadÄ±r</li>
    <li><code>activation(z)</code> â†’ qeyri-xÉ™tti transformasiya (ReLU, GELU vÉ™ s.)</li>
    <li>Aktivasiya nÉ™ticÉ™ni formalaÅŸdÄ±rÄ±r vÉ™ son Ã§Ä±xÄ±ÅŸa tÉ™sir edir</li>
</ul>

</body>
</html>
