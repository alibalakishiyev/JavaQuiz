<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8" />
    <title>Activation Function (Aktivasiya FunksiyasÄ±) - Riyazi Ä°zah</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; padding: 30px; background-color: #fafafa; color: #222; }
        h1, h2 { color: #2c3e50; }
        code { background: #eee; padding: 3px 6px; border-radius: 4px; }
        .formula { background: #fff; border-left: 5px solid #2980b9; padding: 15px; margin: 15px 0; font-size: 18px; }
    </style>
</head>
<body>

<h1>ğŸ”† Aktivasiya FunksiyasÄ± (Activation Function) vÉ™ Riyazi Ä°zahÄ±</h1>

<h2>1. Aktivasiya funksiyasÄ± nÉ™dir?</h2>
<p>
    Aktivasiya funksiyasÄ± neyron ÅŸÉ™bÉ™kÉ™sinin daxilindÉ™ qeyri-xÉ™tti transformasiya yaradÄ±r. Bu, modelin mÃ¼rÉ™kkÉ™b nÃ¼munÉ™lÉ™ri Ã¶yrÉ™nmÉ™sinÉ™ imkan verir.
</p>

<h2>2. Transformer modellÉ™rindÉ™ É™sas aktivasiya funksiyalarÄ±</h2>
<ul>
    <li><strong>ReLU</strong>: <code>f(x) = max(0, x)</code></li>
    <li><strong>GELU</strong> (Gaussian Error Linear Unit): <code>f(x) = x * P(X â‰¤ x)</code>, burada <code>P</code> normal paylanmanÄ±n paylanma funksiyasÄ±dÄ±r.</li>
</ul>

<h2>3. GELU funksiyasÄ±nÄ±n riyazi ifadÉ™si</h2>
<p>
    GELU, daxil olan <code>x</code> dÉ™yÉ™rinin standart normal paylanmada mÉ™nfi vÉ™ ya mÃ¼sbÉ™t olma ehtimalÄ±nÄ± É™sas alÄ±r:
</p>

<div class="formula">
    <code>
        \displaystyle
        \mathrm{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2} \left[ 1 + \operatorname{erf} \left( \frac{x}{\sqrt{2}} \right) \right]
    </code>
</div>

<p>Burada:</p>
<ul>
    <li><code>\Phi(x)</code> â€” standart normal paylanmanÄ±n kÃ¼mÃ¼latif paylanma funksiyasÄ± (CDF)</li>
    <li><code>\operatorname{erf}(z)</code> â€” sÉ™hv funksiyasÄ± (error function)</li>
</ul>

<h2>4. YaxÄ±nlaÅŸdÄ±rma formulu</h2>
<p>Hesablama sÉ™mÉ™rÉ™liliyi Ã¼Ã§Ã¼n GELU aÅŸaÄŸÄ±dakÄ± yaxÄ±nlaÅŸma ilÉ™ istifadÉ™ edilir:</p>

<div class="formula">
    <code>
        \displaystyle
        \mathrm{GELU}(x) \approx 0.5 \cdot x \left( 1 + \tanh \left[ \sqrt{\frac{2}{\pi}} \left( x + 0.044715 \cdot x^3 \right) \right] \right)
    </code>
</div>

<h2>5. ReLU ilÉ™ mÃ¼qayisÉ™</h2>
<ul>
    <li>ReLU sadÉ™dir vÉ™ <code>f(x) = \max(0, x)</code></li>
    <li>GELU isÉ™ daha hamar vÉ™ statistika É™saslÄ± qeyri-xÉ™tti funksiya olaraq tÉ™klif edilir</li>
    <li>GELU aÅŸaÄŸÄ±dakÄ± xassÉ™lÉ™rÉ™ malikdir:
        <ul>
            <li>KiÃ§ik mÉ™nfi dÉ™yÉ™rlÉ™ri sÄ±fÄ±ra Ã§evirmir, yavaÅŸca sÄ±fÄ±ra yaxÄ±nlaÅŸdÄ±rÄ±r</li>
            <li>ReLU-dan daha hamar tÉ™rcÃ¼mÉ™ yaratmaqla, optimallaÅŸdÄ±rmanÄ± vÉ™ Ã¼mumi performansÄ± artÄ±rÄ±r</li>
        </ul>
    </li>
</ul>

<h2>6. Riyazi nÃ¼munÉ™</h2>
<p>MÉ™sÉ™lÉ™n, <code>x = 1.0</code> Ã¼Ã§Ã¼n GELU:</p>

<div class="formula">
    <code>
        \Phi(1.0) = \frac{1}{2} \left[ 1 + \operatorname{erf} \left( \frac{1.0}{\sqrt{2}} \right) \right] \approx 0.8413
    </code>
</div>
<p>BelÉ™liklÉ™,</p>
<div class="formula">
    <code>
        \mathrm{GELU}(1.0) = 1.0 \times 0.8413 = 0.8413
    </code>
</div>

<h2>7. NiyÉ™ Transformer-dÉ™ GELU istifadÉ™ olunur?</h2>
<ul>
    <li>Modelin daha hamar vÉ™ stabillÉ™ÅŸmiÅŸ Ã¶yrÉ™nmÉ™sini tÉ™min edir</li>
    <li>DÉ™rin neyron ÅŸÉ™bÉ™kÉ™lÉ™rdÉ™ ReLU ilÉ™ mÃ¼qayisÉ™dÉ™ daha yaxÅŸÄ± Ã¼mumilÉ™ÅŸdirmÉ™ qabiliyyÉ™ti verir</li>
</ul>

<h2>8. Yekun</h2>
<p>
    Aktivasiya funksiyasÄ± Transformer modellÉ™rindÉ™ daxil olan x-lÉ™ri qeyri-xÉ™tti ÅŸÉ™kildÉ™ dÉ™yiÅŸdirÉ™rÉ™k modelin mÃ¼rÉ™kkÉ™b nÃ¼munÉ™lÉ™ri Ã¶yrÉ™nmÉ™sini tÉ™min edir.
    GELU, ReLU-nun daha inkiÅŸaf etmiÅŸ formasÄ±dÄ±r vÉ™ modern Transformer modellÉ™rindÉ™ standartdÄ±r.
</p>

</body>
</html>
