<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8" />
    <title>Activation Function (Aktivasiya FunksiyasÄ±) - Riyazi Ä°zah</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          padding: 30px;
          background-color: #f7f9fc;
          color: #2c3e50;
          line-height: 1.7;
        }
        h1, h2 {
          color: #2c3e50;
        }
        .code {
          background: #eef;
          padding: 8px 12px;
          border-left: 4px solid #3498db;
          font-family: Consolas, monospace;
          margin: 10px 0;

        }
        .note {
          background: #e8f5e9;
          border-left: 6px solid #4caf50;
          padding: 10px;
          margin: 20px 0;
        }
        .warn {
          background: #fff3cd;
          border-left: 6px solid #ffc107;
          padding: 10px;
          margin: 20px 0;
        }
        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 10px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        ul {
          margin-left: 20px;
        }
    </style>
</head>
<body>

<h1>ğŸ”† Aktivasiya FunksiyasÄ± (Activation Function) vÉ™ Riyazi Ä°zahÄ±</h1>

<h2>1. Aktivasiya funksiyasÄ± nÉ™dir?</h2>
<p>
    Aktivasiya funksiyasÄ± neyron ÅŸÉ™bÉ™kÉ™sinin daxilindÉ™ qeyri-xÉ™tti transformasiya yaradÄ±r. Bu, modelin mÃ¼rÉ™kkÉ™b nÃ¼munÉ™lÉ™ri Ã¶yrÉ™nmÉ™sinÉ™ imkan verir.
</p>

<h2>2. Transformer modellÉ™rindÉ™ É™sas aktivasiya funksiyalarÄ±</h2>
<ul>
    <li><strong>ReLU</strong>: <code>f(x) = max(0, x)</code></li>
    <li><strong>GELU</strong> (Gaussian Error Linear Unit): <code>f(x) = x * P(X â‰¤ x)</code>, burada <code>P</code> normal paylanmanÄ±n paylanma funksiyasÄ±dÄ±r.</li>
</ul>

<h2>3. GELU funksiyasÄ±nÄ±n riyazi ifadÉ™si</h2>
<p>
    GELU, daxil olan <code>x</code> dÉ™yÉ™rinin standart normal paylanmada mÉ™nfi vÉ™ ya mÃ¼sbÉ™t olma ehtimalÄ±nÄ± É™sas alÄ±r:
</p>

<div class="formula">
    <code>
        \displaystyle
        \mathrm{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2} \left[ 1 + \operatorname{erf} \left( \frac{x}{\sqrt{2}} \right) \right]
    </code>
</div>

<p>Burada:</p>
<ul>
    <li><code>\Phi(x)</code> â€” standart normal paylanmanÄ±n kÃ¼mÃ¼latif paylanma funksiyasÄ± (CDF)</li>
    <li><code>\operatorname{erf}(z)</code> â€” sÉ™hv funksiyasÄ± (error function)</li>
</ul>

<h2>4. YaxÄ±nlaÅŸdÄ±rma formulu</h2>
<p>Hesablama sÉ™mÉ™rÉ™liliyi Ã¼Ã§Ã¼n GELU aÅŸaÄŸÄ±dakÄ± yaxÄ±nlaÅŸma ilÉ™ istifadÉ™ edilir:</p>

<div class="formula">
    <code>
        \displaystyle
        \mathrm{GELU}(x) \approx 0.5 \cdot x \left( 1 + \tanh \left[ \sqrt{\frac{2}{\pi}} \left( x + 0.044715 \cdot x^3 \right) \right] \right)
    </code>
</div>

<h2>5. ReLU ilÉ™ mÃ¼qayisÉ™</h2>
<ul>
    <li>ReLU sadÉ™dir vÉ™ <code>f(x) = \max(0, x)</code></li>
    <li>GELU isÉ™ daha hamar vÉ™ statistika É™saslÄ± qeyri-xÉ™tti funksiya olaraq tÉ™klif edilir</li>
    <li>GELU aÅŸaÄŸÄ±dakÄ± xassÉ™lÉ™rÉ™ malikdir:
        <ul>
            <li>KiÃ§ik mÉ™nfi dÉ™yÉ™rlÉ™ri sÄ±fÄ±ra Ã§evirmir, yavaÅŸca sÄ±fÄ±ra yaxÄ±nlaÅŸdÄ±rÄ±r</li>
            <li>ReLU-dan daha hamar tÉ™rcÃ¼mÉ™ yaratmaqla, optimallaÅŸdÄ±rmanÄ± vÉ™ Ã¼mumi performansÄ± artÄ±rÄ±r</li>
        </ul>
    </li>
</ul>

<h2>6. Riyazi nÃ¼munÉ™</h2>
<p>MÉ™sÉ™lÉ™n, <code>x = 1.0</code> Ã¼Ã§Ã¼n GELU:</p>

<div class="formula">
    <code>
        \Phi(1.0) = \frac{1}{2} \left[ 1 + \operatorname{erf} \left( \frac{1.0}{\sqrt{2}} \right) \right] \approx 0.8413
    </code>
</div>
<p>BelÉ™liklÉ™,</p>
<div class="formula">
    <code>
        \mathrm{GELU}(1.0) = 1.0 \times 0.8413 = 0.8413
    </code>
</div>

<h2>7. NiyÉ™ Transformer-dÉ™ GELU istifadÉ™ olunur?</h2>
<ul>
    <li>Modelin daha hamar vÉ™ stabillÉ™ÅŸmiÅŸ Ã¶yrÉ™nmÉ™sini tÉ™min edir</li>
    <li>DÉ™rin neyron ÅŸÉ™bÉ™kÉ™lÉ™rdÉ™ ReLU ilÉ™ mÃ¼qayisÉ™dÉ™ daha yaxÅŸÄ± Ã¼mumilÉ™ÅŸdirmÉ™ qabiliyyÉ™ti verir</li>
</ul>

<h2>8. Yekun</h2>
<p>
    Aktivasiya funksiyasÄ± Transformer modellÉ™rindÉ™ daxil olan x-lÉ™ri qeyri-xÉ™tti ÅŸÉ™kildÉ™ dÉ™yiÅŸdirÉ™rÉ™k modelin mÃ¼rÉ™kkÉ™b nÃ¼munÉ™lÉ™ri Ã¶yrÉ™nmÉ™sini tÉ™min edir.
    GELU, ReLU-nun daha inkiÅŸaf etmiÅŸ formasÄ±dÄ±r vÉ™ modern Transformer modellÉ™rindÉ™ standartdÄ±r.
</p>

</body>
</html>
