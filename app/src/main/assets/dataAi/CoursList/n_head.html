<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>n_head AÃ§Ä±qlamasÄ± - Multi-Head Attention</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          padding: 30px;
          background-color: #f7f9fc;
          color: #2c3e50;
          line-height: 1.7;
        }
        h1, h2 {
          color: #2c3e50;
        }
        .code {
          background: #eef;
          padding: 8px 12px;
          border-left: 4px solid #3498db;
          font-family: Consolas, monospace;
          margin: 10px 0;
        }
        .note {
          background: #e8f5e9;
          border-left: 6px solid #4caf50;
          padding: 10px;
          margin: 20px 0;
        }
        .warn {
          background: #fff3cd;
          border-left: 6px solid #ffc107;
          padding: 10px;
          margin: 20px 0;
        }
        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 10px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        ul {
          margin-left: 20px;
        }
    </style>
</head>
<body>

<h1>ğŸ”€ <code>n_head = 12</code> â€” Multi-Head Attention (Ã‡ox BaÅŸlÄ±qlÄ± DiqqÉ™t)</h1>

<div class="formula">
    config = GPT2Config(<br>
    &nbsp;&nbsp;&nbsp;&nbsp;n_head = 12,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;n_embd = 768,<br>
    ...)
</div>

<h2>ğŸ“Œ NÉ™dir <code>n_head</code>?</h2>
<div class="formula">
    <strong>n_head</strong> â€” Transformer blokundakÄ± diqqÉ™t (attention) mexanizminin neÃ§É™ baÅŸlÄ±qdan ibarÉ™t olduÄŸunu gÃ¶stÉ™rir.
    <br><br>
    HÉ™r baÅŸlÄ±q fÉ™rqli bir alt-mÉ™kanda (subspace) diqqÉ™t (self-attention) tÉ™tbiq edir vÉ™ nÉ™ticÉ™dÉ™ mÃ¼xtÉ™lif semantik kontekstlÉ™ri tuta bilir.
</div>

<h2>ğŸ§  Multi-Head Attention Prinsipi</h2>
<ul>
    <li>Model mÉ™lumatÄ± paralel ÅŸÉ™kildÉ™ <strong>n_head</strong> sayda parÃ§alayÄ±r.</li>
    <li>HÉ™r parÃ§a (baÅŸlÄ±q) Ã¶z <code>Q (query), K (key), V (value)</code> matrislÉ™ri ilÉ™ iÅŸlÉ™yir.</li>
    <li>ÆldÉ™ olunan diqqÉ™t nÉ™ticÉ™lÉ™ri birlÉ™ÅŸdirilir vÉ™ yenidÉ™n xÉ™tti transformasiyadan keÃ§irilir.</li>
</ul>

<h2>ğŸ’¡ Riyazi Struktur</h2>
<div class="formula">
    Head DimensiyasÄ± = n_embd / n_head = 768 / 12 = 64<br><br>
    HÉ™r baÅŸlÄ±ÄŸÄ±n Ã¶lÃ§Ã¼sÃ¼: 64<br>
    Toplam paralel baÅŸlÄ±q: 12
</div>

<h2>ğŸ“Š Parametr HesablamasÄ±</h2>
<div class="formula">
    Q, K, V Ã¼Ã§Ã¼n hÉ™r biri:<br>
    â†’ (n_embd Ã— head_dim) Ã— n_head = (768 Ã— 64) Ã— 3 = 147,456<br><br>
    BirlÉ™ÅŸdirmÉ™ Ã¼Ã§Ã¼n xÉ™tti qat (W<sub>O</sub>):<br>
    â†’ 768 Ã— 768 = 589,824<br><br>
    Toplam Parametr: 147,456 + 589,824 = <strong>737,280</strong> parametr yalnÄ±z Attention Ã¼Ã§Ã¼n
</div>

<h2>ğŸ“Œ NÉ™ Ã¼Ã§Ã¼n Ã§ox baÅŸlÄ±q?</h2>
<div class="note">
    <ul>
        <li>â• HÉ™r baÅŸlÄ±q fÉ™rqli semantikaya diqqÉ™t edir (mÉ™s: qrammatika, mÉ™safÉ™, É™laqÉ™lÉ™r)</li>
        <li>â• Daha paralel vÉ™ sÃ¼rÉ™tli Ã¶yrÉ™nmÉ™</li>
        <li>â• DÉ™rin É™laqÉ™lÉ™ri gÃ¶rmÉ™k imkanÄ±</li>
        <li>â– YÃ¼ksÉ™k hesablama yÃ¼kÃ¼</li>
    </ul>
</div>

<h2>ğŸ“‰ YÃ¼klÉ™mÉ™ vÉ™ Performans</h2>
<div class="warn">
    NÉ™ qÉ™dÉ™r Ã§ox <code>n_head</code> olsa, GPU yaddaÅŸ yÃ¼kÃ¼ vÉ™ parametr sayÄ± da artÄ±r.
    <br>
    Amma az baÅŸlÄ±q modelin zÉ™if kontekstdÉ™ uÄŸursuzluÄŸuna sÉ™bÉ™b ola bilÉ™r.
</div>

<h2>ğŸ“ TÉ™crÃ¼bÉ™dÉ™ NÉ™ SeÃ§ilmÉ™lidir?</h2>
<ul>
    <li><strong>n_embd = 768</strong> Ã¼Ã§Ã¼n tipik <strong>n_head = 12</strong> olur</li>
    <li>BaÅŸlÄ±q Ã¶lÃ§Ã¼sÃ¼ <strong>n_embd / n_head</strong> tam bÃ¶lÃ¼nmÉ™lidir</li>
    <li>n_head Ã§ox bÃ¶yÃ¼k olarsa: parametr sayÄ± vÉ™ yÃ¼k eksponent ÅŸÉ™kildÉ™ artÄ±r</li>
</ul>

<h2>âœ… Yekun</h2>
<div class="note">
    <strong>n_head = 12</strong> â€” 768 vektoru 12 parÃ§aya bÃ¶lÃ¼b hÉ™r birini fÉ™rqli semantik kontekstdÉ™ diqqÉ™tlÉ™ incÉ™lÉ™yir.<br>
    Bu, dil modellÉ™rinin kontekstdÉ™ ardÄ±cÄ±llÄ±q qurmasÄ±nÄ± vÉ™ uzunmÃ¼ddÉ™tli É™laqÉ™lÉ™ri tutmasÄ±nÄ± tÉ™min edir.
</div>

</body>
</html>
