<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>Self-Attention Ä°zahÄ±</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            padding: 30px;
            max-width: 900px;
            margin: auto;
        }
        h2 {
            color: #4b0082;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-left: 3px solid #4b0082;
            overflow-x: auto;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 15px 0;
        }
        th, td {
            border: 1px solid #888;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #ddd;
        }
        code {
            color: #c7254e;
            background-color: #f9f2f4;
            padding: 2px 4px;
        }
    </style>
</head>
<body>

<h2>ğŸ§  1. Self-Attention nÉ™dir vÉ™ nÉ™ iÅŸ gÃ¶rÃ¼r?</h2>
<p>Self-Attention Transformer modelindÉ™ hÉ™r bir sÃ¶zÃ¼n digÉ™r sÃ¶zlÉ™rlÉ™ <strong>nÉ™ qÉ™dÉ™r É™laqÉ™li olduÄŸunu</strong> hesablamaq Ã¼Ã§Ã¼n istifadÉ™ olunur.</p>
<p>YÉ™ni hÉ™r sÃ¶z digÉ™r bÃ¼tÃ¼n sÃ¶zlÉ™rÉ™ â€œbaxÄ±râ€ vÉ™ Ã¶zÃ¼ Ã¼Ã§Ã¼n hansÄ±larÄ±nÄ±n <strong>daha vacib olduÄŸunu</strong> tapÄ±r.</p>

<h2>ğŸ’¡ Misal:</h2>
<p><code>â€œMÉ™ryÉ™m kitab oxuyur Ã§Ã¼nki o sevir.â€</code> â€” burada â€œoâ€ sÃ¶zÃ¼ â€œMÉ™ryÉ™mâ€i nÉ™zÉ™rdÉ™ tutur.</p>
<p>Model bunu Self-Attention vasitÉ™silÉ™ anlaya bilÉ™r.</p>

<h2>ğŸ”¬ 2. Riyazi Formul</h2>
<p>Self-Attention formul aÅŸaÄŸÄ±dakÄ± kimidir:</p>
<p>
    $$
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    $$
</p>

<table>
    <tr><th>Simvol</th><th>AÃ§Ä±qlama</th></tr>
    <tr><td><strong>Q (Query)</strong></td><td>HÉ™dÉ™f sÃ¶zÃ¼n sorÄŸusu (nÉ™yÉ™ baxmaq istÉ™yir)</td></tr>
    <tr><td><strong>K (Key)</strong></td><td>DigÉ™r sÃ¶zlÉ™rin tanÄ±dÄ±cÄ±larÄ± (baxÄ±lacaq sÃ¶zlÉ™r)</td></tr>
    <tr><td><strong>V (Value)</strong></td><td>Æsas informasiyanÄ± daÅŸÄ±yan vektorlar</td></tr>
    <tr><td><strong>d<sub>k</sub></strong></td><td>Key Ã¶lÃ§Ã¼sÃ¼ (normallaÅŸma Ã¼Ã§Ã¼n)</td></tr>
</table>

<h2>ğŸ§® 3. Riyazi Hesablama â€“ SadÉ™ NÃ¼munÉ™</h2>

<p>Tutaq ki, 3 sÃ¶zlÃ¼k cÃ¼mlÉ™miz var: <code>["MÉ™ryÉ™m", "kitab", "o"]</code></p>
<p>HÉ™r biri 4 Ã¶lÃ§Ã¼lÃ¼ vektorlarla tÉ™msil olunur:</p>

<pre><code>Q = [
 [0.2, 0.4, 0.6, 0.1],   # MÉ™ryÉ™m
 [0.1, 0.3, 0.7, 0.0],   # kitab
 [0.9, 0.1, 0.4, 0.3]    # o
]

K = [
 [0.1, 0.3, 0.5, 0.2],   # MÉ™ryÉ™m
 [0.2, 0.2, 0.6, 0.1],   # kitab
 [0.8, 0.1, 0.3, 0.4]    # o
]</code></pre>

<p>HÉ™r sÃ¶z Ã¼Ã§Ã¼n <code>Q_i â‹… K_j</code> nÃ¶qtÉ™ hasilini tapÄ±rÄ±q.</p>

<p><strong>Misal:</strong> â€œoâ€ sÃ¶zÃ¼nÃ¼n â€œMÉ™ryÉ™mâ€É™ diqqÉ™ti:</p>
<p>
    $$
    \begin{align*}
    Q_o &= [0.9, 0.1, 0.4, 0.3] \\
    K_{mÉ™ryÉ™m} &= [0.1, 0.3, 0.5, 0.2] \\
    Q_o \cdot K_{mÉ™ryÉ™m} &= (0.9Ã—0.1) + (0.1Ã—0.3) + (0.4Ã—0.5) + (0.3Ã—0.2) \\
    &= 0.09 + 0.03 + 0.20 + 0.06 = 0.38
    \end{align*}
    $$
</p>

<h2>ğŸ“ 4. SkorlarÄ± NormallaÅŸdÄ±r</h2>
<p>ÆgÉ™r <code>d<sub>k</sub> = 4</code> isÉ™, kÃ¶k altÄ±: <code>âˆš4 = 2</code>.</p>
<p>BÃ¼tÃ¼n dot product skorlarÄ± bu É™dÉ™dÉ™ bÃ¶lÃ¼nÃ¼r.</p>

<h2>ğŸ“Š 5. Softmax tÉ™tbiqi</h2>
<p>NormallaÅŸmÄ±ÅŸ skorlar Ã¼zÉ™rindÉ™ <strong>Softmax</strong> tÉ™tbiq olunur ki, diqqÉ™t paylarÄ± alÄ±nsÄ±n:</p>

<p>
    $$
    \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
    $$
</p>

<h2>ğŸ“¦ 6. DiqqÉ™t PayÄ± ilÉ™ V dÉ™yÉ™rlÉ™rinin Ã§É™kili cÉ™mi</h2>
<p>NÉ™ticÉ™dÉ™ Ã§Ä±xÄ±ÅŸ vektoru alÄ±nÄ±r:</p>

<p>
    $$
    \text{output}_i = \sum_j \left( \text{attention score}_{i,j} \cdot V_j \right)
    $$
</p>

<h2>ğŸ§  NÉ™ticÉ™dÉ™ NÉ™ BaÅŸ Verir?</h2>
<ul>
    <li>â€œoâ€ sÃ¶zÃ¼ â€œMÉ™ryÉ™mâ€É™ daha Ã§ox diqqÉ™t verirsÉ™ â†’ model anlayacaq ki, â€œoâ€ = â€œMÉ™ryÉ™mâ€</li>
    <li>BÃ¼tÃ¼n sÃ¶zlÉ™r bir-biri ilÉ™ É™laqÉ™li ÅŸÉ™kildÉ™ tÉ™hlil olunur</li>
    <li>Bu prosesi paralel ÅŸÉ™kildÉ™ bÃ¼tÃ¼n cÃ¼mlÉ™yÉ™ tÉ™tbiq edir</li>
</ul>

<h2>ğŸ¯ Self-Attention nÉ™ iÅŸÉ™ yarayÄ±r?</h2>

<table>
    <tr><th>Ä°mkan</th><th>TÉ™siri</th></tr>
    <tr><td>KontekstÉ™ É™sasÉ™n É™laqÉ™ qurur</td><td>ÆvÉ™zliklÉ™ri dÃ¼zgÃ¼n anlamaÄŸa kÃ¶mÉ™k edir</td></tr>
    <tr><td>Uzun mÉ™safÉ™li É™laqÉ™</td><td>ÆvvÉ™ldÉ™n gÉ™lÉ™n sÃ¶zlÉ™ri xatÄ±rlayÄ±r</td></tr>
    <tr><td>Paralel vÉ™ sÃ¼rÉ™tli</td><td>RNN-dÉ™n daha sÃ¼rÉ™tli tÉ™lim vÉ™ inferens</td></tr>
    <tr><td>Transformer É™saslÄ±dÄ±r</td><td>BERT, GPT vÉ™ T5 bu É™sasda qurulub</td></tr>
</table>

</body>
</html>
