<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>ffn_dim (n_inner) â€“ Feed Forward Layer Ä°zahedici</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          padding: 30px;
          background-color: #f7f9fc;
          color: #2c3e50;
          line-height: 1.7;
        }
        h1, h2 {
          color: #2c3e50;
        }
        .code {
          background: #eef;
          padding: 8px 12px;
          border-left: 4px solid #3498db;
          font-family: Consolas, monospace;
          margin: 10px 0;

        }
        .note {
          background: #e8f5e9;
          border-left: 6px solid #4caf50;
          padding: 10px;
          margin: 20px 0;
        }
        .warn {
          background: #fff3cd;
          border-left: 6px solid #ffc107;
          padding: 10px;
          margin: 20px 0;
        }
        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 10px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        ul {
          margin-left: 20px;
        }
    </style>
</head>
<body>

<h1>ğŸ”¢ <code>ffn_dim</code> (vÉ™ ya <code>n_inner</code>) nÉ™dir?</h1>

<p>
    Transformer modelindÉ™ <b>hÉ™r Transformer blokunun daxilindÉ™</b> bir <strong>Feed Forward Network (FFN)</strong>
    hissÉ™si var. <code>ffn_dim</code> bu FFN hissÉ™sinin daxili qatÄ±nÄ±n Ã¶lÃ§Ã¼sÃ¼nÃ¼ mÃ¼É™yyÉ™n edir.
</p>

<h2>ğŸ“ Strukturu necÉ™dir?</h2>
<p>FFN iki tam baÄŸlÄ± qatdan ibarÉ™tdir:</p>
<div class="formula">
    <code>FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚</code>
</div>

<ul>
    <li><code>Wâ‚</code> Ã¶lÃ§Ã¼sÃ¼: <code>[d_model, ffn_dim]</code></li>
    <li><code>Wâ‚‚</code> Ã¶lÃ§Ã¼sÃ¼: <code>[ffn_dim, d_model]</code></li>
    <li><code>d_model</code> â€” É™sas embedding Ã¶lÃ§Ã¼sÃ¼ (mÉ™sÉ™lÉ™n: 768)</li>
    <li><code>ffn_dim</code> â€” gizli qatÄ±n Ã¶lÃ§Ã¼sÃ¼ (mÉ™sÉ™lÉ™n: 3072)</li>
</ul>

<h2>ğŸ§® Riyazi Hesablama:</h2>
<p>Tutaq ki:</p>
<ul>
    <li><code>d_model = 768</code></li>
    <li><code>ffn_dim = 3072</code></li>
</ul>

<p>ÆmÉ™liyyat ardÄ±cÄ±llÄ±ÄŸÄ±:</p>
<ol>
    <li><code>x</code> Ã¶lÃ§Ã¼lÃ¼ vektor (1 Ã— 768)</li>
    <li><code>x @ Wâ‚ â†’ 1 Ã— 3072</code></li>
    <li><code>ReLU â†’ max(0, Â·)</code></li>
    <li><code>@ Wâ‚‚ â†’ 1 Ã— 768</code> (yenidÉ™n orijinal Ã¶lÃ§Ã¼yÉ™ dÃ¼ÅŸÃ¼r)</li>
</ol>

<div class="formula">
    Hesablanacaq aÄŸÄ±rlÄ±qlarÄ±n sayÄ±:<br>
    <strong>
        768 Ã— 3072 + 3072 (bias) + 3072 Ã— 768 + 768 (bias) = <br>
        â‰ˆ 4.7 milyon parametr tÉ™kcÉ™ FFN Ã¼Ã§Ã¼n.
    </strong>
</div>

<h2>ğŸ¯ NiyÉ™ bu qÉ™dÉ™r bÃ¶yÃ¼k?</h2>
<ul>
    <li><strong>TransformerdÉ™ É™sas "hesablama gÃ¼cÃ¼" FFN qatlarÄ±ndadÄ±r</strong></li>
    <li>Bu qatlarda mÉ™lumat qeyri-xÉ™tti ÅŸÉ™kildÉ™ "transformasiya" olunur</li>
    <li>Tipik olaraq <code>ffn_dim = 4 Ã— d_model</code></li>
    <li>GPT-2 Ã¼Ã§Ã¼n: <code>d_model = 768</code> â†’ <code>ffn_dim = 3072</code></li>
</ul>

<h2>ğŸ§  Dropout vÉ™ Aktivasiya</h2>
<ul>
    <li>Ä°ki fully-connected qat arasÄ±nda <code>ReLU</code> vÉ™ ya <code>GELU</code> aktivasiya funksiyasÄ± olur</li>
    <li>BÉ™zÉ™n dropout da tÉ™tbiq edilir (resid_pdrop kimi)</li>
</ul>

<h2>ğŸ“Œ ffn_dim necÉ™ seÃ§ilmÉ™lidir?</h2>
<ul>
    <li>ÆgÉ™r <code>d_model = 1024</code> â†’ <code>ffn_dim â‰ˆ 4096</code></li>
    <li>ÆgÉ™r Ã§ox kiÃ§ik seÃ§ilsÉ™ â†’ Modelin gÃ¼cÃ¼ azalÄ±r</li>
    <li>Ã‡ox bÃ¶yÃ¼k seÃ§ilsÉ™ â†’ Parametr sayÄ± vÉ™ GPU istifadÉ™si artÄ±r</li>
</ul>

<h2>ğŸ“ˆ NÉ™ticÉ™:</h2>
<p>
    <code>ffn_dim</code> modeli daha gÃ¼clÃ¼ vÉ™ qeyri-xÉ™tti Ã§evirmÉ™yÉ™ imkan verir.
    Lakin Ã§ox bÃ¶yÃ¼k seÃ§imlÉ™r yaddaÅŸ vÉ™ vaxt baxÄ±mÄ±ndan baha baÅŸa gÉ™lÉ™ bilÉ™r.
    Optimal dÉ™yÉ™r adÉ™tÉ™n <code>ffn_dim = 4 Ã— d_model</code> olaraq qÉ™bul edilir.
</p>

</body>
</html>
