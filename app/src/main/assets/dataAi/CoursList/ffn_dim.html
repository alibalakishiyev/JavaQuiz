<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>ffn_dim (n_inner) – Feed Forward Layer İzahedici</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          padding: 30px;
          background-color: #f7f9fc;
          color: #2c3e50;
          line-height: 1.7;
        }
        h1, h2 {
          color: #2c3e50;
        }
        .code {
          background: #eef;
          padding: 8px 12px;
          border-left: 4px solid #3498db;
          font-family: Consolas, monospace;
          margin: 10px 0;

        }
        .note {
          background: #e8f5e9;
          border-left: 6px solid #4caf50;
          padding: 10px;
          margin: 20px 0;
        }
        .warn {
          background: #fff3cd;
          border-left: 6px solid #ffc107;
          padding: 10px;
          margin: 20px 0;
        }
        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 10px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        ul {
          margin-left: 20px;
        }
    </style>
</head>
<body>

<h1>🔢 <code>ffn_dim</code> (və ya <code>n_inner</code>) nədir?</h1>

<p>
    Transformer modelində <b>hər Transformer blokunun daxilində</b> bir <strong>Feed Forward Network (FFN)</strong>
    hissəsi var. <code>ffn_dim</code> bu FFN hissəsinin daxili qatının ölçüsünü müəyyən edir.
</p>

<h2>📐 Strukturu necədir?</h2>
<p>FFN iki tam bağlı qatdan ibarətdir:</p>
<div class="formula">
    <code>FFN(x) = max(0, xW₁ + b₁)W₂ + b₂</code>
</div>

<ul>
    <li><code>W₁</code> ölçüsü: <code>[d_model, ffn_dim]</code></li>
    <li><code>W₂</code> ölçüsü: <code>[ffn_dim, d_model]</code></li>
    <li><code>d_model</code> — əsas embedding ölçüsü (məsələn: 768)</li>
    <li><code>ffn_dim</code> — gizli qatın ölçüsü (məsələn: 3072)</li>
</ul>

<h2>🧮 Riyazi Hesablama:</h2>
<p>Tutaq ki:</p>
<ul>
    <li><code>d_model = 768</code></li>
    <li><code>ffn_dim = 3072</code></li>
</ul>

<p>Əməliyyat ardıcıllığı:</p>
<ol>
    <li><code>x</code> ölçülü vektor (1 × 768)</li>
    <li><code>x @ W₁ → 1 × 3072</code></li>
    <li><code>ReLU → max(0, ·)</code></li>
    <li><code>@ W₂ → 1 × 768</code> (yenidən orijinal ölçüyə düşür)</li>
</ol>

<div class="formula">
    Hesablanacaq ağırlıqların sayı:<br>
    <strong>
        768 × 3072 + 3072 (bias) + 3072 × 768 + 768 (bias) = <br>
        ≈ 4.7 milyon parametr təkcə FFN üçün.
    </strong>
</div>

<h2>🎯 Niyə bu qədər böyük?</h2>
<ul>
    <li><strong>Transformerdə əsas "hesablama gücü" FFN qatlarındadır</strong></li>
    <li>Bu qatlarda məlumat qeyri-xətti şəkildə "transformasiya" olunur</li>
    <li>Tipik olaraq <code>ffn_dim = 4 × d_model</code></li>
    <li>GPT-2 üçün: <code>d_model = 768</code> → <code>ffn_dim = 3072</code></li>
</ul>

<h2>🧠 Dropout və Aktivasiya</h2>
<ul>
    <li>İki fully-connected qat arasında <code>ReLU</code> və ya <code>GELU</code> aktivasiya funksiyası olur</li>
    <li>Bəzən dropout da tətbiq edilir (resid_pdrop kimi)</li>
</ul>

<h2>📌 ffn_dim necə seçilməlidir?</h2>
<ul>
    <li>Əgər <code>d_model = 1024</code> → <code>ffn_dim ≈ 4096</code></li>
    <li>Əgər çox kiçik seçilsə → Modelin gücü azalır</li>
    <li>Çox böyük seçilsə → Parametr sayı və GPU istifadəsi artır</li>
</ul>

<h2>📈 Nəticə:</h2>
<p>
    <code>ffn_dim</code> modeli daha güclü və qeyri-xətti çevirməyə imkan verir.
    Lakin çox böyük seçimlər yaddaş və vaxt baxımından baha başa gələ bilər.
    Optimal dəyər adətən <code>ffn_dim = 4 × d_model</code> olaraq qəbul edilir.
</p>

</body>
</html>
