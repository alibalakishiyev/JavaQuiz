<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>attn_pdrop â€“ Attention Dropout Ä°zahedici</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          padding: 30px;
          background-color: #f7f9fc;
          color: #2c3e50;
          line-height: 1.7;
        }
        h1, h2 {
          color: #2c3e50;
        }
        .code {
          background: #eef;
          padding: 8px 12px;
          border-left: 4px solid #3498db;
          font-family: Consolas, monospace;
          margin: 10px 0;

        }
        .note {
          background: #e8f5e9;
          border-left: 6px solid #4caf50;
          padding: 10px;
          margin: 20px 0;
        }
        .warn {
          background: #fff3cd;
          border-left: 6px solid #ffc107;
          padding: 10px;
          margin: 20px 0;
        }
        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 10px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        ul {
          margin-left: 20px;
        }
    </style>
</head>
<body>

<h1>ğŸ¯ <code>attn_pdrop</code> nÉ™dir?</h1>

<p>
    <strong><code>attn_pdrop</code></strong> â€” Transformer modelindÉ™ <b>self-attention mexanizmi</b> Ã¼zÉ™rindÉ™
    tÉ™tbiq olunan <b>dropout ehtimalÄ±dÄ±r</b>. Bu, <code>softmax</code> nÉ™ticÉ™sindÉ™ É™ldÉ™ olunan attention skorlarÄ±nÄ±n
    bÉ™zilÉ™rini tÉ™sadÃ¼fi ÅŸÉ™kildÉ™ <b>0</b>-lamaqla modelin overfitting etmÉ™sinin qarÅŸÄ±sÄ±nÄ± alÄ±r.
</p>

<h2>ğŸ”¬ Ä°stifadÉ™ edildiyi yer:</h2>
<p>
    Bu dropout <b>attention weights</b> yÉ™ni:
    <code>Softmax(QKáµ€ / âˆšd_k)</code> nÉ™ticÉ™sinÉ™ tÉ™tbiq olunur.
</p>

<div class="formula">
    Attention Ã§Ä±xÄ±ÅŸÄ±: <br>
    <code>Attention(Q, K, V) = Dropout(Softmax(QKáµ€ / âˆšdâ‚–)) Â· V</code>
</div>

<h2>âœï¸ Riyazi izah:</h2>
<p>
    Tutaq ki:
<ul>
    <li><code>attn_pdrop = 0.1</code> â†’ YÉ™ni 10% attention skorlarÄ± sÄ±fÄ±rlanacaq</li>
    <li><code>QKáµ€ / âˆšd_k = [0.2, 0.7, 1.5, -0.2]</code></li>
</ul>
<p>Softmax nÉ™ticÉ™si: <code>[0.1, 0.3, 0.5, 0.1]</code></p>
<p>Dropout sonrasÄ± (10% ehtimalla): <code>[0.1, 0.0, 0.5, 0.1]</code> (mÉ™sÉ™lÉ™n, ikinci sÉ™tir sÄ±fÄ±rlandÄ±)</p>
</p>

<h2>ğŸ§  MÉ™qsÉ™d nÉ™dir?</h2>
<ul>
    <li>Attention mexanizminin bÉ™zi hissÉ™lÉ™rinin Ã¶yrÉ™nmÉ™sini <b>tÉ™sadÃ¼filÉ™ÅŸdirmÉ™k</b></li>
    <li><b>Overfitting riskini azaltmaq</b></li>
    <li><b>Modelin Ã¼mumilÉ™ÅŸmÉ™ qabiliyyÉ™tini artÄ±rmaq</b></li>
</ul>

<h2>âš™ï¸ Texniki qeydlÉ™r:</h2>
<ul>
    <li>AdÉ™tÉ™n <code>attn_pdrop = 0.1</code> olur (GPT-2-dÉ™ default)</li>
    <li>YalnÄ±z tÉ™lim zamanÄ± aktivdir</li>
    <li>Transformer bloklarÄ±nÄ±n <code>MultiHeadAttention</code> hissÉ™sindÉ™ tÉ™tbiq olunur</li>
</ul>

<h2>ğŸ“Œ DÉ™yÉ™r seÃ§imi necÉ™ tÉ™sir edir?</h2>
<ul>
    <li><b>Ã‡ox yÃ¼ksÉ™k olarsa</b> (mÉ™s. 0.5) â†’ model Ã¶yrÉ™nÉ™ bilmir</li>
    <li><b>Ã‡ox aÅŸaÄŸÄ± olarsa</b> (mÉ™s. 0.0) â†’ overfitting riski artÄ±r</li>
    <li><code>0.1</code> â€” balanslÄ± seÃ§im</li>
</ul>

<h2>ğŸ“ˆ NÉ™ticÉ™:</h2>
<p>
    <b><code>attn_pdrop</code></b> modelin diqqÉ™t bÃ¶lgÃ¼sÃ¼nÃ¼ daha sabit vÉ™ Ã¼mumilÉ™ÅŸdirici etmÉ™k Ã¼Ã§Ã¼n É™hÉ™miyyÉ™tlidir.
    Riyazi olaraq dropout ehtimalÄ± <code>p</code> ilÉ™ hÉ™r bir attention skoruna <code>Bernoulli(p)</code> testi tÉ™tbiq olunur.
</p>

</body>
</html>
