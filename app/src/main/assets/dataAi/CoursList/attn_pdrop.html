<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>attn_pdrop – Attention Dropout İzahedici</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          padding: 30px;
          background-color: #f7f9fc;
          color: #2c3e50;
          line-height: 1.7;
        }
        h1, h2 {
          color: #2c3e50;
        }
        .code {
          background: #eef;
          padding: 8px 12px;
          border-left: 4px solid #3498db;
          font-family: Consolas, monospace;
          margin: 10px 0;

        }
        .note {
          background: #e8f5e9;
          border-left: 6px solid #4caf50;
          padding: 10px;
          margin: 20px 0;
        }
        .warn {
          background: #fff3cd;
          border-left: 6px solid #ffc107;
          padding: 10px;
          margin: 20px 0;
        }
        .formula {
            background: #e3f2fd;
            border-left: 6px solid #2196f3;
            padding: 10px;
            margin: 15px 0;
            font-family: monospace;
            color: black;
        }
        ul {
          margin-left: 20px;
        }
    </style>
</head>
<body>

<h1>🎯 <code>attn_pdrop</code> nədir?</h1>

<p>
    <strong><code>attn_pdrop</code></strong> — Transformer modelində <b>self-attention mexanizmi</b> üzərində
    tətbiq olunan <b>dropout ehtimalıdır</b>. Bu, <code>softmax</code> nəticəsində əldə olunan attention skorlarının
    bəzilərini təsadüfi şəkildə <b>0</b>-lamaqla modelin overfitting etməsinin qarşısını alır.
</p>

<h2>🔬 İstifadə edildiyi yer:</h2>
<p>
    Bu dropout <b>attention weights</b> yəni:
    <code>Softmax(QKᵀ / √d_k)</code> nəticəsinə tətbiq olunur.
</p>

<div class="formula">
    Attention çıxışı: <br>
    <code>Attention(Q, K, V) = Dropout(Softmax(QKᵀ / √dₖ)) · V</code>
</div>

<h2>✍️ Riyazi izah:</h2>
<p>
    Tutaq ki:
<ul>
    <li><code>attn_pdrop = 0.1</code> → Yəni 10% attention skorları sıfırlanacaq</li>
    <li><code>QKᵀ / √d_k = [0.2, 0.7, 1.5, -0.2]</code></li>
</ul>
<p>Softmax nəticəsi: <code>[0.1, 0.3, 0.5, 0.1]</code></p>
<p>Dropout sonrası (10% ehtimalla): <code>[0.1, 0.0, 0.5, 0.1]</code> (məsələn, ikinci sətir sıfırlandı)</p>
</p>

<h2>🧠 Məqsəd nədir?</h2>
<ul>
    <li>Attention mexanizminin bəzi hissələrinin öyrənməsini <b>təsadüfiləşdirmək</b></li>
    <li><b>Overfitting riskini azaltmaq</b></li>
    <li><b>Modelin ümumiləşmə qabiliyyətini artırmaq</b></li>
</ul>

<h2>⚙️ Texniki qeydlər:</h2>
<ul>
    <li>Adətən <code>attn_pdrop = 0.1</code> olur (GPT-2-də default)</li>
    <li>Yalnız təlim zamanı aktivdir</li>
    <li>Transformer bloklarının <code>MultiHeadAttention</code> hissəsində tətbiq olunur</li>
</ul>

<h2>📌 Dəyər seçimi necə təsir edir?</h2>
<ul>
    <li><b>Çox yüksək olarsa</b> (məs. 0.5) → model öyrənə bilmir</li>
    <li><b>Çox aşağı olarsa</b> (məs. 0.0) → overfitting riski artır</li>
    <li><code>0.1</code> — balanslı seçim</li>
</ul>

<h2>📈 Nəticə:</h2>
<p>
    <b><code>attn_pdrop</code></b> modelin diqqət bölgüsünü daha sabit və ümumiləşdirici etmək üçün əhəmiyyətlidir.
    Riyazi olaraq dropout ehtimalı <code>p</code> ilə hər bir attention skoruna <code>Bernoulli(p)</code> testi tətbiq olunur.
</p>

</body>
</html>
