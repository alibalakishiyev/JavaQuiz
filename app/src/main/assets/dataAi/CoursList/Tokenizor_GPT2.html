<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Azerbaijani Tokenizer Izahi</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          background: #f9f9f9;
          padding: 30px;
          line-height: 1.7;
        }
        pre, code {
            background-color: #ffffff; /* aÄŸ fon */
            color: #000000;            /* qara mÉ™tn */
        }
        code {
            background-color: #ffffff; /* Arxa fon: aÄŸ */
            color: #000000;            /* MÉ™tnin rÉ™ngi: qara */
            padding: 2px 4px;
            border-radius: 4px;
            font-family: "Courier New", monospace;
        }
        pre {
            background: #ffffff;       /* Arxa fon: aÄŸ */
            color: #000000;            /* MÉ™tnin rÉ™ngi: qara */
            padding: 10px;
            border-left: 4px solid #ccc;
            overflow-x: auto;
        }
        h2, h3 {
          color: #444;
        }
        .section {
          margin-bottom: 40px;
        }
    </style>
</head>
<body>

<h1>

    GPT2Config sinfi modelin É™sas arxitektura parametrlÉ™rini saxlayÄ±r.
    Buraya qat sayÄ±, baÅŸ sayÄ±, embedding Ã¶lÃ§Ã¼sÃ¼ vÉ™ s. daxildir.

</h1>

<pre><code class=formula">
class GPT2Config:
    def __init__(self, n_layer=12, n_head=12, n_embd=768, block_size=1024, vocab_size=50257):
        self.n_layer = n_layer         # Transformer qatlarÄ±nÄ±n sayÄ±
        self.n_head = n_head           # Multi-head attention baÅŸlarÄ±nÄ±n sayÄ±
        self.n_embd = n_embd           # HÉ™r token Ã¼Ã§Ã¼n embedding Ã¶lÃ§Ã¼sÃ¼
        self.block_size = block_size   # Maksimum kontekst uzunluÄŸu (seq_len)
        self.vocab_size = vocab_size   # Tokenizer-in vokabulyar Ã¶lÃ§Ã¼sÃ¼
</code></pre>

<h1>

    LayerNorm sinfi giriÅŸ tensÃ¶rÃ¼nÃ¼ standartlaÅŸdÄ±rÄ±r.
    Bu modelin Ã¶yrÉ™nmÉ™sini stabillÉ™ÅŸdirir vÉ™ konvergensiyanÄ± sÃ¼rÉ™tlÉ™ndirir.

</h1>

<pre><code class=formula">
class LayerNorm(nn.Module):
    def __init__(self, ndim, eps=1e-5):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))  # Normalizasiya sonrasÄ± miqyas faktorlarÄ±
        self.bias = nn.Parameter(torch.zeros(ndim))   # Normalizasiya sonrasÄ± bias
        self.eps = eps                                # KiÃ§ik dÉ™yÉ™r â€“ sÄ±fÄ±ra bÃ¶lmÉ™nin qarÅŸÄ±sÄ±nÄ± alÄ±r

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)               # Son Ã¶lÃ§Ã¼ Ã¼zrÉ™ orta dÉ™yÉ™ri hesabla
        var = x.var(-1, keepdim=True, unbiased=False) # DispersiyanÄ± hesabla
        x_norm = (x - mean) / torch.sqrt(var + self.eps)  # Z-score normalizasiya
        return self.weight * x_norm + self.bias       # Miqyasla vÉ™ bias É™lavÉ™ et
</code></pre>

<h1>

    Bu blok GPT-2-nin É™sas komponenti olan "causal self-attention" hissÉ™sidir.
    DiqqÉ™t yalnÄ±z keÃ§miÅŸ tokenlÉ™rÉ™ baxa bilir (gÉ™lÉ™cÉ™yÉ™ yox) â€“ bu, auto-regressive tÉ™biÉ™ti tÉ™min
    edir.

</h1>

<pre><code class=formula">
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        self.key = nn.Linear(config.n_embd, config.n_embd)
        self.query = nn.Linear(config.n_embd, config.n_embd)
        self.value = nn.Linear(config.n_embd, config.n_embd)
        self.attn_drop = nn.Dropout(0.1)
        self.resid_drop = nn.Dropout(0.1)
        self.proj = nn.Linear(config.n_embd, config.n_embd)
        self.register_buffer("mask", torch.tril(torch.ones(config.block_size, config.block_size))
                                      .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size()
        k = self.key(x).view(B, T, config.n_head, C // config.n_head).transpose(1, 2)
        q = self.query(x).view(B, T, config.n_head, C // config.n_head).transpose(1, 2)
        v = self.value(x).view(B, T, config.n_head, C // config.n_head).transpose(1, 2)

        att = (q @ k.transpose(-2, -1)) / (C // config.n_head)**0.5
        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))  # GÉ™lÉ™cÉ™k tokenlÉ™rÉ™ baxma
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        return self.resid_drop(self.proj(y))
</code></pre>

<h1>

    MLP bloku hÉ™r Transformer qatÄ±nÄ±n iÃ§indÉ™ yerlÉ™ÅŸir.
    O, attention-dan sonra mÉ™lumatÄ± daha yÃ¼ksÉ™k Ã¶lÃ§Ã¼lÃ¼ fÉ™zalarda iÅŸlÉ™yir.

</h1>

<pre><code class=formula">
class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.fc1 = nn.Linear(config.n_embd, 4 * config.n_embd)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(4 * config.n_embd, config.n_embd)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        return self.dropout(x)
</code></pre>

<h1>

    Bu blokda LayerNorm + Attention + MLP ardÄ±cÄ±llÄ±ÄŸÄ± var.
    GPT-nin hÉ™r qatÄ±nda bu struktur eynidir.

</h1>

<pre><code class=formula">
class TransformerBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln1 = LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln2 = LayerNorm(config.n_embd)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln1(x))  # Residual baÄŸlantÄ± ilÉ™ attention
        x = x + self.mlp(self.ln2(x))   # Residual baÄŸlantÄ± ilÉ™ MLP
        return x
</code></pre>

<h1>

    Burada embedding-lÉ™r, Ã§oxlu Transformer bloklarÄ± vÉ™ Ã§Ä±xÄ±ÅŸ linear qat var.
    GPT-2 burada tam ÅŸÉ™kildÉ™ yÄ±ÄŸÄ±lÄ±r.

</h1>

<pre><code class=formula">
class GPT2(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)
        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))
        self.drop = nn.Dropout(0.1)
        self.blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config.n_layer)])
        self.ln_f = LayerNorm(config.n_embd)
        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

    def forward(self, idx):
        B, T = idx.size()
        tok_emb = self.token_emb(idx)        # Token embedding
        pos_emb = self.pos_emb[:, :T, :]     # MÃ¶vqe embedding
        x = self.drop(tok_emb + pos_emb)     # Toplam embedding vÉ™ dropout
        x = self.blocks(x)                   # N qat Transformer bloku
        x = self.ln_f(x)                     # Final LayerNorm
        logits = self.head(x)                # Vokabulyara logit Ã§Ä±xÄ±ÅŸÄ±
        return logits
</code></pre>


<h1>Azerbaijani Tokenizer Python Kodu - ÆtraflÄ± Ä°zah</h1>
<pre class=formula">
<b># 1. KitabxanalarÄ± import edirik</b>
import os, re
from tokenizers import models, trainers, pre_tokenizers, processors, normalizers, Regex, Tokenizer
from tokenizers.models import BPE
from tokenizers.decoders import ByteLevel
from transformers import GPT2TokenizerFast
from collections import defaultdict
from tqdm import tqdm

<b># 2. AzÉ™rbaycan dili Ã¼Ã§Ã¼n xÃ¼susi tokenizer sinifi yaradÄ±lÄ±r</b>
class AzerbaijaniTokenizer:
    def __init__(self, data_dir="../data/training_data/large_data"):
        self.data_dir = data_dir
        self.protected_patterns = self._load_protected_patterns()
        self.az_chars = "abcÃ§deÉ™fgÄŸhxÄ±ijkqlmnoÃ¶prsÅŸtuÃ¼vyzABCÃ‡DEÆFGÄHXIÄ°JKQLMNOÃ–PRSÅTUÃœVYZ"
        self.min_word_freq = 5
        self.vocab_size = 40000

<b>    # Qorunan tokenlÉ™r siyahÄ±sÄ± (parcalanmamalÄ±dÄ±r)</b>
    def _load_protected_patterns(self):
        return {"AzÉ™rbaycan", "BakÄ±", "QarabaÄŸ", "ÅuÅŸa", "XankÉ™ndi", "TÉ™briz",
                "Ä°ran", "TÃ¼rkiyÉ™", "Rusiya", "NATO", "CÉ™mil", "Ä°brahim", "AÄŸayev",
                "ÅŸÉ™hÉ™rindÉ™", "mÃ¼hacirÉ™tinin", "gedÉ™cÉ™yik", "DÃ¶vlÉ™t", "Pedaqoji",
                "Ä°nstitutunun", "gÉ™lir", "illÉ™rdÉ™", "mÃ¼É™llim", "qÉ™lÉ™mini", "bÉ™dii",
                "publisistik", "yaradÄ±cÄ±lÄ±q", "sahÉ™sindÉ™", "sÄ±namaÄŸa", "baÅŸlayÄ±r",
                "lÉ™rinÉ™", "lÉ™ri", "dakÄ±", "dÉ™ki", "dan", "dÉ™n", "lÄ±q", "lik",
                "luq", "lÃ¼k", "Ã§Ä±", "Ã§i", "Ã§u", "Ã§Ã¼"}

<b>    # Korpus faylÄ±nda qorunan sÃ¶zlÉ™ri xÃ¼susi markerlÉ™rlÉ™ É™vÉ™z et</b>
    def _preprocess_corpus(self):
        input_path = os.path.join(self.data_dir, "large_az_data.txt")
        output_path = os.path.join(self.data_dir, "protected_az_data.txt")
        if os.path.exists(output_path): return output_path

        print("ğŸ”§ Qorunan sÃ¶zlÉ™r É™lavÉ™ olunur...")
        protection_regex = re.compile(r'\b(' + '|'.join(re.escape(w) for w in self.protected_patterns) + r')\b')

        with open(input_path, 'r', encoding='utf-8') as fin, open(output_path, 'w', encoding='utf-8') as fout:
            for line in tqdm(fin, desc="Processing"):
                protected = protection_regex.sub(lambda m: f"â–{m.group(0)}â–", line.strip())
                fout.write(protected + "\n")

        return output_path

<b>    # Tokenizer-i yaradÄ±b tÉ™lim etdir</b>
    def create_tokenizer(self, output_dir="azeri_tokenizer"):
        try:
            tokenizer = Tokenizer(models.BPE(unk_token="[UNK]", fuse_unk=True, dropout=0.1))

            tokenizer.normalizer = normalizers.Sequence([
                normalizers.NFC(),
                normalizers.Replace(Regex(r"[Ê¼Â´â€˜â€™`"]), "'"),
                normalizers.Replace(Regex(r"â–"), ""),
                normalizers.StripAccents()
            ])

            tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
                pre_tokenizers.Split(Regex(r"https?://\S+|www\.\S+"), "isolated"),
                pre_tokenizers.Split(Regex(r"\b\d+[\.,]?\d*\b"), "isolated"),
                pre_tokenizers.WhitespaceSplit(),
                pre_tokenizers.Punctuation(),
                pre_tokenizers.Split(Regex(r"[^\wÉ™ÄŸÃ¼ÅŸÄ±Ã¶Ã§ÆÄÃœÅÄ°Ã–Ã‡]"), "isolated")
            ])

            trainer = trainers.BpeTrainer(
                vocab_size=self.vocab_size,
                special_tokens=["[UNK]", "[PAD]", "<|endoftext|>", "[CLS]", "[SEP]", "[MASK]", "[USER]",
                               "[ASSISTANT]", "[prompt]", "[response]", "prompt", "response", "USER", "ASSÄ°STANT",
                               "[SYSTEM]", "[BOT]", "[HUMAN]", "[MODERATOR]",
                               "[TITLE]", "[SECTION]", "[DATE]", "[TIME]", "[LOCATION]", "[SUMMARY]",
                               "[ENTITY]", "[QUESTION]", "[ANSWER]", "[KEYWORD]", "[TOPIC]",
                               "<|bos|>", "<|eos|>", "<|sep|>", "<|user|>", "<|assistant|>", "<|system|>",
                               "[YES]", "[NO]", "[MAYBE]", "[UNKNOWN]", "[EXAMPLE]", "[INSTRUCTION]"],
                min_frequency=self.min_word_freq,
                initial_alphabet=list(self.az_chars),
                continuing_subword_prefix="",
                show_progress=True,
                limit_alphabet=1000
            )

            processed_file = self._preprocess_corpus()
            print("\nğŸ”„ TÉ™lim baÅŸlayÄ±r...")
            tokenizer.train([processed_file], trainer)

            tokenizer.post_processor = processors.TemplateProcessing(
                single="<|endoftext|> $A <|endoftext|>",
                pair="<|endoftext|> $A $B <|endoftext|>",
                special_tokens=[("<|endoftext|>", tokenizer.token_to_id("<|endoftext|>"))]
            )
            tokenizer.decoder = ByteLevel(add_prefix_space=True)

            az_tokenizer = GPT2TokenizerFast(
                tokenizer_object=tokenizer,
                bos_token="<|endoftext|>",
                eos_token="<|endoftext|>",
                unk_token="[UNK]",
                pad_token="[PAD]",
                additional_special_tokens=list(self.protected_patterns),
                model_max_length=4096
            )

            os.makedirs(output_dir, exist_ok=True)
            az_tokenizer.save_pretrained(output_dir)
            test_tokenizer = GPT2TokenizerFast.from_pretrained(output_dir)
            print("\nâœ… Tokenizer yaradÄ±ldÄ± vÉ™ testdÉ™n keÃ§di!")
            return test_tokenizer

        except Exception as e:
            print(f"âŒ SÉ™hv baÅŸ verdi: {str(e)}")
            raise

<b># Main hissÉ™: Tokenizer yaradÄ±lÄ±r vÉ™ test edilir</b>
if __name__ == "__main__":
    try:
        print("ğŸš€ Tokenizer yaradÄ±lÄ±r...")
        tokenizer = AzerbaijaniTokenizer()
        trained_tokenizer = tokenizer.create_tokenizer()

        test_phrases = [
            "BakÄ± ÅŸÉ™hÉ™rindÉ™ 2023-cÃ¼ ildÉ™ QarabaÄŸa gedÉ™cÉ™yik",
            "AzÉ™rbaycan DÃ¶vlÉ™t Pedaqoji Ä°nstitutunun",
            "Bu illÉ™rdÉ™ gÉ™nc mÃ¼É™llim qÉ™lÉ™mini bÉ™dii publisistik yaradÄ±cÄ±lÄ±q sahÉ™sindÉ™ sÄ±namaÄŸa baÅŸlayÄ±r",
            "CÉ™mil Ä°brahim oÄŸlu AÄŸayev"
        ]

        print("\nğŸ” TokenlÉ™ÅŸdirilmÉ™ nÉ™ticÉ™lÉ™ri:")
        for phrase in test_phrases:
            tokens = trained_tokenizer.tokenize(phrase)
            print(f"\nPhrase: {phrase}")
            print(f"Tokens: {tokens}")
            print(f"Token IDs: {trained_tokenizer.encode(phrase)}")

    except Exception as e:
        print(f"âŒ Kritik xÉ™ta: {str(e)}")
</pre>
</body>
</html>
