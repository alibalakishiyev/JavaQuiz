<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <title>GPT2 Model Prosesi - Ä°zah</title>
    <style>
        body {
          font-family: Arial, sans-serif;
          background: #f9f9f9;
          padding: 30px;
          line-height: 1.7;
        }
        pre, code {
            background-color: #ffffff; /* aÄŸ fon */
            color: #000000;            /* qara mÉ™tn */
        }
        code {
            background-color: #ffffff; /* Arxa fon: aÄŸ */
            color: #000000;            /* MÉ™tnin rÉ™ngi: qara */
            padding: 2px 4px;
            border-radius: 4px;
            font-family: "Courier New", monospace;
        }
        pre {
            background: #ffffff;       /* Arxa fon: aÄŸ */
            color: #000000;            /* MÉ™tnin rÉ™ngi: qara */
            padding: 10px;
            border-left: 4px solid #ccc;
            overflow-x: auto;
        }
        h2, h3 {
          color: #444;
        }
        .section {
          margin-bottom: 40px;
        }
    </style>
</head>
<body>

<h1>GPT-2 Model Prosesi â€“ AÅŸamalar vÉ™ Ä°zah</h1>

<div class="section">
    <h2>ğŸ” 1. <code>prompt</code> daxil edÉ™ndÉ™ nÉ™ olur?</h2>
    <p>
        MÉ™sÉ™lÉ™n:
    <pre><code>SualÄ±nÄ±zÄ± daxil edin: Salam necÉ™sÉ™n?</code></pre>
    Bu input bu sÉ™tirdÉ™n oxunur:
    <pre><code>prompt = input("\nSualÄ±nÄ±zÄ± daxil edin (Ã§Ä±xmaq Ã¼Ã§Ã¼n 'quit' yazÄ±n): ")</code></pre>
    </p>
</div>

<div class="section">
    <h2>ğŸ§± 2. Tokenizer mÉ™rhÉ™lÉ™si (input â†’ ID-lÉ™r)</h2>
    <pre><code>input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)</code></pre>
    <p>
        Bu hissÉ™:
    <ul>
        <li><code>"Salam necÉ™sÉ™n?"</code> sÃ¶zlÉ™rini tokenizÉ™ edir â†’ mÉ™sÉ™lÉ™n: <code>["Salam", "necÉ™sÉ™n", "?"]</code></li>
        <li>Sonra bu tokenlÉ™ri ID-yÉ™ Ã§evirir â†’ mÉ™sÉ™lÉ™n: <code>[4231, 9821, 34]</code></li>
        <li>Tensor formatÄ±na salÄ±r vÉ™ modeli iÅŸlÉ™diyi cihaz Ã¼zÉ™rinÉ™ gÃ¶ndÉ™rir.</li>
    </ul>
    </p>
</div>

<div class="section">
    <h2>ğŸ§  3. ModeldÉ™ nÉ™ baÅŸ verir?</h2>
    <pre><code>output = model.generate(...)</code></pre>

    <h3>ğŸ”¹ a) Embedding Layer</h3>
    <p>
        Token ID-lÉ™ri vektorlara Ã§evrilir:<br>
        MÉ™sÉ™lÉ™n: <code>4231 â†’ [0.11, 0.85, ..., 0.22]</code> (Ã¶lÃ§Ã¼: <code>n_embd</code>)
    </p>

    <h3>ğŸ”¹ b) Rotary Positional Encoding</h3>
    <p>
        HÉ™r tokenin mÃ¶vqeyini fÉ™rqlÉ™ndirmÉ™k Ã¼Ã§Ã¼n É™lavÉ™ vektor informasiyasÄ± verilir.
    </p>

    <h3>ğŸ” c) Transformer BloklarÄ±</h3>
    <p>ModeldÉ™ 12 blok var. HÉ™r blokda:</p>

    <h4>âœ… Causal Self-Attention</h4>
    <p>
        HÉ™r token ancaq Ã¶zÃ¼ndÉ™n É™vvÉ™lkilÉ™rÉ™ baxÄ±r. MÉ™sÉ™lÉ™n:
    <ul>
        <li><code>Salam</code> â†’ tÉ™k</li>
        <li><code>necÉ™sÉ™n</code> â†’ hÉ™m <code>Salam</code>Ä±, hÉ™m Ã¶zÃ¼nÃ¼ gÃ¶rÃ¼r</li>
        <li><code>?</code> â†’ hÉ™r Ã¼Ã§Ã¼nÃ¼ gÃ¶rÃ¼r</li>
    </ul>
    Q, K, V vektorlarÄ± hesablanÄ±r vÉ™ belÉ™ bir dÃ¼sturla iÅŸlÉ™yir:
    <pre><code>Attention(Q, K, V) = softmax(QKáµ€ / âˆšd) Â· V</code></pre>
    </p>

    <h4>âœ… LayerNorm + Residual</h4>
    <p>
        Sabitliyi qorumaq Ã¼Ã§Ã¼n hÉ™r mÉ™rhÉ™lÉ™dÉ™n sonra tÉ™tbiq olunur:
        <code>x + LayerNorm(layer(x))</code>
    </p>

    <h4>âœ… Feed-Forward Network</h4>
    <p>
        Attention sonrasÄ± vektorlar daha da iÅŸlÉ™nir:
    <pre><code>x = Linear1(x) â†’ GELU â†’ Linear2(x)</code></pre>
    </p>

    <h3>ğŸ”š d) Ã‡Ä±xÄ±ÅŸ: LogitlÉ™r vÉ™ Sampling</h3>
    <p>
        Bloklardan sonra model nÃ¶vbÉ™ti tokenin ehtimallarÄ±nÄ± hesablayÄ±r.<br>
        <code>generate()</code> funksiyasÄ± <b>sampling</b> edir:
    <ul>
        <li><code>top_k</code>, <code>top_p</code>, <code>temperature</code></li>
        <li>Æn uyÄŸun tokeni seÃ§ir</li>
        <li>Yeni token É™lavÉ™ olunur vÉ™ proses davam edir</li>
    </ul>
    </p>
</div>

<div class="section">
    <h2>ğŸ”„ 4. Output TokenlÉ™rdÉ™n MÉ™tnÉ™ KeÃ§id</h2>
    <pre><code>
tokens = tokenizer.convert_ids_to_tokens(output[0])
final_text = loader.decoder.decode(tokens, skip_special_tokens=True)
    </code></pre>
    <p>
        Model Ã§Ä±xÄ±ÅŸÄ± ID-lÉ™rlÉ™ olur.<br>
        Bu ID-lÉ™r tokenlÉ™rÉ™ Ã§evrilir â†’ <code>["Salam", "necÉ™sÉ™n", "?"]</code><br>
        Custom decoder vasitÉ™silÉ™ mÉ™tndÉ™ toplanÄ±r:
    <pre><code>Cavab: MÉ™n yaxÅŸÄ±yam, sÉ™n necÉ™sÉ™n?</code></pre>
    </p>
</div>

<div class="section">
    <h2>âœ… BÃ¼tÃ¶v Prosesi SadÉ™ DillÉ™ DesÉ™k:</h2>
    <pre><code>
MÉ™tn daxil et â†’ TokenlÉ™r â†’ ID â†’ Vektorlar â†’
Attention vÉ™ FFN-lÉ™rlÉ™ emal â†’ Yeni ID-lÉ™r â†’
Token â†’ MÉ™tn
    </code></pre>
</div>

<h2>GPT-2 ModelinÉ™ SorÄŸu (Prompt) AxÄ±nÄ±</h2>

<ol>
    <li><strong>Ä°stifadÉ™Ã§i Prompt daxil edir:</strong><br>
        MÉ™sÉ™lÉ™n: <code>Salam, necÉ™sÉ™n?</code>
    </li>

    <li><strong>Tokenizer bu promptu tokenlÉ™rÉ™ Ã§evirir:</strong><br>
        Ælifba, sÃ¶zlÉ™r vÉ™ hissÉ™ciklÉ™r aÅŸaÄŸÄ±dakÄ± kimi unikal ID-lÉ™rÉ™ (vektorlara) Ã§evrilir:<br>
        <code>[2187, 43, 882, 28173]</code> &larr; bu token ID-lÉ™rdir (sÃ¶zlÉ™rin rÉ™qÉ™mlÉ™rlÉ™ tÉ™msilidir)
    </li>

    <li><strong>Bu token ID-lÉ™r tensora Ã§evrilir:</strong><br>
        ModelÉ™ verilmÉ™k Ã¼Ã§Ã¼n PyTorch tensora Ã§evrilir:<br>
        <code>input_ids = tensor([[2187, 43, 882, 28173]])</code><br>
        Bu 1xN Ã¶lÃ§Ã¼lÃ¼ bir vektordur, burada N = prompt uzunluÄŸu
    </li>

    <li><strong>Model bu input tensoru alÄ±r vÉ™ embedding verir:</strong><br>
        HÉ™r token ID Ã¼Ã§Ã¼n <code>d_model</code> Ã¶lÃ§Ã¼lÃ¼ vektor Ã§Ä±xarÄ±lÄ±r (mÉ™sÉ™lÉ™n, d_model = 768):<br>
        <code>X = input_ids * W_token_embedding</code><br>
        NÉ™ticÉ™dÉ™ Ã¶lÃ§Ã¼: <code>[1, N, 768]</code>
    </li>

    <li><strong>Model iÃ§indÉ™ Transformer bloklarÄ± iÅŸlÉ™yir:</strong>
        <ul>
            <li><code>LayerNorm â†’ SelfAttention â†’ MLP â†’ Residual Connections</code></li>
            <li>Bu ardÄ±cÄ±l qatlarda tokenlÉ™r arasÄ±nda kontekst qurulur</li>
            <li>Riyazi olaraq: <code>Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Â· V</code></li>
        </ul>
    </li>

    <li><strong>Son Ã§Ä±xÄ±ÅŸ vektoru logit-lÉ™rÉ™ Ã§evrilir:</strong><br>
        HÉ™r mÃ¶vcud token Ã¼Ã§Ã¼n ehtimal vektorlarÄ± alÄ±nÄ±r:<br>
        <code>logits = X * W_outáµ€</code> â†’ Ã¶lÃ§Ã¼: <code>[1, N, vocab_size]</code>
    </li>

    <li><strong>Sampling edilir (mÉ™sÉ™lÉ™n, top-k, top-p, temperature):</strong><br>
        Ehtimallar iÃ§indÉ™n nÃ¶vbÉ™ti token seÃ§ilir:<br>
        <code>next_token = sample(softmax(logits[-1]))</code>
    </li>

    <li><strong>Yeni tokenlÉ™r É™lavÉ™ edilir vÉ™ bu proses davam edir:</strong><br>
        Bu dÃ¶ngÃ¼ 100 tokenÉ™ qÉ™dÉ™r tÉ™krarlanÄ±r (vÉ™ ya <code>eos_token_id</code> gÉ™lÉ™nÉ™ kimi)
    </li>

    <li><strong>ÆldÉ™ olunan tokenlÉ™r decoder ilÉ™ mÉ™tndÉ™ Ã§evrilir:</strong><br>
        <code>[2187, 43, 992] â†’ "Salam, yaxÅŸÄ±yam."</code>
    </li>
</ol>

<p><strong>NÉ™ticÉ™:</strong> Cavab istifadÉ™Ã§iyÉ™ gÃ¶stÉ™rilir.</p>

</body>
</html>
