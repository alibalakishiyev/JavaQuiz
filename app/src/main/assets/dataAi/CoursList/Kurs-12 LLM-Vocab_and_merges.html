<!DOCTYPE html>
<html lang="az">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Azerbaycan Tokenizer Modeli - Ä°zah</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f9;
            color: #333;
        }
        h1,h2,h3,h4,h5 {
            color: #e74c3c;
        }
        pre {
            background-color: #2d2d2d;  /* QaranlÄ±q fon */
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            white-space: pre-wrap;
        }
        pre code {
            color: #00ffb7;  /* Kod yazÄ±sÄ± â€” parlaq yaÅŸÄ±l/mavi */
            font-family: Consolas, Monaco, monospace;
            font-size: 15px;
            display: block;
        }
        p {
            line-height: 1.6;
        }
        strong {
            color: #e74c3c;
        }
    </style>


</head>
<body>

<h1>ğŸ§  AzÉ™rbaycan Tokenizer QurulmasÄ± vÉ™ TÉ™hlili</h1>

<h2>1ï¸âƒ£ MÉ™qsÉ™d</h2>
<p>Bu layihÉ™dÉ™ xÃ¼susi AzÉ™rbaycan dilinÉ™ uyÄŸunlaÅŸdÄ±rÄ±lmÄ±ÅŸ tokenizer yaradÄ±lÄ±r. MÉ™qsÉ™d, <strong>BPE modelindÉ™n</strong> istifadÉ™ etmÉ™klÉ™ GPT-2 ilÉ™ uyÄŸun tokenizer hazÄ±rlamaqdÄ±r.</p>

<h2>2ï¸âƒ£ Ä°stifadÉ™ olunan Kitabxanalar</h2>
<pre><code>from tokenizers import Tokenizer, models, trainers, normalizers, pre_tokenizers
from tokenizers.normalizers import NFD, Sequence, Replace
from tokenizers.pre_tokenizers import Split, WhitespaceSplit
from tokenizers import Regex</code></pre>

<h2>3ï¸âƒ£ Æsas Konfiqurasiya</h2>
<ul>
    <li><strong>Vocab Size:</strong> 130000</li>
    <li><strong>XÃ¼susi TokenlÉ™r:</strong> &lt;s&gt;, &lt;/s&gt;, &lt;pad&gt;, &lt;unk&gt;, &lt;mask&gt;, vÉ™ s.</li>
    <li><strong>Daxil edilÉ™n fayllar:</strong> Qanun, texnologiya, tÉ™hsil, tÉ™mizlÉ™nmiÅŸ lÃ¼ÄŸÉ™t vÉ™ s. sahÉ™lÉ™ri É™hatÉ™ edir.</li>
</ul>

<h2>4ï¸âƒ£ Normalizasiya Prosesi</h2>
<p>AÅŸaÄŸÄ±dakÄ± hissÉ™ mÉ™tni tÉ™mizlÉ™mÉ™k vÉ™ xÃ¼susi hÉ™rflÉ™ri qorumaq Ã¼Ã§Ã¼ndÃ¼r:</p>
<pre><code>tokenizer.normalizer = Sequence([
    Replace(Regex(fr'[^-ï¿¿\s{AZ_LOWER}{AZ_UPPER}{re.escape(PUNCTUATION)}]'), ' '),
    NFD()
])</code></pre>

<h2>5ï¸âƒ£ Pre-tokenizer StrukturlaÅŸdÄ±rmasÄ±</h2>
<pre><code>tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
    Split(pattern=Regex(fr'([^{AZ_LOWER}{AZ_UPPER}\w\s])'), behavior='isolated'),
    WhitespaceSplit()
])</code></pre>
<p>Bu, yazÄ±dakÄ± <strong>durÄŸu iÅŸarÉ™lÉ™rini</strong> ayÄ±rmaq vÉ™ sÃ¶zlÉ™ri dÃ¼zgÃ¼n seqmentlÉ™ÅŸdirmÉ™k Ã¼Ã§Ã¼ndÃ¼r.</p>

<h2>6ï¸âƒ£ Tokenizer TÉ™lim Prosesi</h2>
<pre><code>trainer = trainers.BpeTrainer(
    vocab_size=130000,
    special_tokens=list(SPECIAL_TOKENS.keys()),
    initial_alphabet=list(AZ_LOWER + AZ_UPPER + PUNCTUATION),
    min_frequency=1
)</code></pre>
<p>Bu trainer ilÉ™ fayllar Ã¼zÉ™rindÉ™ tÉ™lim keÃ§irilir vÉ™ model <code>merges.txt</code> vÉ™ <code>vocab.json</code> fayllarÄ±nÄ± Ã§Ä±xarÄ±r.</p>

<h2>7ï¸âƒ£ Tokenizer Testi</h2>
<p>AÅŸaÄŸÄ±dakÄ± kod blokunda test edilÉ™n cÃ¼mlÉ™dÉ™ AzÉ™rbaycan dilinin Ã¶zÃ¼nÉ™mÉ™xsus hÉ™rflÉ™ri sÄ±naqdan keÃ§irilir:</p>
<pre><code>test_text = "AzÉ™rbaycan dilindÉ™ test: É™, Ã¼, Ã¶, Ä±, ÄŸ, ÅŸ, Ã§"
encoding = tokenizer.encode(test_text)
print(encoding.tokens)
print(tokenizer.decode(encoding.ids))</code></pre>

<h2>8ï¸âƒ£ Yadda Saxlama (GPT-2 ÃœÃ§Ã¼n)</h2>
<pre><code>tokenizer.save("tokenizer_temp.json")
# vocab.json vÉ™ merges.txt fayllarÄ±nÄ± Ã§Ä±xarma prosesi burada icra olunur</code></pre>
<p>Bunlar GPT-2 modelinin istifadÉ™ edÉ™ bilÉ™cÉ™yi formata Ã§evrilir.</p>

<h2>ğŸ“Œ NÉ™ticÉ™</h2>
<p>Bu tokenizer AzÉ™rbaycan dilinÉ™ uyÄŸunlaÅŸdÄ±rÄ±lmÄ±ÅŸ vÉ™ tÉ™mizlÉ™nmiÅŸ mÉ™tnlÉ™rlÉ™ Ã¶yrÉ™dilmiÅŸdir. XÃ¼susi simvollarÄ±n dÉ™stÉ™klÉ™nmÉ™si vÉ™ dÃ¼zgÃ¼n seqmentasiya tÉ™min olunub. HazÄ±r <code>vocab.json</code> vÉ™ <code>merges.txt</code> fayllarÄ± ilÉ™ istÉ™nilÉ™n GPT-2 modeli Ã¼zÉ™rindÉ™ istifadÉ™ edilÉ™ bilÉ™r.</p>

<h2>ğŸ“ŒTam Kod</h2>
<pre><code>
    import os
    import json
    import re
    from tokenizers import Tokenizer, models, trainers, normalizers, pre_tokenizers
    from tokenizers.normalizers import NFD, StripAccents, Sequence, Replace
    from tokenizers import Regex
    from tokenizers.pre_tokenizers import Split, WhitespaceSplit


    class AzerbaijaniTokenizer:
        def __init__(self):
            self.VOCAB_SIZE = 130000  # Tokenizer Ã¼Ã§Ã¼n maksimum sÃ¶zlÃ¼k Ã¶lÃ§Ã¼sÃ¼
            self.SPECIAL_TOKENS = {  # XÃ¼susi tokenlÉ™r vÉ™ onlarÄ±n ID-lÉ™ri
               "&lt;s&gt;": 0,
            "&lt;pad&gt;": 1,
            "&lt;/s&gt;": 2,
            "&lt;unk&gt;": 3,
            "&lt;mask&gt;": 4,
            "&lt;|endoftext|&gt;": 5,
            "&lt;case&gt;": 6
        }
            self.AZ_LOWER = "abcÃ§deÉ™fgÄŸhxÄ±ijkqlmnoÃ¶prsÅŸtuÃ¼vyz"  # KiÃ§ik hÉ™rflÉ™r
            self.AZ_UPPER = "ABCÃ‡DEÆFGÄHXIÄ°JKQLMNOÃ–PRSÅTUÃœVYZ"  # BÃ¶yÃ¼k hÉ™rflÉ™r
            self.PUNCTUATION = ".,;:!?\"'()[]{}<>@#$%^&*+-/=|\\~`"  # NÃ¶qtÉ™-vergÃ¼l vÉ™ s.
            self.TEXT_DIR = r"C:\Users\Mafia\PycharmProjects\DataCleaner\textler"  # MÉ™tnlÉ™rin yerlÉ™ÅŸdiyi qovluq

        def get_training_files(self):
            files = [  # Ä°stifadÉ™ olunacaq tÉ™lim fayllarÄ±
                "asan_xidmet_tarixi.txt",
                "azerbaijani_bank_text.txt",

            ]
            return [f for f in [os.path.join(self.TEXT_DIR, file) for file in files] if os.path.exists(f)]  # MÃ¶vcud fayllarÄ± gÃ¶tÃ¼r

        def create_base_vocab(self):
            vocab = self.SPECIAL_TOKENS.copy()  # XÃ¼susi tokenlÉ™rdÉ™n baÅŸla

            for i in range(33, 127):  # ASCII simvollarÄ± É™lavÉ™ et
                char = chr(i)
                if char not in vocab:
                    vocab[char] = len(vocab)

            for char in self.AZ_LOWER + self.AZ_UPPER:  # AzÉ™rbaycan hÉ™rflÉ™rini É™lavÉ™ et
                if char not in vocab:
                    vocab[char] = len(vocab)

            return vocab  # Æsas sÃ¶zlÃ¼yÃ¼ qaytar

        def train_tokenizer(self):
            tokenizer = Tokenizer(models.BPE(unk_token="&lt;unk&gt;"))  # BPE modeli ilÉ™ tokenizer yarat

            # Normalizator: xÃ¼susi simvollarÄ± boÅŸluqla É™vÉ™zlÉ™, aksentlÉ™ri saxla
            tokenizer.normalizer = normalizers.Sequence([
                Replace(Regex(fr'[^\w\s{self.AZ_LOWER}{self.AZ_UPPER}{re.escape(self.PUNCTUATION)}]'), ' '),
                NFD()
            ])

            # Pre-tokenizer: durÄŸu iÅŸarÉ™lÉ™rini ayÄ±r vÉ™ boÅŸluqlarla bÃ¶l
            tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
                pre_tokenizers.Split(
                    pattern=Regex(fr'([^{self.AZ_LOWER}{self.AZ_UPPER}\w\s])'),
                    behavior='isolated'
                ),
                pre_tokenizers.WhitespaceSplit()
            ])

            # BPE trainer konfiqurasiyasÄ±
            trainer = trainers.BpeTrainer(
                vocab_size=self.VOCAB_SIZE,  # Maksimum sÃ¶zlÃ¼k Ã¶lÃ§Ã¼sÃ¼
                special_tokens=list(self.SPECIAL_TOKENS.keys()),  # XÃ¼susi tokenlÉ™r
                initial_alphabet=list(self.AZ_LOWER + self.AZ_UPPER + self.PUNCTUATION),  # BaÅŸlanÄŸÄ±c É™lifba
                min_frequency=1,  # Tokenin daxil olmasÄ± Ã¼Ã§Ã¼n minimal tezlik
                show_progress=True  # Proqres gÃ¶stÉ™ricisi aktiv olsun
            )

            files = self.get_training_files()  # TÉ™lim fayllarÄ±nÄ± gÃ¶tÃ¼r
            if not files:
                raise ValueError("No training files found!")  # HeÃ§ bir fayl tapÄ±lmasa xÉ™ta at

            tokenizer.train(files, trainer)  # Tokenizer-i Ã¶yrÉ™t
            return tokenizer  # HazÄ±r tokenizer-i qaytar

        def save_for_gpt2(self, tokenizer, output_dir):
            os.makedirs(output_dir, exist_ok=True)  # Qovluq mÃ¶vcud deyilsÉ™, yarat

            learned_vocab = tokenizer.get_vocab()  # Ã–yrÉ™nilmiÅŸ sÃ¶zlÃ¼k
            base_vocab = self.create_base_vocab()  # Æsas (É™lavÉ™) sÃ¶zlÃ¼k
            merged_vocab = {**base_vocab, **learned_vocab}  # Ä°kisini birlÉ™ÅŸdir

            with open(os.path.join(output_dir, "vocab.json"), "w", encoding="utf-8") as f:
                json.dump(merged_vocab, f, ensure_ascii=False, indent=2)  # Vocab JSON kimi saxla

            tokenizer.save(os.path.join(output_dir, "tokenizer_temp.json"))  # Tokenizer-in JSON versiyasÄ±

            with open(os.path.join(output_dir, "tokenizer_temp.json"), "r", encoding="utf-8") as f:
                tokenizer_data = json.load(f)  # JSON faylÄ±nÄ± oxu

            with open(os.path.join(output_dir, "merges.txt"), "w", encoding="utf-8") as f:
                f.write("#version: 1.0 - Azerbaijani BPE merges\n")  # BaÅŸlÄ±q yaz
                merges = tokenizer_data["model"]["merges"]  # Merge qaydalarÄ±nÄ± Ã§Ä±xar
                f.write("\n".join(" ".join(pair) for pair in merges))  # Fayla yaz

            os.remove(os.path.join(output_dir, "tokenizer_temp.json"))  # MÃ¼vÉ™qqÉ™ti faylÄ± sil

            print(f"\nâœ… Successfully created GPT-2 compatible files in {output_dir}")  # UÄŸurlu mesaj

        def test_tokenizer(self, tokenizer):
            test_text = "AzÉ™rbaycan dilindÉ™ test: É™, Ã¼, Ã¶, Ä±, ÄŸ, ÅŸ, Ã§"  # Test mÉ™tni
            encoding = tokenizer.encode(test_text)  # TokenlÉ™ÅŸdir

            print("\nTest Results:")
            print(f"Original: {test_text}")  # Æsl mÉ™tn
            print(f"Tokens: {encoding.tokens}")  # TokenlÉ™r
            print(f"IDs: {encoding.ids}")  # Token ID-lÉ™ri
            print(f"Decoded: {tokenizer.decode(encoding.ids)}")  # Dekod edilmiÅŸ mÉ™tn
            print(f"Roundtrip: {tokenizer.decode(encoding.ids) == test_text}")  # Orijinala uyÄŸunluq

            # ÆlifbadakÄ± xÃ¼susi hÉ™rflÉ™rin token kimi tanÄ±nÄ±b-tanÄ±nmadÄ±ÄŸÄ±nÄ± yoxla
            special_chars = {'É™', 'Ã¼', 'Ã¶', 'Ä±', 'ÄŸ', 'ÅŸ', 'Ã§'}
            for char in special_chars:
                char_id = tokenizer.token_to_id(char)
                print(f"{char}: {'âœ“' if char_id is not None else 'âœ—'} (ID: {char_id})")


    if __name__ == "__main__":
        az_tokenizer = AzerbaijaniTokenizer()  # Tokenizer sinfini yarat
        tokenizer = az_tokenizer.train_tokenizer()  # Tokenizer-i Ã¶yrÉ™t
        az_tokenizer.test_tokenizer(tokenizer)  # Tokenizer-i test et
        output_dir = "azeri_gpt2_tokenizer"  # Ã‡Ä±xÄ±ÅŸ qovluÄŸu
        az_tokenizer.save_for_gpt2(tokenizer, output_dir)  # GPT-2 uyÄŸun fayllarÄ± saxla
</code></pre>

</body>
</html>
